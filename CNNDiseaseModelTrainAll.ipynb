{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "from CNNDiseaseModel import CNNDisease\n",
    "from CNNdata_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_integer(\"num_filters\", 32,\"number of filter each filter_size\")\n",
    "tf.flags.DEFINE_integer(\"num_classes\", 22, \"number of labels\")\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.01, \"learning rate\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"batch size for train or evaluate\")\n",
    "tf.flags.DEFINE_integer(\"sequence_length\", 81, \"max sequence_length\")\n",
    "tf.flags.DEFINE_integer(\"embed_size\",100, \"embedding size\" )\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 20, \"number of epochs to train.\")\n",
    "tf.flags.DEFINE_boolean(\"is_training\", True,\"if is train step\")\n",
    "tf.app.flags.DEFINE_integer(\"decay_steps\", 3500, \"how many steps before decay learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"decay_rate\", 0.65, \"Rate of decay for learning rate.\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"train_data_path\", \"./cnnModel/datasets/trainall.feature\",\n",
    "                           \"path of traning data.\")\n",
    "tf.app.flags.DEFINE_string(\"train_label_path\", \"./cnnModel/datasets/trainall.label\",\n",
    "                           \"path of labels of traning data.\")\n",
    "tf.flags.DEFINE_string(\"test_data_path\", \"./cnnModel/datasets/testall.feature\", \"Test data source\")\n",
    "tf.flags.DEFINE_string(\"test_label_path\", \"./cnnModel/datasets/testall.label\", \"Label for test data\")\n",
    "tf.app.flags.DEFINE_string(\"word2vec_model_path\", \"./model/word2VecModelsh.bin5_100_1e-05_15\",\n",
    "                           \"word2vec's vocabulary and vectors\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"number of checkpoints\")\n",
    "tf.flags.DEFINE_boolean(\"use_embedding\", True,\"if use pre trained word2vec embedding\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"dropout_keep_prob\")\n",
    "tf.flags.DEFINE_integer(\"validate_every\", 5, \"Validate every validate_every epochs.\")\n",
    "tf.flags.DEFINE_string(\"ckpt_dir\", \"./runs/cnn_disease_checkpoint3in/\",\n",
    "                           \"checkpoint location for the model\")\n",
    "FLAGS = tf.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path, name_scope=''):\n",
    "    cache_path = './cache_vocabulary_label_pik3in/' + name_scope + \"_word_voabulary.pik\"\n",
    "    #print(\"cache_path:\", cache_path, \"file_exists:\", os.path.exists(cache_path))\n",
    "    # load the cache file if exists\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as data_f:\n",
    "            vocabulary_word2index, vocabulary_index2word = pickle.load(data_f)\n",
    "            return vocabulary_word2index, vocabulary_index2word\n",
    "    else:\n",
    "        vocabulary_word2index = {}\n",
    "        vocabulary_index2word = {}\n",
    "        model = Word2Vec.load(word2vec_model_path)\n",
    "        print(\"vocabulary:\", len(model.wv.vocab))\n",
    "        vocabulary_word2index[\"0\"] = 0\n",
    "        vocabulary_index2word[0] = \"0\"\n",
    "        for i, vocab in enumerate(model.wv.vocab):\n",
    "            vocabulary_word2index[vocab] = i + 1\n",
    "            vocabulary_index2word[i + 1] = vocab\n",
    "\n",
    "        # save to file system if vocabulary of words is not exists.\n",
    "        print(len(vocabulary_word2index))\n",
    "        if not os.path.exists(cache_path):\n",
    "            with open(cache_path, 'wb') as data_f:\n",
    "                pickle.dump((vocabulary_word2index, vocabulary_index2word), data_f)\n",
    "    return vocabulary_word2index, vocabulary_index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_pretrained_word_embedding(sess, cnnDisease, word2vec_model):\n",
    "    print(\"using pre-trained word emebedding.started.word2vec_model_path:\", FLAGS.word2vec_model_path)\n",
    "    word2vec_dict = {}\n",
    "    vocab_size = len(word2vec_model.wv.index2word)\n",
    "    print(\"vocab_size=\",vocab_size)\n",
    "    \n",
    "    word_embedding_2dlist = [[]] * (vocab_size+1)  # create an empty word_embedding list.\n",
    "    bound = np.sqrt(6.0) / np.sqrt(vocab_size)\n",
    "    count_exist = 0\n",
    "    count_not_exist = 0\n",
    "    word_embedding_2dlist[0] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n",
    "    for i, word in enumerate(model.wv.vocab):\n",
    "    #for i in range(vocab_size):\n",
    "        #word = word2vec_model.wv.index2word[i]\n",
    "        embedding = None\n",
    "        try:\n",
    "            embedding = word2vec_model.wv[word]\n",
    "        except:\n",
    "            embedding = None\n",
    "        if embedding is not None:\n",
    "            word_embedding_2dlist[i+1] = embedding\n",
    "            count_exist += 1\n",
    "        else:\n",
    "            word_embedding_2dlist[i+1] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n",
    "            count_not_exist += 1\n",
    "        \n",
    "    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n",
    "    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n",
    "    t_assign_embedding = tf.assign(cnnDisease.Embedding,\n",
    "                                   word_embedding)  # assign this value to our embedding variables of our model.\n",
    "    sess.run(t_assign_embedding)\n",
    "    print(\"word. exists embedding:\", count_exist, \" ;word not exist embedding:\", count_not_exist)\n",
    "    print(\"using pre-trained word emebedding.ended...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(train_data_path, train_label_path, test_data_path, test_label_path, vocabulary_word2index):\n",
    "    print(\"Loading data...\")\n",
    "    x_train, y_train = loadTrainOrTest_data(train_data_path, train_label_path,vocabulary_word2index)\n",
    "    x_test, y_test = loadTrainOrTest_data(test_data_path, test_label_path, vocabulary_word2index)\n",
    "    train = (x_train, y_train)\n",
    "    test = (x_test, y_test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make evaluation on test set\n",
    "def do_eval(sess, cnnDisease, evalX, evalY, batch_size):\n",
    "    number_examples = len(evalX)\n",
    "    eval_loss, eval_acc, eval_counter = 0.0, 0.0, 0\n",
    "    for start, end in zip(range(0, number_examples, batch_size), range(batch_size, number_examples, batch_size)):\n",
    "        feed_dict = {cnnDisease.input_x: evalX[start:end], cnnDisease.dropout_keep_prob: 1.0}\n",
    "        feed_dict[cnnDisease.input_y] = evalY[start:end]\n",
    "        curr_eval_loss, logits, curr_eval_acc = sess.run([cnnDisease.loss_val, cnnDisease.logits, cnnDisease.accuracy],\n",
    "                                                         feed_dict)\n",
    "        eval_loss, eval_acc, eval_counter = eval_loss + curr_eval_loss, eval_acc + curr_eval_acc, eval_counter + 1\n",
    "    return eval_loss / float(eval_counter), eval_acc / float(eval_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(FLAGS.train_data_path,\"r\")\n",
    "count = 0\n",
    "for line in f:\n",
    "    #print(line)\n",
    "    count +=1\n",
    "    #print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_word2index.vocab_size: 192413\n",
      "cnn_model.vocab_size: 192413\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "    # 1.load data(X:list of lint,y:int).\n",
    "    model = Word2Vec.load(FLAGS.word2vec_model_path)\n",
    "    vocabulary_word2index, vocabulary_index2word = create_voabulary(\n",
    "        word2vec_model_path=FLAGS.word2vec_model_path, \n",
    "        name_scope=\"cnn\")  # simple='simple'\n",
    "    vocab_size = len(vocabulary_word2index)\n",
    "    print(\"vocabulary_word2index.vocab_size:\", vocab_size)\n",
    "    vocab_size = len(model.wv.index2word)\n",
    "    print(\"cnn_model.vocab_size:\", vocab_size)\n",
    "    trainX, trainY, testX, testY = None, None, None, None\n",
    "    train, test = load_data(train_data_path=FLAGS.train_data_path, train_label_path=FLAGS.train_label_path,\n",
    "                            test_data_path=FLAGS.test_data_path, test_label_path=FLAGS.test_label_path,\n",
    "                            vocabulary_word2index=vocabulary_word2index)\n",
    "    trainX, trainY = train\n",
    "    testX, testY = test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start padding & transform to one hot...\n",
      "maxlen= 81\n",
      "maxlen= 81\n"
     ]
    }
   ],
   "source": [
    "    # 2.Data preprocessing.Sequence padding\n",
    "    print(\"start padding & transform to one hot...\")\n",
    "    trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length)  # padding to max length\n",
    "    testX = pad_sequences(testX, maxlen=FLAGS.sequence_length)  # padding to max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[7, 2, 1, 0, 3, 4, 6, 8, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "x = [i for i in range(10) ]\n",
    "print(x)\n",
    "random.shuffle(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learing rate: <tf.Variable 'learning_rate:0' shape=() dtype=float32_ref>\n",
      "global_step: <tf.Variable 'global_step:0' shape=() dtype=int32_ref>\n",
      "decay_steps: 3500\n",
      "decay_rate: 0.65\n",
      "decay_steps=3500 decay_rate=0.650000\n",
      "Initializing Variables\n",
      "using pre-trained word emebedding.started.word2vec_model_path: ./model/word2VecModelsh.bin5_100_1e-05_15\n",
      "vocab_size= 192413\n",
      "word. exists embedding: 192413  ;word not exist embedding: 0\n",
      "using pre-trained word emebedding.ended...\n",
      "curr_epoch= 0\n",
      "number_of_training_data= 46239\n",
      "batch_size= 64\n",
      "trainX[start:end]: [[169772  13919 121828  98928 117592  61202 100656  49430  10186 192114\n",
      "  172887  35237  98333 113032 149534  29235 153913 189587 154865  18223\n",
      "   31658 120780 147987  52378 103837   4155  23245 106036  25200 112186\n",
      "   33199  76013 121102 147540  23453   7027 172435  54476   4382  33088\n",
      "    7487 117592   6272 138917  76121 169772  17923  56255  49430  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 117592   6272 138917  10442 169772  17923  56255  49430  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [106842 106842  66268  39359 107638 147746  70745 130814  48968 132197\n",
      "   90835  60911 189268  31799  49690 142158  93796  39664 114186  26756\n",
      "  137419 138164  87304 113002 157335 187243  72635  48968  78103  21052\n",
      "  121602  31176 181995  28499  33088  93101 102579 124316   3789  80826\n",
      "   48945  56255 106509  77141 106842 145173  71524 138082 107188 161521\n",
      "  178973  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  56255 106509 106842  77141 164843 145173  71524 138082 107188\n",
      "  161521 178973  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 0\tBatch 100\tTrain Loss:14.709\tTrain Accuracy:0.428\n",
      "Epoch 0\tBatch 200\tTrain Loss:8.378\tTrain Accuracy:0.558\n",
      "Epoch 0\tBatch 300\tTrain Loss:6.107\tTrain Accuracy:0.635\n",
      "Epoch 0\tBatch 400\tTrain Loss:4.941\tTrain Accuracy:0.683\n",
      "Epoch 0\tBatch 500\tTrain Loss:4.226\tTrain Accuracy:0.716\n",
      "Epoch 0\tBatch 600\tTrain Loss:3.738\tTrain Accuracy:0.740\n",
      "Epoch 0\tBatch 700\tTrain Loss:3.388\tTrain Accuracy:0.758\n",
      "going to increment epoch counter....\n",
      "0 5 True\n",
      "Epoch 0 Validation Loss:1.038\tValidation Accuracy: 0.921\t time: 2018-01-05T17:50:20.219203\n",
      "trainX[start:end]: [[ 84499 180992 131786  73156  31963  99725 136619  55481 163781 101556\n",
      "  130096 148160 108246  98813 179101  74529  82270 130566  79009  80427\n",
      "   68928 180992 131786  73156  31963  99725 136619  55481 163781 101556\n",
      "  130096 148160 108246  98813 179101  74529  82270 130566  79009  80427\n",
      "   68928 180992 131786  73156  31963  99725 136619  55481 163781 101556\n",
      "  130096 148160 108246  98813 179101  74529  82270 130566  79009  80427\n",
      "   68928 180992 131786  73156  31963  99725 136619  55481 163781 101556\n",
      "  130096 148160 108246  98813 179101  74529  82270 130566  79009  80427\n",
      "   68928]\n",
      " [ 84898  68484  52089 146060  38306  79454 171885 145314  55782  93715\n",
      "   99416 110841 108843  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  68484  52089 146060  38306  79454 171885 145314  55782  93715\n",
      "   99416 110841 108843  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  68484  52089 146060  38306  79454 171885 145314  55782  93715\n",
      "   99416 110841 108843  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  68484  52089 146060  38306  79454 171885 145314  55782  93715\n",
      "   99416 110841 108843  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 1\tBatch 100\tTrain Loss:1.124\tTrain Accuracy:0.895\n",
      "Epoch 1\tBatch 200\tTrain Loss:1.130\tTrain Accuracy:0.894\n",
      "Epoch 1\tBatch 300\tTrain Loss:1.131\tTrain Accuracy:0.894\n",
      "Epoch 1\tBatch 400\tTrain Loss:1.136\tTrain Accuracy:0.894\n",
      "Epoch 1\tBatch 500\tTrain Loss:1.134\tTrain Accuracy:0.895\n",
      "Epoch 1\tBatch 600\tTrain Loss:1.124\tTrain Accuracy:0.897\n",
      "Epoch 1\tBatch 700\tTrain Loss:1.113\tTrain Accuracy:0.897\n",
      "going to increment epoch counter....\n",
      "1 5 False\n",
      "trainX[start:end]: [[ 70972  90928  63439  73704  83932   9886 130471  47351 116086  79403\n",
      "  103067 173103  28336 108398  32096  84715  26008  73211 100145  81816\n",
      "  139166  90928  63439  73704  83932   9886 130471  47351 116086  79403\n",
      "  103067 173103  28336 108398  32096  84715  26008  73211 100145  81816\n",
      "  139166  90928  63439  73704  83932   9886 130471  47351 116086  79403\n",
      "  103067 173103  28336 108398  32096  84715  26008  73211 100145  81816\n",
      "  139166  90928  63439  73704  83932   9886 130471  47351 116086  79403\n",
      "  103067 173103  28336 108398  32096  84715  26008  73211 100145  81816\n",
      "  139166]\n",
      " [ 37336  77209 147986 104450   8978  75191 164464 140879  84120  32333\n",
      "   17398 116011  67994 142922 125832 116072 187068  47583 184920 149091\n",
      "  143053  77209 147986 104450   8978  75191 164464 140879  84120  32333\n",
      "   17398 116011  67994 142922 125832 116072 187068  47583 184920 149091\n",
      "  143053  77209 147986 104450   8978  75191 164464 140879  84120  32333\n",
      "   17398 116011  67994 142922 125832 116072 187068  47583 184920 149091\n",
      "  143053  77209 147986 104450   8978  75191 164464 140879  84120  32333\n",
      "   17398 116011  67994 142922 125832 116072 187068  47583 184920 149091\n",
      "  143053]]\n",
      "Epoch 2\tBatch 100\tTrain Loss:0.956\tTrain Accuracy:0.923\n",
      "Epoch 2\tBatch 200\tTrain Loss:0.967\tTrain Accuracy:0.919\n",
      "Epoch 2\tBatch 300\tTrain Loss:0.974\tTrain Accuracy:0.917\n",
      "Epoch 2\tBatch 400\tTrain Loss:0.981\tTrain Accuracy:0.917\n",
      "Epoch 2\tBatch 500\tTrain Loss:0.988\tTrain Accuracy:0.916\n",
      "Epoch 2\tBatch 600\tTrain Loss:0.993\tTrain Accuracy:0.915\n",
      "Epoch 2\tBatch 700\tTrain Loss:0.996\tTrain Accuracy:0.914\n",
      "going to increment epoch counter....\n",
      "2 5 False\n",
      "trainX[start:end]: [[ 71524  49017  81373  98901 181268 102579 124316 107877  66268  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  49017  81373  98901 181268 102579 124316 107877  66268  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  49017  81373  98901 181268 102579 124316 107877  66268  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  49017  81373  98901 181268 102579 124316 107877  66268  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [177822  82314  73676 117637  71576  87304  24671 159683  79454  73998\n",
      "   31469  71747 110841 108843  68699  68699  68699  68699  68699  68699\n",
      "   68699  82314  73676 117637  71576  87304  24671 159683  79454  73998\n",
      "   31469  71747 110841 108843  68699  68699  68699  68699  68699  68699\n",
      "   68699  82314  73676 117637  71576  87304  24671 159683  79454  73998\n",
      "   31469  71747 110841 108843  68699  68699  68699  68699  68699  68699\n",
      "   68699  82314  73676 117637  71576  87304  24671 159683  79454  73998\n",
      "   31469  71747 110841 108843  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 3\tBatch 100\tTrain Loss:0.957\tTrain Accuracy:0.919\n",
      "Epoch 3\tBatch 200\tTrain Loss:0.962\tTrain Accuracy:0.918\n",
      "Epoch 3\tBatch 300\tTrain Loss:0.970\tTrain Accuracy:0.917\n",
      "Epoch 3\tBatch 400\tTrain Loss:0.969\tTrain Accuracy:0.917\n",
      "Epoch 3\tBatch 500\tTrain Loss:0.968\tTrain Accuracy:0.917\n",
      "Epoch 3\tBatch 600\tTrain Loss:0.963\tTrain Accuracy:0.918\n",
      "Epoch 3\tBatch 700\tTrain Loss:0.965\tTrain Accuracy:0.917\n",
      "going to increment epoch counter....\n",
      "3 5 False\n",
      "trainX[start:end]: [[ 84499 126329 180992  84499  80427 113324  31963  66864 143460  98813\n",
      "  120474 148826  86878  63861  37042 146619  22971  86418  40467  41552\n",
      "   48968 126329 180992  84499  80427 113324  31963  66864 143460  98813\n",
      "  120474 148826  86878  63861  37042 146619  22971  86418  40467  41552\n",
      "   48968 126329 180992  84499  80427 113324  31963  66864 143460  98813\n",
      "  120474 148826  86878  63861  37042 146619  22971  86418  40467  41552\n",
      "   48968 126329 180992  84499  80427 113324  31963  66864 143460  98813\n",
      "  120474 148826  86878  63861  37042 146619  22971  86418  40467  41552\n",
      "   48968]\n",
      " [ 48215  84715    409  12287  52695  89888  90928  63439 111541  21660\n",
      "  130471  65768  63090  89196 147511  47351 116086  50471  79403 133493\n",
      "   60653  84715    409  12287  52695  89888  90928  63439 111541  21660\n",
      "  130471  65768  63090  89196 147511  47351 116086  50471  79403 133493\n",
      "   60653  84715    409  12287  52695  89888  90928  63439 111541  21660\n",
      "  130471  65768  63090  89196 147511  47351 116086  50471  79403 133493\n",
      "   60653  84715    409  12287  52695  89888  90928  63439 111541  21660\n",
      "  130471  65768  63090  89196 147511  47351 116086  50471  79403 133493\n",
      "   60653]]\n",
      "Epoch 4\tBatch 100\tTrain Loss:0.866\tTrain Accuracy:0.929\n",
      "Epoch 4\tBatch 200\tTrain Loss:0.860\tTrain Accuracy:0.929\n",
      "Epoch 4\tBatch 300\tTrain Loss:0.869\tTrain Accuracy:0.928\n",
      "Epoch 4\tBatch 400\tTrain Loss:0.874\tTrain Accuracy:0.928\n",
      "Epoch 4\tBatch 500\tTrain Loss:0.881\tTrain Accuracy:0.926\n",
      "Epoch 4\tBatch 600\tTrain Loss:0.895\tTrain Accuracy:0.924\n",
      "Epoch 4\tBatch 700\tTrain Loss:0.892\tTrain Accuracy:0.925\n",
      "going to increment epoch counter....\n",
      "4 5 False\n",
      "trainX[start:end]: [[168720  66268  59248 101265 131907 126778  76710  40130 131836  34404\n",
      "    8625 108334 127243  89475  70321 123629 174891  18070 169219 188519\n",
      "   95964  71747  57767  79454  95376 105092  28708  10519  50127 145314\n",
      "   93715  48945  99416 110841  91841  68699  68699  68699  68699  68699\n",
      "   68699   1862 169219 174891  68699  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699   1862 169219 174891  68699  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [116342  85268  66268 128124   3162  48968  47042   6540    283  73156\n",
      "   19184  30202  71747  49167 107638  61813 169584   1368  77370  35232\n",
      "   92244   6540  71747 177701 152285  79454 111299 162327 186121 126411\n",
      "   51807 101658 102579 124236  99416 110841 120695  84431 108843 162786\n",
      "   68699  57978  30067  17109 130886 139003  56255   9423  51807  42257\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  57978  30067  17109 130886 139003  56255   9423  51807  42257\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 5\tBatch 100\tTrain Loss:0.700\tTrain Accuracy:0.945\n",
      "Epoch 5\tBatch 200\tTrain Loss:0.656\tTrain Accuracy:0.950\n",
      "Epoch 5\tBatch 300\tTrain Loss:0.642\tTrain Accuracy:0.948\n",
      "Epoch 5\tBatch 400\tTrain Loss:0.624\tTrain Accuracy:0.948\n",
      "Epoch 5\tBatch 500\tTrain Loss:0.623\tTrain Accuracy:0.947\n",
      "Epoch 5\tBatch 600\tTrain Loss:0.621\tTrain Accuracy:0.947\n",
      "Epoch 5\tBatch 700\tTrain Loss:0.619\tTrain Accuracy:0.946\n",
      "going to increment epoch counter....\n",
      "5 5 True\n",
      "Epoch 5 Validation Loss:0.493\tValidation Accuracy: 0.968\t time: 2018-01-05T18:05:53.187983\n",
      "trainX[start:end]: [[ 32805   2996 135625  66268  33088  80140 166361   9317 101265  59248\n",
      "  163550  94903 188507 134986 186611   6428   5956 107638 101807  80070\n",
      "  134151 147146  92053 173248 174817  32805  38933 115064    740  33088\n",
      "   50127  71747  99416 110841  68699  68699  68699  68699  68699  68699\n",
      "   68699  62612  32805  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  62612  32805  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [165757 166428 155711  31221 106473  16378 101684 184102  23630 115017\n",
      "   38092  45339 137916 171357 124616   6429  83549 192033  98901 107877\n",
      "  107579 166428 155711  31221 106473  16378 101684 184102  23630 115017\n",
      "   38092  45339 137916 171357 124616   6429  83549 192033  98901 107877\n",
      "  107579 166428 155711  31221 106473  16378 101684 184102  23630 115017\n",
      "   38092  45339 137916 171357 124616   6429  83549 192033  98901 107877\n",
      "  107579 166428 155711  31221 106473  16378 101684 184102  23630 115017\n",
      "   38092  45339 137916 171357 124616   6429  83549 192033  98901 107877\n",
      "  107579]]\n",
      "Epoch 6\tBatch 100\tTrain Loss:0.549\tTrain Accuracy:0.954\n",
      "Epoch 6\tBatch 200\tTrain Loss:0.532\tTrain Accuracy:0.954\n",
      "Epoch 6\tBatch 300\tTrain Loss:0.528\tTrain Accuracy:0.953\n",
      "Epoch 6\tBatch 400\tTrain Loss:0.532\tTrain Accuracy:0.952\n",
      "Epoch 6\tBatch 500\tTrain Loss:0.533\tTrain Accuracy:0.952\n",
      "Epoch 6\tBatch 600\tTrain Loss:0.538\tTrain Accuracy:0.951\n",
      "Epoch 6\tBatch 700\tTrain Loss:0.541\tTrain Accuracy:0.950\n",
      "going to increment epoch counter....\n",
      "6 5 False\n",
      "trainX[start:end]: [[ 71903 134876  61058  49045 137945  20609 125378  90832 180992   5917\n",
      "  162671  76710 127821  99416 161247 102051 131787  68699  68699  68699\n",
      "   68699 134876  61058  49045 137945  20609 125378  90832 180992   5917\n",
      "  162671  76710 127821  99416 161247 102051 131787  68699  68699  68699\n",
      "   68699 134876  61058  49045 137945  20609 125378  90832 180992   5917\n",
      "  162671  76710 127821  99416 161247 102051 131787  68699  68699  68699\n",
      "   68699 134876  61058  49045 137945  20609 125378  90832 180992   5917\n",
      "  162671  76710 127821  99416 161247 102051 131787  68699  68699  68699\n",
      "   68699]\n",
      " [ 62612  91898 152928 124668 170788    451  99191 144308 162242  48968\n",
      "  179450  76172  20292  54943   4918  45725 181933  68699  68699  68699\n",
      "   68699  91898 152928 124668 170788    451  99191 144308 162242  48968\n",
      "  179450  76172  20292  54943   4918  45725 181933  68699  68699  68699\n",
      "   68699  91898 152928 124668 170788    451  99191 144308 162242  48968\n",
      "  179450  76172  20292  54943   4918  45725 181933  68699  68699  68699\n",
      "   68699  91898 152928 124668 170788    451  99191 144308 162242  48968\n",
      "  179450  76172  20292  54943   4918  45725 181933  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 7\tBatch 100\tTrain Loss:0.516\tTrain Accuracy:0.957\n",
      "Epoch 7\tBatch 200\tTrain Loss:0.516\tTrain Accuracy:0.956\n",
      "Epoch 7\tBatch 300\tTrain Loss:0.509\tTrain Accuracy:0.957\n",
      "Epoch 7\tBatch 400\tTrain Loss:0.513\tTrain Accuracy:0.955\n",
      "Epoch 7\tBatch 500\tTrain Loss:0.517\tTrain Accuracy:0.954\n",
      "Epoch 7\tBatch 600\tTrain Loss:0.518\tTrain Accuracy:0.954\n",
      "Epoch 7\tBatch 700\tTrain Loss:0.522\tTrain Accuracy:0.953\n",
      "going to increment epoch counter....\n",
      "7 5 False\n",
      "trainX[start:end]: [[129453 143053 144308 161050  96307 161231 177119  17119  98560 162156\n",
      "   36804 108246 116011 156189   6039 103731 113987 142922 109980 180992\n",
      "    8410   9785  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  28069  96601  39182 154717 119614 127398 125086  54025 102408\n",
      "  191645 162097 125154  56032 133754  81426 103516  48973  92974   5849\n",
      "   70098  97895 124316 121996  68699  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [168627  66268  33088  71747  39359 124316    283  21235  22323  46299\n",
      "  181979  84431 151693 101265  60786  99324  60676  10946 106173 178119\n",
      "  100712  73676  71747  21235  21052 136468  87304 137525  22323  56961\n",
      "   24671  90639  92253  31176   6540 188329  33088 185394 108843  68699\n",
      "   68699  30183 150435  56255 168627   7289  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  30183   7289 168627  56255 121230 169293  94787  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 8\tBatch 100\tTrain Loss:0.487\tTrain Accuracy:0.963\n",
      "Epoch 8\tBatch 200\tTrain Loss:0.496\tTrain Accuracy:0.959\n",
      "Epoch 8\tBatch 300\tTrain Loss:0.492\tTrain Accuracy:0.959\n",
      "Epoch 8\tBatch 400\tTrain Loss:0.500\tTrain Accuracy:0.958\n",
      "Epoch 8\tBatch 500\tTrain Loss:0.507\tTrain Accuracy:0.957\n",
      "Epoch 8\tBatch 600\tTrain Loss:0.512\tTrain Accuracy:0.957\n",
      "Epoch 8\tBatch 700\tTrain Loss:0.513\tTrain Accuracy:0.956\n",
      "going to increment epoch counter....\n",
      "8 5 False\n",
      "trainX[start:end]: [[ 71903  71747 159674  61496  90819 125371  98274 180297 147146  33088\n",
      "   61986 162671 124316  28336  76710 178791 123558 185664 120618 122542\n",
      "   99416  71747 159674  61496  90819 125371  98274 180297 147146  33088\n",
      "   61986 162671 124316  28336  76710 178791 123558 185664 120618 122542\n",
      "   99416  71747 159674  61496  90819 125371  98274 180297 147146  33088\n",
      "   61986 162671 124316  28336  76710 178791 123558 185664 120618 122542\n",
      "   99416  71747 159674  61496  90819 125371  98274 180297 147146  33088\n",
      "   61986 162671 124316  28336  76710 178791 123558 185664 120618 122542\n",
      "   99416]\n",
      " [121082 162424 136619 105534  95149  46239  66548 154220  71762  76710\n",
      "   68803  66268 154609  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 162424 136619 105534  95149  46239  66548 154220  71762  76710\n",
      "   68803  66268 154609  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 162424 136619 105534  95149  46239  66548 154220  71762  76710\n",
      "   68803  66268 154609  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 162424 136619 105534  95149  46239  66548 154220  71762  76710\n",
      "   68803  66268 154609  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 9\tBatch 100\tTrain Loss:0.488\tTrain Accuracy:0.961\n",
      "Epoch 9\tBatch 200\tTrain Loss:0.473\tTrain Accuracy:0.963\n",
      "Epoch 9\tBatch 300\tTrain Loss:0.479\tTrain Accuracy:0.962\n",
      "Epoch 9\tBatch 400\tTrain Loss:0.488\tTrain Accuracy:0.960\n",
      "Epoch 9\tBatch 500\tTrain Loss:0.493\tTrain Accuracy:0.960\n",
      "Epoch 9\tBatch 600\tTrain Loss:0.493\tTrain Accuracy:0.960\n",
      "Epoch 9\tBatch 700\tTrain Loss:0.487\tTrain Accuracy:0.960\n",
      "going to increment epoch counter....\n",
      "9 5 False\n",
      "trainX[start:end]: [[ 84499   8410 113687  73156 157747  12287 167739   8274 159049  63318\n",
      "   74844 122765 131730  86418 126455 111693  40467  60954 185911 190695\n",
      "  125378 136619  33088  66268  73156  45725 166807 178834 151653 133831\n",
      "   28708  84378   2835 127398  94893   7289 134370  92980  33290  59277\n",
      "   81776  84499  70399  41262  68699  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  17253 143215  73372  56255  70399 116061  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [130682   2996  39359  66268 144881  83527  48968  58135 174529  23315\n",
      "  107638 114198 127243  99725  89471  16968  84431 165266 131907  31799\n",
      "  126669  66268 139479  71747  91601 151362 129190  24988 176960  73676\n",
      "   58011 130682 141847 169349  33088 132805 125030  84431  68699  68699\n",
      "   68699  56255 130682  18111 166408  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 141847  56255  18111 166408  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 10\tBatch 100\tTrain Loss:0.375\tTrain Accuracy:0.974\n",
      "Epoch 10\tBatch 200\tTrain Loss:0.368\tTrain Accuracy:0.974\n",
      "Epoch 10\tBatch 300\tTrain Loss:0.362\tTrain Accuracy:0.973\n",
      "Epoch 10\tBatch 400\tTrain Loss:0.359\tTrain Accuracy:0.972\n",
      "Epoch 10\tBatch 500\tTrain Loss:0.358\tTrain Accuracy:0.972\n",
      "Epoch 10\tBatch 600\tTrain Loss:0.356\tTrain Accuracy:0.971\n",
      "Epoch 10\tBatch 700\tTrain Loss:0.354\tTrain Accuracy:0.971\n",
      "going to increment epoch counter....\n",
      "10 5 True\n",
      "Epoch 10 Validation Loss:0.269\tValidation Accuracy: 0.988\t time: 2018-01-05T18:21:01.254077\n",
      "trainX[start:end]: [[ 23128  37100  33088  39359  66268 135052   3478   7389 101265 122542\n",
      "  175462  33155 154626 112216 129372  23429  77735  41482  19768 111836\n",
      "  113916 149576  29501  42779 117903 166984    740  33088  50127 131713\n",
      "   51849 127821  71747  99416 110841  68699  68699  68699  68699  68699\n",
      "   68699 140773 154626  56255  66895  95814 164429  23128  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 140773  56255  66895  95814 164429  23128  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [ 64574 131786 130096  74999   7714 161050 186218  42658 124860  47230\n",
      "  120474 182879 179998 111862   6954 108246  61601 146548  56534  82270\n",
      "  105942 131786 130096  74999   7714 161050 186218  42658 124860  47230\n",
      "  120474 182879 179998 111862   6954 108246  61601 146548  56534  82270\n",
      "  105942 131786 130096  74999   7714 161050 186218  42658 124860  47230\n",
      "  120474 182879 179998 111862   6954 108246  61601 146548  56534  82270\n",
      "  105942 131786 130096  74999   7714 161050 186218  42658 124860  47230\n",
      "  120474 182879 179998 111862   6954 108246  61601 146548  56534  82270\n",
      "  105942]]\n",
      "Epoch 11\tBatch 100\tTrain Loss:0.288\tTrain Accuracy:0.982\n",
      "Epoch 11\tBatch 200\tTrain Loss:0.290\tTrain Accuracy:0.979\n",
      "Epoch 11\tBatch 300\tTrain Loss:0.299\tTrain Accuracy:0.977\n",
      "Epoch 11\tBatch 400\tTrain Loss:0.300\tTrain Accuracy:0.977\n",
      "Epoch 11\tBatch 500\tTrain Loss:0.303\tTrain Accuracy:0.976\n",
      "Epoch 11\tBatch 600\tTrain Loss:0.306\tTrain Accuracy:0.975\n",
      "Epoch 11\tBatch 700\tTrain Loss:0.308\tTrain Accuracy:0.975\n",
      "going to increment epoch counter....\n",
      "11 5 False\n",
      "trainX[start:end]: [[ 84499 163337  86418 106166     75  99191 190473 132303 103259  98560\n",
      "    8410  52169   5917  91898 109880  71564 137419 142854 104291  66268\n",
      "   68699 163337  86418 106166     75  99191 190473 132303 103259  98560\n",
      "    8410  52169   5917  91898 109880  71564 137419 142854 104291  66268\n",
      "   68699 163337  86418 106166     75  99191 190473 132303 103259  98560\n",
      "    8410  52169   5917  91898 109880  71564 137419 142854 104291  66268\n",
      "   68699 163337  86418 106166     75  99191 190473 132303 103259  98560\n",
      "    8410  52169   5917  91898 109880  71564 137419 142854 104291  66268\n",
      "   68699]\n",
      " [ 20203  37100  66268  33155 149973 180992  76113  33088  88818  91898\n",
      "   39359   9472 101265  43175  73772 111862 186218 171867   1244 189255\n",
      "  106266  79454 143970  38933  71886 115064  69185    740  50127  29888\n",
      "  145314  93715  71747  99416 110841  84431  68699  68699  68699  68699\n",
      "   68699  37100  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  37100  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 12\tBatch 100\tTrain Loss:0.277\tTrain Accuracy:0.980\n",
      "Epoch 12\tBatch 200\tTrain Loss:0.273\tTrain Accuracy:0.981\n",
      "Epoch 12\tBatch 300\tTrain Loss:0.279\tTrain Accuracy:0.979\n",
      "Epoch 12\tBatch 400\tTrain Loss:0.284\tTrain Accuracy:0.978\n",
      "Epoch 12\tBatch 500\tTrain Loss:0.288\tTrain Accuracy:0.977\n",
      "Epoch 12\tBatch 600\tTrain Loss:0.294\tTrain Accuracy:0.976\n",
      "Epoch 12\tBatch 700\tTrain Loss:0.298\tTrain Accuracy:0.975\n",
      "going to increment epoch counter....\n",
      "12 5 False\n",
      "trainX[start:end]: [[ 84499   8410  73156  55481  22971 185821 143460  88242  86418 190695\n",
      "   23679  12566   5917 136632 102051  68699  68699  68699  68699  68699\n",
      "   68699   8410  73156  55481  22971 185821 143460  88242  86418 190695\n",
      "   23679  12566   5917 136632 102051  68699  68699  68699  68699  68699\n",
      "   68699   8410  73156  55481  22971 185821 143460  88242  86418 190695\n",
      "   23679  12566   5917 136632 102051  68699  68699  68699  68699  68699\n",
      "   68699   8410  73156  55481  22971 185821 143460  88242  86418 190695\n",
      "   23679  12566   5917 136632 102051  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [ 84499  66268  88726 151626   7532 109880  36648 137419 106839 178973\n",
      "  162925  52799 152737 172894 129416 124735 162384  91082 107638 114875\n",
      "   99085  86418 109925  66268  45725  71747 161662  94913 168211  58298\n",
      "    4999  79454 126497 129533  16791  59277   7289 102402 116692 190792\n",
      "   28080 179128 139834 113503  45077  38213 150435  56255 119900 185019\n",
      "   20334  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  56255  38213   7289  98971 119900 139834 121230 157691 179128\n",
      "   45077 113503  20334  74934  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 13\tBatch 100\tTrain Loss:0.281\tTrain Accuracy:0.981\n",
      "Epoch 13\tBatch 200\tTrain Loss:0.284\tTrain Accuracy:0.980\n",
      "Epoch 13\tBatch 300\tTrain Loss:0.285\tTrain Accuracy:0.980\n",
      "Epoch 13\tBatch 400\tTrain Loss:0.286\tTrain Accuracy:0.979\n",
      "Epoch 13\tBatch 500\tTrain Loss:0.288\tTrain Accuracy:0.978\n",
      "Epoch 13\tBatch 600\tTrain Loss:0.289\tTrain Accuracy:0.978\n",
      "Epoch 13\tBatch 700\tTrain Loss:0.292\tTrain Accuracy:0.977\n",
      "going to increment epoch counter....\n",
      "13 5 False\n",
      "trainX[start:end]: [[ 37336  82270  48968  77209  98379 155984  28217 162447 146401  67409\n",
      "  130096  63976 122179 111862  21048 116011 138864  32345  83777  85429\n",
      "  143460  82270  48968  77209  98379 155984  28217 162447 146401  67409\n",
      "  130096  63976 122179 111862  21048 116011 138864  32345  83777  85429\n",
      "  143460  82270  48968  77209  98379 155984  28217 162447 146401  67409\n",
      "  130096  63976 122179 111862  21048 116011 138864  32345  83777  85429\n",
      "  143460  82270  48968  77209  98379 155984  28217 162447 146401  67409\n",
      "  130096  63976 122179 111862  21048 116011 138864  32345  83777  85429\n",
      "  143460]\n",
      " [  5392  79454 121921  73676  90366  52089 103944  50127 145314  93715\n",
      "  141558  71747  99416 110841  68699  68699  68699  68699  68699  68699\n",
      "   68699  79454 121921  73676  90366  52089 103944  50127 145314  93715\n",
      "  141558  71747  99416 110841  68699  68699  68699  68699  68699  68699\n",
      "   68699  79454 121921  73676  90366  52089 103944  50127 145314  93715\n",
      "  141558  71747  99416 110841  68699  68699  68699  68699  68699  68699\n",
      "   68699  79454 121921  73676  90366  52089 103944  50127 145314  93715\n",
      "  141558  71747  99416 110841  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 14\tBatch 100\tTrain Loss:0.274\tTrain Accuracy:0.981\n",
      "Epoch 14\tBatch 200\tTrain Loss:0.276\tTrain Accuracy:0.980\n",
      "Epoch 14\tBatch 300\tTrain Loss:0.276\tTrain Accuracy:0.980\n",
      "Epoch 14\tBatch 400\tTrain Loss:0.276\tTrain Accuracy:0.980\n",
      "Epoch 14\tBatch 500\tTrain Loss:0.279\tTrain Accuracy:0.979\n",
      "Epoch 14\tBatch 600\tTrain Loss:0.277\tTrain Accuracy:0.979\n",
      "Epoch 14\tBatch 700\tTrain Loss:0.277\tTrain Accuracy:0.978\n",
      "going to increment epoch counter....\n",
      "14 5 False\n",
      "trainX[start:end]: [[185654  66268  59145 133219 135530   9367  46230  56415 176753  69671\n",
      "   71086  99901 164464 180992 137075  57528  48222  19422   5917 112749\n",
      "  151626  66268  59145 133219 135530   9367  46230  56415 176753  69671\n",
      "   71086  99901 164464 180992 137075  57528  48222  19422   5917 112749\n",
      "  151626  66268  59145 133219 135530   9367  46230  56415 176753  69671\n",
      "   71086  99901 164464 180992 137075  57528  48222  19422   5917 112749\n",
      "  151626  66268  59145 133219 135530   9367  46230  56415 176753  69671\n",
      "   71086  99901 164464 180992 137075  57528  48222  19422   5917 112749\n",
      "  151626]\n",
      " [ 84499   8410  45725 188249 163781  46239  44315  68836  12793  62073\n",
      "   30431  67994  22971  66548  23679   5917   9773 124316  37598 109880\n",
      "  137419   8410  45725 188249 163781  46239  44315  68836  12793  62073\n",
      "   30431  67994  22971  66548  23679   5917   9773 124316  37598 109880\n",
      "  137419   8410  45725 188249 163781  46239  44315  68836  12793  62073\n",
      "   30431  67994  22971  66548  23679   5917   9773 124316  37598 109880\n",
      "  137419   8410  45725 188249 163781  46239  44315  68836  12793  62073\n",
      "   30431  67994  22971  66548  23679   5917   9773 124316  37598 109880\n",
      "  137419]]\n",
      "Epoch 15\tBatch 100\tTrain Loss:0.217\tTrain Accuracy:0.988\n",
      "Epoch 15\tBatch 200\tTrain Loss:0.218\tTrain Accuracy:0.987\n",
      "Epoch 15\tBatch 300\tTrain Loss:0.214\tTrain Accuracy:0.987\n",
      "Epoch 15\tBatch 400\tTrain Loss:0.215\tTrain Accuracy:0.985\n",
      "Epoch 15\tBatch 500\tTrain Loss:0.216\tTrain Accuracy:0.985\n",
      "Epoch 15\tBatch 600\tTrain Loss:0.214\tTrain Accuracy:0.985\n",
      "Epoch 15\tBatch 700\tTrain Loss:0.214\tTrain Accuracy:0.985\n",
      "going to increment epoch counter....\n",
      "15 5 True\n",
      "Epoch 15 Validation Loss:0.170\tValidation Accuracy: 0.994\t time: 2018-01-05T18:35:57.873245\n",
      "trainX[start:end]: [[ 94085 130096 147992 125356   6116 186218 146352 161050 148214  23429\n",
      "  154120 111862  77209  37905 145617  95830 115009 179998 157120  90396\n",
      "  144308   6540  82314  87304  79454  28708 143269 140875  37905 124316\n",
      "  124236 120618  93715  94437    283  54240 110841 120695  91841 154609\n",
      "  108843  30183 148214 128028  58669  36972  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  30183 148214 128028  58669  36972  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [ 24267  31382  14831  99416 110841  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  31382  14831  99416 110841  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  31382  14831  99416 110841  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  31382  14831  99416 110841  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 16\tBatch 100\tTrain Loss:0.189\tTrain Accuracy:0.988\n",
      "Epoch 16\tBatch 200\tTrain Loss:0.189\tTrain Accuracy:0.988\n",
      "Epoch 16\tBatch 300\tTrain Loss:0.194\tTrain Accuracy:0.987\n",
      "Epoch 16\tBatch 400\tTrain Loss:0.195\tTrain Accuracy:0.987\n",
      "Epoch 16\tBatch 500\tTrain Loss:0.197\tTrain Accuracy:0.986\n",
      "Epoch 16\tBatch 600\tTrain Loss:0.197\tTrain Accuracy:0.986\n",
      "Epoch 16\tBatch 700\tTrain Loss:0.198\tTrain Accuracy:0.986\n",
      "going to increment epoch counter....\n",
      "16 5 False\n",
      "trainX[start:end]: [[181242 160917  90928  44269  89196 116086  79403  60653 103067    409\n",
      "   84715  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 160917  90928  44269  89196 116086  79403  60653 103067    409\n",
      "   84715  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 160917  90928  44269  89196 116086  79403  60653 103067    409\n",
      "   84715  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 160917  90928  44269  89196 116086  79403  60653 103067    409\n",
      "   84715  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [154334 167574 135625 188233 146749  63611 144308 171691  44651 177539\n",
      "   14263  45080  45274  98296  75165  39778 172607  10706 100038 131315\n",
      "   91827 167574 135625 188233 146749  63611 144308 171691  44651 177539\n",
      "   14263  45080  45274  98296  75165  39778 172607  10706 100038 131315\n",
      "   91827 167574 135625 188233 146749  63611 144308 171691  44651 177539\n",
      "   14263  45080  45274  98296  75165  39778 172607  10706 100038 131315\n",
      "   91827 167574 135625 188233 146749  63611 144308 171691  44651 177539\n",
      "   14263  45080  45274  98296  75165  39778 172607  10706 100038 131315\n",
      "   91827]]\n",
      "Epoch 17\tBatch 100\tTrain Loss:0.175\tTrain Accuracy:0.991\n",
      "Epoch 17\tBatch 200\tTrain Loss:0.174\tTrain Accuracy:0.991\n",
      "Epoch 17\tBatch 300\tTrain Loss:0.177\tTrain Accuracy:0.989\n",
      "Epoch 17\tBatch 400\tTrain Loss:0.179\tTrain Accuracy:0.989\n",
      "Epoch 17\tBatch 500\tTrain Loss:0.180\tTrain Accuracy:0.989\n",
      "Epoch 17\tBatch 600\tTrain Loss:0.182\tTrain Accuracy:0.988\n",
      "Epoch 17\tBatch 700\tTrain Loss:0.185\tTrain Accuracy:0.987\n",
      "going to increment epoch counter....\n",
      "17 5 False\n",
      "trainX[start:end]: [[168136  31611  12287  97113 148386 177084 116011 161050 156052 125378\n",
      "  180992  88157 189222 126916  24050 112749  37009 139275  81330 100131\n",
      "   84219  33088 161247 116439  87304 126920 100489  71249 127398 168794\n",
      "  164081 154794 139823  52395 175674 128539  72225 149243  31201  93558\n",
      "   23233 141402 168136 168794  31201  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 129135 141402 168136 168794  31201  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [129938   2996  66268 129938  80826  39359 126778 189755 104450 163550\n",
      "  159019 107638 109791 120320 108896 124389 156947  77805  48968  81097\n",
      "    2932 189608  99416  79454  70409 155314 136603  52089  10216 164186\n",
      "   28708  20181 149576 129489   7389 110841  71886 115064  50127 124316\n",
      "   90475 123533 129938  56255 120958  68699  68699  68699  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699 138457 122431   3977 123533 178684 122201  56255 105715  27078\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 18\tBatch 100\tTrain Loss:0.178\tTrain Accuracy:0.990\n",
      "Epoch 18\tBatch 200\tTrain Loss:0.180\tTrain Accuracy:0.989\n",
      "Epoch 18\tBatch 300\tTrain Loss:0.179\tTrain Accuracy:0.989\n",
      "Epoch 18\tBatch 400\tTrain Loss:0.180\tTrain Accuracy:0.988\n",
      "Epoch 18\tBatch 500\tTrain Loss:0.182\tTrain Accuracy:0.988\n",
      "Epoch 18\tBatch 600\tTrain Loss:0.185\tTrain Accuracy:0.987\n",
      "Epoch 18\tBatch 700\tTrain Loss:0.188\tTrain Accuracy:0.986\n",
      "going to increment epoch counter....\n",
      "18 5 False\n",
      "trainX[start:end]: [[ 90173 111299 117304 146060  55737 128578  79454  52089  71476  59841\n",
      "   50127 124316 145314  93715 141558  71747 190368  99416 110841  68699\n",
      "   68699 111299 117304 146060  55737 128578  79454  52089  71476  59841\n",
      "   50127 124316 145314  93715 141558  71747 190368  99416 110841  68699\n",
      "   68699 111299 117304 146060  55737 128578  79454  52089  71476  59841\n",
      "   50127 124316 145314  93715 141558  71747 190368  99416 110841  68699\n",
      "   68699 111299 117304 146060  55737 128578  79454  52089  71476  59841\n",
      "   50127 124316 145314  93715 141558  71747 190368  99416 110841  68699\n",
      "   68699]\n",
      " [ 78309  66268  25127  62073 157635 122531  82270  67994 169924 143053\n",
      "  108246  79009 116011  31611 180992 102769 154220  76024  39662  78459\n",
      "   98361  66268  25127  62073 157635 122531  82270  67994 169924 143053\n",
      "  108246  79009 116011  31611 180992 102769 154220  76024  39662  78459\n",
      "   98361  66268  25127  62073 157635 122531  82270  67994 169924 143053\n",
      "  108246  79009 116011  31611 180992 102769 154220  76024  39662  78459\n",
      "   98361  66268  25127  62073 157635 122531  82270  67994 169924 143053\n",
      "  108246  79009 116011  31611 180992 102769 154220  76024  39662  78459\n",
      "   98361]]\n",
      "Epoch 19\tBatch 100\tTrain Loss:0.175\tTrain Accuracy:0.991\n",
      "Epoch 19\tBatch 200\tTrain Loss:0.176\tTrain Accuracy:0.990\n",
      "Epoch 19\tBatch 300\tTrain Loss:0.177\tTrain Accuracy:0.989\n",
      "Epoch 19\tBatch 400\tTrain Loss:0.178\tTrain Accuracy:0.989\n",
      "Epoch 19\tBatch 500\tTrain Loss:0.176\tTrain Accuracy:0.989\n",
      "Epoch 19\tBatch 600\tTrain Loss:0.177\tTrain Accuracy:0.988\n",
      "Epoch 19\tBatch 700\tTrain Loss:0.177\tTrain Accuracy:0.988\n",
      "going to increment epoch counter....\n",
      "19 5 False\n",
      "trainX[start:end]: [[ 84499   1182 102769 124316  93056 189608 131554 131787  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699   1182 102769 124316  93056 189608 131554 131787  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699   1182 102769 124316  93056 189608 131554 131787  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699   1182 102769 124316  93056 189608 131554 131787  68699  68699\n",
      "   68699  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]\n",
      " [  9673  46239 131907 189268   9773 124316  26756 137419  76710  66268\n",
      "  154609  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  46239 131907 189268   9773 124316  26756 137419  76710  66268\n",
      "  154609  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  46239 131907 189268   9773 124316  26756 137419  76710  66268\n",
      "  154609  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699  46239 131907 189268   9773 124316  26756 137419  76710  66268\n",
      "  154609  68699  68699  68699  68699  68699  68699  68699  68699  68699\n",
      "   68699]]\n",
      "Epoch 20\tBatch 100\tTrain Loss:0.161\tTrain Accuracy:0.990\n",
      "Epoch 20\tBatch 200\tTrain Loss:0.158\tTrain Accuracy:0.991\n",
      "Epoch 20\tBatch 300\tTrain Loss:0.156\tTrain Accuracy:0.991\n",
      "Epoch 20\tBatch 400\tTrain Loss:0.154\tTrain Accuracy:0.991\n",
      "Epoch 20\tBatch 500\tTrain Loss:0.153\tTrain Accuracy:0.991\n",
      "Epoch 20\tBatch 600\tTrain Loss:0.153\tTrain Accuracy:0.991\n",
      "Epoch 20\tBatch 700\tTrain Loss:0.152\tTrain Accuracy:0.991\n",
      "going to increment epoch counter....\n",
      "20 5 True\n",
      "Epoch 20 Validation Loss:0.124\tValidation Accuracy: 0.996\t time: 2018-01-05T18:56:37.195541\n"
     ]
    }
   ],
   "source": [
    "    import random\n",
    "    # 3.create session.\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        # Instantiate Model\n",
    "        filter_sizes = [3,4,5]\n",
    "        cnnDisease = CNNDisease(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size,\n",
    "                          FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length, vocab_size, FLAGS.embed_size,\n",
    "                          FLAGS.is_training)\n",
    "        # Initialize Save\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "        print('Initializing Variables')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        if FLAGS.use_embedding:  # load pre-trained word embedding\n",
    "            assign_pretrained_word_embedding(sess, cnnDisease, model)\n",
    "        curr_epoch = sess.run(cnnDisease.epoch_step)\n",
    "       \n",
    "        print(\"curr_epoch=\", curr_epoch)\n",
    "        number_of_training_data = len(trainX)\n",
    "        print(\"number_of_training_data=\",number_of_training_data)\n",
    "       \n",
    "        batch_size = FLAGS.batch_size\n",
    "        print(\"batch_size=\", batch_size)\n",
    "        #4 feed data\n",
    "        for epoch in range(curr_epoch, FLAGS.num_epochs + 1):\n",
    "            loss, acc, counter = 0.0, 0.0, 0\n",
    "            '''\n",
    "            indexList = [i for i in range(number_of_training_data)]\n",
    "            random.shuffle(indexList)\n",
    "            x = trainX\n",
    "            y = trainY\n",
    "            for i in range(number_of_training_data):\n",
    "                x[i] = trainX[indexList[i]]\n",
    "                y[i] = trainY[indexList[i]]\n",
    "            trainX = x\n",
    "            trainY = y\n",
    "            '''\n",
    "            #每个epoch ，shuffle数据\n",
    "            np.random.seed(10) \n",
    "            shuffle_indices = np.random.permutation(np.arange(number_of_training_data))\n",
    "            x = trainX[shuffle_indices]\n",
    "            y = trainY[shuffle_indices]\n",
    "            trainX = x\n",
    "            trainY = y\n",
    "                                                    \n",
    "            for start, end in zip(range(0, number_of_training_data, batch_size),\n",
    "                                  range(batch_size, number_of_training_data, batch_size)):\n",
    "                if counter == 0:\n",
    "                    print(\"trainX[start:end]:\", trainX[start:2])  # ;print(\"trainY[start:end]:\",trainY[start:end])\n",
    "                #use the word index as the input \n",
    "                feed_dict = {cnnDisease.input_x: trainX[start:end], cnnDisease.dropout_keep_prob: FLAGS.dropout_keep_prob}\n",
    "\n",
    "                feed_dict[cnnDisease.input_y] = trainY[start:end]\n",
    "                #5 training \n",
    "                curr_loss, curr_acc, _ = sess.run([cnnDisease.loss_val, cnnDisease.accuracy, cnnDisease.train_op],\n",
    "                                                 feed_dict)\n",
    "                \n",
    "                loss, counter, acc = loss + curr_loss, counter + 1, acc + curr_acc\n",
    "                if counter % 100 == 0:\n",
    "                    print(\"Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f\" % (\n",
    "                    epoch, counter, loss / float(counter), acc / float(counter)))\n",
    "\n",
    "            # epoch increment\n",
    "            print(\"going to increment epoch counter....\")\n",
    "            sess.run(cnnDisease.epoch_increment)\n",
    "        \n",
    "             # 6.validation\n",
    "            print(epoch, FLAGS.validate_every, (epoch % FLAGS.validate_every == 0))\n",
    "            if epoch % FLAGS.validate_every == 0:\n",
    "                eval_loss, eval_acc = do_eval(sess, cnnDisease, testX, testY, batch_size)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f\\t time: %s\" % (\n",
    "                epoch, eval_loss, eval_acc, time_str))\n",
    "                # save model to checkpoint\n",
    "                if not os.path.exists(FLAGS.ckpt_dir + \"checkpoint\"):\n",
    "                    os.makedirs(FLAGS.ckpt_dir)\n",
    "                save_path = FLAGS.ckpt_dir + \"model.ckpt\"\n",
    "                saver.save(sess, save_path, global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
