{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "from CNNDiseaseModel import CNNDisease\n",
    "from CNNdata_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_integer(\"num_filters\", 32,\"number of filter each filter_size\")\n",
    "tf.flags.DEFINE_integer(\"num_classes\", 22, \"number of labels\")\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.01, \"learning rate\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"batch size for train or evaluate\")\n",
    "tf.flags.DEFINE_integer(\"sequence_length\", 21, \"max sequence_length\")\n",
    "tf.flags.DEFINE_integer(\"embed_size\",100, \"embedding size\" )\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 15, \"number of epochs to train.\")\n",
    "tf.flags.DEFINE_boolean(\"is_training\", True,\"if is train step\")\n",
    "tf.app.flags.DEFINE_integer(\"decay_steps\", 3500, \"how many steps before decay learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"decay_rate\", 0.65, \"Rate of decay for learning rate.\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"train_data_path\", \"./cnnModel/datasets/train.feature\",\n",
    "                           \"path of traning data.\")\n",
    "tf.app.flags.DEFINE_string(\"train_label_path\", \"./cnnModel/datasets/train.label\",\n",
    "                           \"path of labels of traning data.\")\n",
    "tf.flags.DEFINE_string(\"test_data_path\", \"./cnnModel/datasets/test.feature\", \"Test data source\")\n",
    "tf.flags.DEFINE_string(\"test_label_path\", \"./cnnModel/datasets/test.label\", \"Label for test data\")\n",
    "tf.app.flags.DEFINE_string(\"word2vec_model_path\", \"./model/word2VecModelsh.bin5_100_1e-05_15\",\n",
    "                           \"word2vec's vocabulary and vectors\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"number of checkpoints\")\n",
    "tf.flags.DEFINE_boolean(\"use_embedding\", True,\"if use pre trained word2vec embedding\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"dropout_keep_prob\")\n",
    "tf.flags.DEFINE_integer(\"validate_every\", 5, \"Validate every validate_every epochs.\")\n",
    "tf.flags.DEFINE_string(\"ckpt_dir\", \"./runs/cnn_disease_checkpoint3in/\",\n",
    "                           \"checkpoint location for the model\")\n",
    "FLAGS = tf.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据word2vec模型构建word index 和index word映射词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path, name_scope=''):\n",
    "    cache_path = './cache_vocabulary_label_pik/' + name_scope + \"_word_voabulary.pik\"\n",
    "    #print(\"cache_path:\", cache_path, \"file_exists:\", os.path.exists(cache_path))\n",
    "    # load the cache file if exists\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as data_f:\n",
    "            vocabulary_word2index, vocabulary_index2word = pickle.load(data_f)\n",
    "            return vocabulary_word2index, vocabulary_index2word\n",
    "    else:\n",
    "        vocabulary_word2index = {}\n",
    "        vocabulary_index2word = {}\n",
    "        model = Word2Vec.load(word2vec_model_path)\n",
    "        print(\"vocabulary:\", len(model.wv.vocab))\n",
    "        for i, vocab in enumerate(model.wv.vocab):\n",
    "            vocabulary_word2index[vocab] = i + 1\n",
    "            vocabulary_index2word[i + 1] = vocab\n",
    "\n",
    "        # save to file system if vocabulary of words is not exists.\n",
    "        print(len(vocabulary_word2index))\n",
    "        if not os.path.exists(cache_path):\n",
    "            with open(cache_path, 'wb') as data_f:\n",
    "                pickle.dump((vocabulary_word2index, vocabulary_index2word), data_f)\n",
    "    return vocabulary_word2index, vocabulary_index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将训练好的词向量赋值给tf embedding_lookup的 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_pretrained_word_embedding(sess, cnnDisease, word2vec_model):\n",
    "    print(\"using pre-trained word emebedding.started.word2vec_model_path:\", FLAGS.word2vec_model_path)\n",
    "    word2vec_dict = {}\n",
    "    vocab_size = len(word2vec_model.wv.index2word)\n",
    "    print(\"vocab_size=\",vocab_size)\n",
    "    \n",
    "    word_embedding_2dlist = [[]] * (vocab_size+1)  # create an empty word_embedding list.\n",
    "    bound = np.sqrt(6.0) / np.sqrt(vocab_size)\n",
    "    count_exist = 0\n",
    "    count_not_exist = 0\n",
    "    word_embedding_2dlist[0] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n",
    "    for i, word in enumerate(model.wv.vocab):\n",
    "    #for i in range(vocab_size):\n",
    "        #word = word2vec_model.wv.index2word[i]\n",
    "        embedding = None\n",
    "        try:\n",
    "            embedding = word2vec_model.wv[word]\n",
    "        except:\n",
    "            embedding = None\n",
    "        if embedding is not None:\n",
    "            word_embedding_2dlist[i+1] = embedding\n",
    "            count_exist += 1\n",
    "        else:\n",
    "            word_embedding_2dlist[i+1] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n",
    "            count_not_exist += 1\n",
    "        \n",
    "    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n",
    "    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n",
    "    t_assign_embedding = tf.assign(cnnDisease.Embedding,\n",
    "                                   word_embedding)  # assign this value to our embedding variables of our model.\n",
    "    sess.run(t_assign_embedding)\n",
    "    print(\"word. exists embedding:\", count_exist, \" ;word not exist embedding:\", count_not_exist)\n",
    "    print(\"using pre-trained word emebedding.ended...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 载入训练和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(train_data_path, train_label_path, test_data_path, test_label_path, vocabulary_word2index):\n",
    "    print(\"Loading data...\")\n",
    "    x_train, y_train = loadTrainOrTest_data(train_data_path, train_label_path,vocabulary_word2index)\n",
    "    x_test, y_test = loadTrainOrTest_data(test_data_path, test_label_path, vocabulary_word2index)\n",
    "    train = (x_train, y_train)\n",
    "    test = (x_test, y_test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型效果评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make evaluation on test set\n",
    "def do_eval(sess, cnnDisease, evalX, evalY, batch_size):\n",
    "    number_examples = len(evalX)\n",
    "    eval_loss, eval_acc, eval_counter = 0.0, 0.0, 0\n",
    "    for start, end in zip(range(0, number_examples, batch_size), range(batch_size, number_examples, batch_size)):\n",
    "        feed_dict = {cnnDisease.input_x: evalX[start:end], cnnDisease.dropout_keep_prob: 1.0}\n",
    "        feed_dict[cnnDisease.input_y] = evalY[start:end]\n",
    "        curr_eval_loss, logits, curr_eval_acc = sess.run([cnnDisease.loss_val, cnnDisease.logits, cnnDisease.accuracy],\n",
    "                                                         feed_dict)\n",
    "        eval_loss, eval_acc, eval_counter = eval_loss + curr_eval_loss, eval_acc + curr_eval_acc, eval_counter + 1\n",
    "    return eval_loss / float(eval_counter), eval_acc / float(eval_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.载入训练好的word2vec模型和构建映射词典，载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_word2index.vocab_size: 192413\n",
      "cnn_model.vocab_size: 192413\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "    # 1.load data(X:list of lint,y:int).\n",
    "    model = Word2Vec.load(FLAGS.word2vec_model_path)\n",
    "    vocabulary_word2index, vocabulary_index2word = create_voabulary(\n",
    "        word2vec_model_path=FLAGS.word2vec_model_path, \n",
    "        name_scope=\"cnn\")  # simple='simple'\n",
    "    vocab_size = len(vocabulary_word2index)\n",
    "    print(\"vocabulary_word2index.vocab_size:\", vocab_size)\n",
    "    vocab_size = len(model.wv.index2word)\n",
    "    print(\"cnn_model.vocab_size:\", vocab_size)\n",
    "    trainX, trainY, testX, testY = None, None, None, None\n",
    "    train, test = load_data(train_data_path=FLAGS.train_data_path, train_label_path=FLAGS.train_label_path,\n",
    "                            test_data_path=FLAGS.test_data_path, test_label_path=FLAGS.test_label_path,\n",
    "                            vocabulary_word2index=vocabulary_word2index)\n",
    "    trainX, trainY = train\n",
    "    testX, testY = test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据预处理，所有输入处理成相同长度，打pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start padding & transform to one hot...\n",
      "maxlen= 21\n",
      "maxlen= 21\n"
     ]
    }
   ],
   "source": [
    "    # 2.Data preprocessing.Sequence padding\n",
    "    print(\"start padding & transform to one hot...\")\n",
    "    trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length)  # padding to max length\n",
    "    testX = pad_sequences(testX, maxlen=FLAGS.sequence_length)  # padding to max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[2, 6, 7, 3, 9, 1, 5, 0, 8, 4]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "x = [i for i in range(10) ]\n",
    "print(x)\n",
    "random.shuffle(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learing rate: <tf.Variable 'learning_rate:0' shape=() dtype=float32_ref>\n",
      "global_step: <tf.Variable 'global_step:0' shape=() dtype=int32_ref>\n",
      "decay_steps: 3500\n",
      "decay_rate: 0.65\n",
      "decay_steps=3500 decay_rate=0.650000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a0c7879c459d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                       FLAGS.is_training)\n\u001b[1;32m     11\u001b[0m      \u001b[0;31m# Output directory for models and summaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mout_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"runs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Writing to {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "    import random\n",
    "    import time\n",
    "    # 3.create session.\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        # Instantiate Model\n",
    "        filter_sizes = [3,4,5]\n",
    "        cnnDisease = CNNDisease(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size,\n",
    "                          FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length, vocab_size, FLAGS.embed_size,\n",
    "                          FLAGS.is_training)\n",
    "         # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnnDisease.loss_val)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnnDisease.accuracy)\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        \n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "        \n",
    "        # Initialize Save\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "        print('Initializing Variables')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        if FLAGS.use_embedding:  # load pre-trained word embedding\n",
    "            assign_pretrained_word_embedding(sess, cnnDisease, model)\n",
    "        curr_epoch = sess.run(cnnDisease.epoch_step)\n",
    "       \n",
    "        print(\"curr_epoch=\", curr_epoch)\n",
    "        number_of_training_data = len(trainX)\n",
    "        print(\"number_of_training_data=\",number_of_training_data)\n",
    "       \n",
    "        batch_size = FLAGS.batch_size\n",
    "        print(\"batch_size=\", batch_size)\n",
    "        #4 feed data\n",
    "        for epoch in range(curr_epoch, FLAGS.num_epochs + 1):\n",
    "            loss, acc, counter = 0.0, 0.0, 0\n",
    "            '''\n",
    "            indexList = [i for i in range(number_of_training_data)]\n",
    "            random.shuffle(indexList)\n",
    "            x = trainX\n",
    "            y = trainY\n",
    "            for i in range(number_of_training_data):\n",
    "                x[i] = trainX[indexList[i]]\n",
    "                y[i] = trainY[indexList[i]]\n",
    "            trainX = x\n",
    "            trainY = y\n",
    "            '''\n",
    "            #每个epoch ，shuffle数据\n",
    "            np.random.seed(10) \n",
    "            shuffle_indices = np.random.permutation(np.arange(number_of_training_data))\n",
    "            x = trainX[shuffle_indices]\n",
    "            y = trainY[shuffle_indices]\n",
    "            trainX = x\n",
    "            trainY = y\n",
    "                                                    \n",
    "            for start, end in zip(range(0, number_of_training_data, batch_size),\n",
    "                                  range(batch_size, number_of_training_data, batch_size)):\n",
    "                if counter == 0:\n",
    "                    print(\"trainX[start:end]:\", trainX[start:5])  # ;print(\"trainY[start:end]:\",trainY[start:end])\n",
    "                #use the word index as the input \n",
    "                feed_dict = {cnnDisease.input_x: trainX[start:end], cnnDisease.dropout_keep_prob: FLAGS.dropout_keep_prob}\n",
    "\n",
    "                feed_dict[cnnDisease.input_y] = trainY[start:end]\n",
    "                \n",
    "                #5 training \n",
    "                curr_loss, curr_acc, summaries,_ = sess.run([cnnDisease.loss_val, cnnDisease.accuracy,train_summary_op, cnnDisease.train_op],\n",
    "                                                 feed_dict)\n",
    "                train_summary_writer.add_summary(summaries, epoch)\n",
    "\n",
    "                \n",
    "                loss, counter, acc = loss + curr_loss, counter + 1, acc + curr_acc\n",
    "                if counter % 100 == 0:\n",
    "                    print(\"Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f\" % (\n",
    "                    epoch, counter, loss / float(counter), acc / float(counter)))\n",
    "\n",
    "            # epoch increment\n",
    "            print(\"going to increment epoch counter....\")\n",
    "            sess.run(cnnDisease.epoch_increment)\n",
    "        \n",
    "             # 6.validation\n",
    "            print(epoch, FLAGS.validate_every, (epoch % FLAGS.validate_every == 0))\n",
    "            if epoch % FLAGS.validate_every == 0:\n",
    "                eval_loss, eval_acc = do_eval(sess, cnnDisease, testX, testY, batch_size)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f\\t time: %s\" % (\n",
    "                epoch, eval_loss, eval_acc, time_str))\n",
    "                # save model to checkpoint\n",
    "                if not os.path.exists(FLAGS.ckpt_dir + \"checkpoint\"):\n",
    "                    os.makedirs(FLAGS.ckpt_dir)\n",
    "                save_path = FLAGS.ckpt_dir + \"model.ckpt\"\n",
    "                saver.save(sess, save_path, global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "global_steps = 3000\n",
    "learning_rate = 0.01\n",
    "decay_rate = 0.9#0.65\n",
    "decay_steps = 50#3500\n",
    "# 指数学习率衰减过程\n",
    "for global_step in range(global_steps):\n",
    "    decayed_learning_rate = learning_rate * decay_rate**(global_step / decay_steps)\n",
    "    X.append(global_step / decay_steps)\n",
    "    Y.append(decayed_learning_rate)\n",
    "    print(\"global step: %d, learning rate: %f\" % (global_step,decayed_learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(1)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "curve = ax.plot(X,Y,'r',label=\"learning rate\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"global_step / decay_steps\")\n",
    "ax.set_ylabel(\"learning_rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
