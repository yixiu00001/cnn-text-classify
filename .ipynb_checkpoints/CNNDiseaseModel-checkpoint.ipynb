{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*-coding utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNDisease:\n",
    "    def __init__(self, filter_sizes, num_filters, num_classes, learning_rate,\n",
    "                batch_size, decay_steps, decay_rate, sequence_length, vocab_size, embed_size,\n",
    "                is_training, clip_gradiencets=0.5, decay_rate_big=5.0):\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n",
    "        self.num_filters_total = self.num_filters*len(filter_sizes)\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        \n",
    "        self.is_training = is_training\n",
    "        self.clip_gradiencets = clip_gradiencets\n",
    "        \n",
    "        #每行输入为词数为sequence_length（实验是21），每次输入行数不定，输出label行数也不定\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32,[None], name=\"input_y\")\n",
    "        #drop选取节点失活的概率\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32,name=\"dropout_keep_prob\")\n",
    "        \n",
    "        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=\"learning_rate\")\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "        self.epoch_step = tf.Variable(0, trainable=False, name=\"epoch_step\")\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n",
    "        \n",
    "        self.initializer = initializer\n",
    "        #初始化embedding weight，输出层的w b\n",
    "        self.initialize_weights()\n",
    "        \n",
    "        if not is_training:\n",
    "            return\n",
    "        #预测结果\n",
    "        self.logits,self.predicts = self.inference()\n",
    "        #计算loss\n",
    "        self.loss_val = self.loss()\n",
    "        #计算准确率\n",
    "        self.accuracy = self.accuracy()\n",
    "     \n",
    "        self.train_op = self.train()\n",
    "        \n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.Embedding = tf.get_variable(\n",
    "                \"Embedding\", \n",
    "                shape=[self.vocab_size+1, self.embed_size], \n",
    "                initializer=self.initializer) \n",
    "            self.w_projection = tf.get_variable(\n",
    "                \"w_projection\",\n",
    "                shape=[self.num_filters_total, self.num_classes],\n",
    "                initializer = self.initializer)\n",
    "            self.b_projection = tf.get_variable(\n",
    "                \"b_projection\",\n",
    "                shape=[self.num_classes])\n",
    "    def inference(self):\n",
    "        #1.get embedding word vector\n",
    "        self.embedded_words = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
    "        self.sentence_embedded_expanded = tf.expand_dims(self.embedded_words, -1)\n",
    "        #2.loop each filter size conv->relu->pool\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"convolution-pool=%s\" %(filter_size)):\n",
    "                filter = tf.get_variable(\n",
    "                    \"filter-%s\" %(fiter_size), \n",
    "                    [filter_size, self.embed_size,1, self.num_filters ],\n",
    "                    initializer = self.initializer)\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.sentence_embedded_expanded,\n",
    "                    filter,\n",
    "                    strides=[1,1,1,1],\n",
    "                    padding = \"VALID\",\n",
    "                    name = \"conv\")\n",
    "                b = tf.get_variable(\"b-%s\" %(filter_size), [self.num_filters])\n",
    "                \n",
    "                h = tf.nn.relu(\n",
    "                    tf.nn.bias_add(conv, b), \n",
    "                    \"relu\")\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h, \n",
    "                    ksize=[1, self.sequence_length-filter_size+1,1,1],\n",
    "                    strides=[1,1,1,1],\n",
    "                    padding = \"VALID\",\n",
    "                    NAME = \"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        selg.pooled_flat = tf.reshape(self.h_pool, [-1, self.num_filters_total])\n",
    "            \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(\n",
    "                self.pooled_flat, \n",
    "                [-1, self.num_filters_total])\n",
    "        with tf.name_scope(\"output\"):\n",
    "            logits = tf.matmul(self.h_drop, self.w_projection)+self.b_projection\n",
    "            predicts = tf.argmax(logits, 1, name=\"predictions\")\n",
    "        return logits, predicts\n",
    "    def loss(self, l2_lambda=0.0001):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            #labels:[batch_size], logits:[batch_size, num_classes]\n",
    "            #\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self.input_y,\n",
    "                logits=self.logits)\n",
    "            loss = tf.reduce_mean(losses)\n",
    "            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables if 'bias' not in v.name] )* l2_lambda\n",
    "            loss = loss + l2_loss\n",
    "            return loss\n",
    "    def accuracy(self):\n",
    "        prediction_now = tf.equal(tf.cast(self.predicts, tf.int32), self.input_y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(prediction_now,tf.float32), name=\"accuracy\")\n",
    "        return accuracy\n",
    "    def train(self):\n",
    "        #慢慢减小learn_rate\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
    "                self.decay_steps, self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, \n",
    "                global_step = self.global_step,\n",
    "                learning_rate = learning_rate,\n",
    "                optimizer=\"Adam\",\n",
    "                 clip_gradients=self.clip_gradients)\n",
    "        return train_op"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
