{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "from CNNDiseaseModel import CNNDisease\n",
    "from CNNdata_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_integer(\"num_filters\", 32,\"number of filter each filter_size\")\n",
    "tf.flags.DEFINE_integer(\"num_classes\", 22, \"number of labels\")\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.01, \"learning rate\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"batch size for train or evaluate\")\n",
    "tf.flags.DEFINE_integer(\"sequence_length\", 21, \"max sequence_length\")\n",
    "tf.flags.DEFINE_integer(\"embed_size\",100, \"embedding size\" )\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 2, \"number of epochs to train.\")\n",
    "tf.flags.DEFINE_boolean(\"is_training\", True,\"if is train step\")\n",
    "tf.app.flags.DEFINE_integer(\"decay_steps\", 3500, \"how many steps before decay learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"decay_rate\", 0.65, \"Rate of decay for learning rate.\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"train_data_path\", \"./cnnModel/datasets/train.feature\",\n",
    "                           \"path of traning data.\")\n",
    "tf.app.flags.DEFINE_string(\"train_label_path\", \"./cnnModel/datasets/train.label\",\n",
    "                           \"path of labels of traning data.\")\n",
    "tf.flags.DEFINE_string(\"test_data_path\", \"./cnnModel/datasets/test.feature\", \"Test data source\")\n",
    "tf.flags.DEFINE_string(\"test_label_path\", \"./cnnModel/datasets/test.label\", \"Label for test data\")\n",
    "tf.app.flags.DEFINE_string(\"word2vec_model_path\", \"./model/word2VecModelsh.bin5_100_1e-05_15\",\n",
    "                           \"word2vec's vocabulary and vectors\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"number of checkpoints\")\n",
    "tf.flags.DEFINE_boolean(\"use_embedding\", False,\"if use pre trained word2vec embedding\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"dropout_keep_prob\")\n",
    "tf.flags.DEFINE_integer(\"validate_every\", 1, \"Validate every validate_every epochs.\")\n",
    "tf.flags.DEFINE_string(\"ckpt_dir\", \"./runs/cnn_disease_checkpoint3in/\",\n",
    "                           \"checkpoint location for the model\")\n",
    "FLAGS = tf.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据word2vec模型构建word index 和index word映射词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path, name_scope=''):\n",
    "    cache_path = './cache_vocabulary_label_pik/' + name_scope + \"_word_voabulary.pik\"\n",
    "    #print(\"cache_path:\", cache_path, \"file_exists:\", os.path.exists(cache_path))\n",
    "    # load the cache file if exists\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as data_f:\n",
    "            vocabulary_word2index, vocabulary_index2word = pickle.load(data_f)\n",
    "            return vocabulary_word2index, vocabulary_index2word\n",
    "    else:\n",
    "        vocabulary_word2index = {}\n",
    "        vocabulary_index2word = {}\n",
    "        model = Word2Vec.load(word2vec_model_path)\n",
    "        print(\"vocabulary:\", len(model.wv.vocab))\n",
    "        for i, vocab in enumerate(model.wv.vocab):\n",
    "            vocabulary_word2index[vocab] = i + 1\n",
    "            vocabulary_index2word[i + 1] = vocab\n",
    "\n",
    "        # save to file system if vocabulary of words is not exists.\n",
    "        print(len(vocabulary_word2index))\n",
    "        if not os.path.exists(cache_path):\n",
    "            with open(cache_path, 'wb') as data_f:\n",
    "                pickle.dump((vocabulary_word2index, vocabulary_index2word), data_f)\n",
    "    return vocabulary_word2index, vocabulary_index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将训练好的词向量赋值给tf embedding_lookup的 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_pretrained_word_embedding(sess, cnnDisease, word2vec_model):\n",
    "    print(\"using pre-trained word emebedding.started.word2vec_model_path:\", FLAGS.word2vec_model_path)\n",
    "    word2vec_dict = {}\n",
    "    vocab_size = len(word2vec_model.wv.index2word)\n",
    "    print(\"vocab_size=\",vocab_size)\n",
    "    \n",
    "    word_embedding_2dlist = [[]] * (vocab_size+1)  # create an empty word_embedding list.\n",
    "    bound = np.sqrt(6.0) / np.sqrt(vocab_size)\n",
    "    count_exist = 0\n",
    "    count_not_exist = 0\n",
    "    word_embedding_2dlist[0] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n",
    "    for i, word in enumerate(model.wv.vocab):\n",
    "    #for i in range(vocab_size):\n",
    "        #word = word2vec_model.wv.index2word[i]\n",
    "        embedding = None\n",
    "        try:\n",
    "            embedding = word2vec_model.wv[word]\n",
    "        except:\n",
    "            embedding = None\n",
    "        if embedding is not None:\n",
    "            word_embedding_2dlist[i+1] = embedding\n",
    "            count_exist += 1\n",
    "        else:\n",
    "            word_embedding_2dlist[i+1] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n",
    "            count_not_exist += 1\n",
    "        \n",
    "    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n",
    "    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n",
    "    t_assign_embedding = tf.assign(cnnDisease.Embedding,\n",
    "                                   word_embedding)  # assign this value to our embedding variables of our model.\n",
    "    sess.run(t_assign_embedding)\n",
    "    print(\"word. exists embedding:\", count_exist, \" ;word not exist embedding:\", count_not_exist)\n",
    "    print(\"using pre-trained word emebedding.ended...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 载入训练和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(train_data_path, train_label_path, test_data_path, test_label_path, vocabulary_word2index):\n",
    "    print(\"Loading data...\")\n",
    "    x_train, y_train = loadTrainOrTest_data(train_data_path, train_label_path,vocabulary_word2index)\n",
    "    x_test, y_test = loadTrainOrTest_data(test_data_path, test_label_path, vocabulary_word2index)\n",
    "    train = (x_train, y_train)\n",
    "    test = (x_test, y_test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型效果评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make evaluation on test set\n",
    "def do_eval(sess, cnnDisease, evalX, evalY, batch_size):\n",
    "    number_examples = len(evalX)\n",
    "    eval_loss, eval_acc, eval_counter = 0.0, 0.0, 0\n",
    "    for start, end in zip(range(0, number_examples, batch_size), range(batch_size, number_examples, batch_size)):\n",
    "        feed_dict = {cnnDisease.input_x: evalX[start:end], cnnDisease.dropout_keep_prob: 1.0}\n",
    "        feed_dict[cnnDisease.input_y] = evalY[start:end]\n",
    "        curr_eval_loss, logits, curr_eval_acc = sess.run([cnnDisease.loss_val, cnnDisease.logits, cnnDisease.accuracy],\n",
    "                                                         feed_dict)\n",
    "        eval_loss, eval_acc, eval_counter = eval_loss + curr_eval_loss, eval_acc + curr_eval_acc, eval_counter + 1\n",
    "    return eval_loss / float(eval_counter), eval_acc / float(eval_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.载入训练好的word2vec模型和构建映射词典，载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_word2index.vocab_size: 192413\n",
      "cnn_model.vocab_size: 192413\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "    # 1.load data(X:list of lint,y:int).\n",
    "    model = Word2Vec.load(FLAGS.word2vec_model_path)\n",
    "    vocabulary_word2index, vocabulary_index2word = create_voabulary(\n",
    "        word2vec_model_path=FLAGS.word2vec_model_path, \n",
    "        name_scope=\"cnn\")  # simple='simple'\n",
    "    vocab_size = len(vocabulary_word2index)\n",
    "    print(\"vocabulary_word2index.vocab_size:\", vocab_size)\n",
    "    vocab_size = len(model.wv.index2word)\n",
    "    print(\"cnn_model.vocab_size:\", vocab_size)\n",
    "    trainX, trainY, testX, testY = None, None, None, None\n",
    "    train, test = load_data(train_data_path=FLAGS.train_data_path, train_label_path=FLAGS.train_label_path,\n",
    "                            test_data_path=FLAGS.test_data_path, test_label_path=FLAGS.test_label_path,\n",
    "                            vocabulary_word2index=vocabulary_word2index)\n",
    "    trainX, trainY = train\n",
    "    testX, testY = test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据预处理，所有输入处理成相同长度，打pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start padding & transform to one hot...\n",
      "maxlen= 21\n",
      "maxlen= 21\n"
     ]
    }
   ],
   "source": [
    "    # 2.Data preprocessing.Sequence padding\n",
    "    print(\"start padding & transform to one hot...\")\n",
    "    trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length)  # padding to max length\n",
    "    testX = pad_sequences(testX, maxlen=FLAGS.sequence_length)  # padding to max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[4, 9, 7, 8, 3, 0, 5, 6, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "x = [i for i in range(10) ]\n",
    "print(x)\n",
    "random.shuffle(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learing rate: <tf.Variable 'learning_rate:0' shape=() dtype=float32_ref>\n",
      "global_step: <tf.Variable 'global_step:0' shape=() dtype=int32_ref>\n",
      "decay_steps: 3500\n",
      "decay_rate: 0.65\n",
      "decay_steps=3500 decay_rate=0.650000\n",
      "Writing to /data/1xiu/project/pythonProject/medicalPOC/ICD102018/runs/1515404757\n",
      "\n",
      "Initializing Variables\n",
      "curr_epoch= 0\n",
      "number_of_training_data= 46275\n",
      "batch_size= 64\n",
      "trainX[start:end]: [[144761 188342  62209 144761 151153  51529 166239      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0]\n",
      " [144830 141039  31237 146258 189788 185878  72923  16052 166239      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0]\n",
      " [ 34498  32545 185694 162779  31054  24997 136334  12124  61786   9667\n",
      "   99326 188663  84367 144660  87303 188764  80122 189369 129733 106738\n",
      "  135869]\n",
      " [ 35482  24802  16573 121243 166239  55780 130297 154904  58379 118844\n",
      "   59317  84605  90205  90145  41664 149025  97768  12515 146288 132760\n",
      "   68917]\n",
      " [ 83556 127428  77593 166239 181301   3123  56405 137658  83467 127091\n",
      "   83556  80687 133570 157583 142451 173911   3241  78972 165453  82357\n",
      "  126854]]\n",
      "Epoch 0\tBatch 100\tTrain Loss:2.324\tTrain Accuracy:0.585\n",
      "Epoch 0\tBatch 200\tTrain Loss:1.629\tTrain Accuracy:0.717\n",
      "Epoch 0\tBatch 300\tTrain Loss:1.380\tTrain Accuracy:0.772\n",
      "Epoch 0\tBatch 400\tTrain Loss:1.248\tTrain Accuracy:0.806\n",
      "Epoch 0\tBatch 500\tTrain Loss:1.176\tTrain Accuracy:0.826\n",
      "Epoch 0\tBatch 600\tTrain Loss:1.128\tTrain Accuracy:0.841\n",
      "Epoch 0\tBatch 700\tTrain Loss:1.096\tTrain Accuracy:0.852\n",
      "going to increment epoch counter....\n",
      "0 5 True\n",
      "Epoch 0 Validation Loss:0.727\tValidation Accuracy: 0.958\t time: 2018-01-08T17:49:39.912997\n",
      "trainX[start:end]: [[ 36913 157786 186126 104176  45934  59010   4635 137179  82701 190509\n",
      "    3137  16414  82655 139469  67943 188764  97831 189369  83400 135869\n",
      "   19430]\n",
      " [ 33273   9667  32545  48546  87303 144660 185656  99326 188764 153322\n",
      "   24152 135869  73770 141797 117057 150651 149385 103954  27064  32912\n",
      "  123041]\n",
      " [124477  26570 111255 124477  62981 179354  27705 190273  33189 103757\n",
      "   65583   1504  74909 128952 188764 177595 142090 177830  61043 189116\n",
      "       0]\n",
      " [127081 188764 110255  72981  45287  38614  84315  24152 149573  93793\n",
      "  113969  27064  95105  36238 166239      0      0      0      0      0\n",
      "       0]\n",
      " [126720 166239 165605 124930 172784 134806 161179   9918  84089 116944\n",
      "   80853 155130 189105  20818 185878 172594  46844   9766  29880  67336\n",
      "  117057]]\n",
      "Epoch 1\tBatch 100\tTrain Loss:0.755\tTrain Accuracy:0.941\n",
      "Epoch 1\tBatch 200\tTrain Loss:0.769\tTrain Accuracy:0.936\n",
      "Epoch 1\tBatch 300\tTrain Loss:0.778\tTrain Accuracy:0.936\n",
      "Epoch 1\tBatch 400\tTrain Loss:0.791\tTrain Accuracy:0.936\n",
      "Epoch 1\tBatch 500\tTrain Loss:0.798\tTrain Accuracy:0.936\n",
      "Epoch 1\tBatch 600\tTrain Loss:0.812\tTrain Accuracy:0.934\n",
      "Epoch 1\tBatch 700\tTrain Loss:0.820\tTrain Accuracy:0.934\n",
      "going to increment epoch counter....\n",
      "1 5 False\n",
      "trainX[start:end]: [[ 56347 166239   3241  56347  80917  16358  20232 189369  24568 169271\n",
      "  103954 123757 176262 144312 189740 158189  70918 187805  13262  45757\n",
      "  179384]\n",
      " [124477 170814 101460  69240  20796  99429 137758 131269  75578  24820\n",
      "  190273 134119  44997  24058  40487      0      0      0      0      0\n",
      "       0]\n",
      " [101989 184261 169896 166253  63483  38210   8295  87265  11937 128370\n",
      "   79313 189369 158611 101943 168104  80006   8826  78169 166239  31157\n",
      "       0]\n",
      " [  5614  62982  75480 166239  78169  59322  60719  68933   3137 142524\n",
      "   65798  17109 118597  46990  37643  56534  13262  78972 118313 165227\n",
      "   53177]\n",
      " [124477  46252   7502  24181 189788  16052 166239      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0]]\n",
      "Epoch 2\tBatch 100\tTrain Loss:0.784\tTrain Accuracy:0.941\n",
      "Epoch 2\tBatch 200\tTrain Loss:0.783\tTrain Accuracy:0.939\n",
      "Epoch 2\tBatch 300\tTrain Loss:0.796\tTrain Accuracy:0.938\n",
      "Epoch 2\tBatch 400\tTrain Loss:0.797\tTrain Accuracy:0.939\n",
      "Epoch 2\tBatch 500\tTrain Loss:0.796\tTrain Accuracy:0.939\n",
      "Epoch 2\tBatch 600\tTrain Loss:0.799\tTrain Accuracy:0.939\n",
      "Epoch 2\tBatch 700\tTrain Loss:0.800\tTrain Accuracy:0.939\n",
      "going to increment epoch counter....\n",
      "2 5 False\n"
     ]
    }
   ],
   "source": [
    "    import random\n",
    "    import time\n",
    "    # 3.create session.\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        # Instantiate Model\n",
    "        filter_sizes = [3,4,5]\n",
    "        cnnDisease = CNNDisease(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size,\n",
    "                          FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length, vocab_size, FLAGS.embed_size,\n",
    "                          FLAGS.is_training)\n",
    "        \n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        \n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnnDisease.loss_val)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnnDisease.accuracy)\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        \n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "        \n",
    "        # Initialize Save\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "        print('Initializing Variables')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        if FLAGS.use_embedding:  # load pre-trained word embedding\n",
    "            assign_pretrained_word_embedding(sess, cnnDisease, model)\n",
    "        curr_epoch = sess.run(cnnDisease.epoch_step)\n",
    "       \n",
    "        print(\"curr_epoch=\", curr_epoch)\n",
    "        number_of_training_data = len(trainX)\n",
    "        print(\"number_of_training_data=\",number_of_training_data)\n",
    "       \n",
    "        batch_size = FLAGS.batch_size\n",
    "        print(\"batch_size=\", batch_size)\n",
    "        #4 feed data\n",
    "        for epoch in range(curr_epoch, FLAGS.num_epochs + 1):\n",
    "            loss, acc, counter = 0.0, 0.0, 0\n",
    "            '''\n",
    "            indexList = [i for i in range(number_of_training_data)]\n",
    "            random.shuffle(indexList)\n",
    "            x = trainX\n",
    "            y = trainY\n",
    "            for i in range(number_of_training_data):\n",
    "                x[i] = trainX[indexList[i]]\n",
    "                y[i] = trainY[indexList[i]]\n",
    "            trainX = x\n",
    "            trainY = y\n",
    "            '''\n",
    "            #每个epoch ，shuffle数据\n",
    "            np.random.seed(10) \n",
    "            shuffle_indices = np.random.permutation(np.arange(number_of_training_data))\n",
    "            x = trainX[shuffle_indices]\n",
    "            y = trainY[shuffle_indices]\n",
    "            trainX = x\n",
    "            trainY = y\n",
    "                                                    \n",
    "            for start, end in zip(range(0, number_of_training_data, batch_size),\n",
    "                                  range(batch_size, number_of_training_data, batch_size)):\n",
    "                if counter == 0:\n",
    "                    print(\"trainX[start:end]:\", trainX[start:5])  # ;print(\"trainY[start:end]:\",trainY[start:end])\n",
    "                #use the word index as the input \n",
    "                feed_dict = {cnnDisease.input_x: trainX[start:end], cnnDisease.dropout_keep_prob: FLAGS.dropout_keep_prob}\n",
    "\n",
    "                feed_dict[cnnDisease.input_y] = trainY[start:end]\n",
    "                #5 training \n",
    "                curr_loss, curr_acc, summaries,_ = sess.run([cnnDisease.loss_val, cnnDisease.accuracy,train_summary_op, cnnDisease.train_op],\n",
    "                                                 feed_dict)\n",
    "                train_summary_writer.add_summary(summaries, epoch)\n",
    "                \n",
    "                loss, counter, acc = loss + curr_loss, counter + 1, acc + curr_acc\n",
    "                if counter % 100 == 0:\n",
    "                    print(\"Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f\" % (\n",
    "                    epoch, counter, loss / float(counter), acc / float(counter)))\n",
    "\n",
    "            # epoch increment\n",
    "            print(\"going to increment epoch counter....\")\n",
    "            sess.run(cnnDisease.epoch_increment)\n",
    "        \n",
    "             # 6.validation\n",
    "            print(epoch, FLAGS.validate_every, (epoch % FLAGS.validate_every == 0))\n",
    "            if epoch % FLAGS.validate_every == 0:\n",
    "                \n",
    "                #eval_loss, eval_acc = do_eval(sess, cnnDisease, testX, testY, batch_size)\n",
    "                evalX = testX\n",
    "                evalY = testY\n",
    "                number_examples = len(evalX)\n",
    "                eval_loss, eval_acc, eval_counter = 0.0, 0.0, 0\n",
    "                for start, end in zip(range(0, number_examples, batch_size), range(batch_size, number_examples, batch_size)):\n",
    "                    feed_dict = {cnnDisease.input_x: evalX[start:end], cnnDisease.dropout_keep_prob: 1.0}\n",
    "                    feed_dict[cnnDisease.input_y] = evalY[start:end]\n",
    "                    curr_eval_loss, logits, curr_eval_acc,summariesEval = sess.run([cnnDisease.loss_val, cnnDisease.logits, cnnDisease.accuracy,train_summary_op],\n",
    "                                                                     feed_dict)\n",
    "                    dev_summary_writer.add_summary(summariesEval, epoch)\n",
    "                    \n",
    "                    eval_loss, eval_acc, eval_counter = eval_loss + curr_eval_loss, eval_acc + curr_eval_acc, eval_counter + 1\n",
    "\n",
    "                \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f\\t time: %s\" % (\n",
    "                epoch, eval_loss, eval_acc, time_str))\n",
    "                # save model to checkpoint\n",
    "                if not os.path.exists(FLAGS.ckpt_dir + \"checkpoint\"):\n",
    "                    os.makedirs(FLAGS.ckpt_dir)\n",
    "                save_path = FLAGS.ckpt_dir + \"model.ckpt\"\n",
    "                saver.save(sess, save_path, global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step: 0, learning rate: 0.010000\n",
      "global step: 1, learning rate: 0.009979\n",
      "global step: 2, learning rate: 0.009958\n",
      "global step: 3, learning rate: 0.009937\n",
      "global step: 4, learning rate: 0.009916\n",
      "global step: 5, learning rate: 0.009895\n",
      "global step: 6, learning rate: 0.009874\n",
      "global step: 7, learning rate: 0.009854\n",
      "global step: 8, learning rate: 0.009833\n",
      "global step: 9, learning rate: 0.009812\n",
      "global step: 10, learning rate: 0.009791\n",
      "global step: 11, learning rate: 0.009771\n",
      "global step: 12, learning rate: 0.009750\n",
      "global step: 13, learning rate: 0.009730\n",
      "global step: 14, learning rate: 0.009709\n",
      "global step: 15, learning rate: 0.009689\n",
      "global step: 16, learning rate: 0.009668\n",
      "global step: 17, learning rate: 0.009648\n",
      "global step: 18, learning rate: 0.009628\n",
      "global step: 19, learning rate: 0.009608\n",
      "global step: 20, learning rate: 0.009587\n",
      "global step: 21, learning rate: 0.009567\n",
      "global step: 22, learning rate: 0.009547\n",
      "global step: 23, learning rate: 0.009527\n",
      "global step: 24, learning rate: 0.009507\n",
      "global step: 25, learning rate: 0.009487\n",
      "global step: 26, learning rate: 0.009467\n",
      "global step: 27, learning rate: 0.009447\n",
      "global step: 28, learning rate: 0.009427\n",
      "global step: 29, learning rate: 0.009407\n",
      "global step: 30, learning rate: 0.009387\n",
      "global step: 31, learning rate: 0.009368\n",
      "global step: 32, learning rate: 0.009348\n",
      "global step: 33, learning rate: 0.009328\n",
      "global step: 34, learning rate: 0.009309\n",
      "global step: 35, learning rate: 0.009289\n",
      "global step: 36, learning rate: 0.009269\n",
      "global step: 37, learning rate: 0.009250\n",
      "global step: 38, learning rate: 0.009230\n",
      "global step: 39, learning rate: 0.009211\n",
      "global step: 40, learning rate: 0.009192\n",
      "global step: 41, learning rate: 0.009172\n",
      "global step: 42, learning rate: 0.009153\n",
      "global step: 43, learning rate: 0.009134\n",
      "global step: 44, learning rate: 0.009115\n",
      "global step: 45, learning rate: 0.009095\n",
      "global step: 46, learning rate: 0.009076\n",
      "global step: 47, learning rate: 0.009057\n",
      "global step: 48, learning rate: 0.009038\n",
      "global step: 49, learning rate: 0.009019\n",
      "global step: 50, learning rate: 0.009000\n",
      "global step: 51, learning rate: 0.008981\n",
      "global step: 52, learning rate: 0.008962\n",
      "global step: 53, learning rate: 0.008943\n",
      "global step: 54, learning rate: 0.008924\n",
      "global step: 55, learning rate: 0.008906\n",
      "global step: 56, learning rate: 0.008887\n",
      "global step: 57, learning rate: 0.008868\n",
      "global step: 58, learning rate: 0.008850\n",
      "global step: 59, learning rate: 0.008831\n",
      "global step: 60, learning rate: 0.008812\n",
      "global step: 61, learning rate: 0.008794\n",
      "global step: 62, learning rate: 0.008775\n",
      "global step: 63, learning rate: 0.008757\n",
      "global step: 64, learning rate: 0.008738\n",
      "global step: 65, learning rate: 0.008720\n",
      "global step: 66, learning rate: 0.008702\n",
      "global step: 67, learning rate: 0.008683\n",
      "global step: 68, learning rate: 0.008665\n",
      "global step: 69, learning rate: 0.008647\n",
      "global step: 70, learning rate: 0.008629\n",
      "global step: 71, learning rate: 0.008610\n",
      "global step: 72, learning rate: 0.008592\n",
      "global step: 73, learning rate: 0.008574\n",
      "global step: 74, learning rate: 0.008556\n",
      "global step: 75, learning rate: 0.008538\n",
      "global step: 76, learning rate: 0.008520\n",
      "global step: 77, learning rate: 0.008502\n",
      "global step: 78, learning rate: 0.008484\n",
      "global step: 79, learning rate: 0.008466\n",
      "global step: 80, learning rate: 0.008449\n",
      "global step: 81, learning rate: 0.008431\n",
      "global step: 82, learning rate: 0.008413\n",
      "global step: 83, learning rate: 0.008395\n",
      "global step: 84, learning rate: 0.008378\n",
      "global step: 85, learning rate: 0.008360\n",
      "global step: 86, learning rate: 0.008343\n",
      "global step: 87, learning rate: 0.008325\n",
      "global step: 88, learning rate: 0.008307\n",
      "global step: 89, learning rate: 0.008290\n",
      "global step: 90, learning rate: 0.008272\n",
      "global step: 91, learning rate: 0.008255\n",
      "global step: 92, learning rate: 0.008238\n",
      "global step: 93, learning rate: 0.008220\n",
      "global step: 94, learning rate: 0.008203\n",
      "global step: 95, learning rate: 0.008186\n",
      "global step: 96, learning rate: 0.008169\n",
      "global step: 97, learning rate: 0.008151\n",
      "global step: 98, learning rate: 0.008134\n",
      "global step: 99, learning rate: 0.008117\n",
      "global step: 100, learning rate: 0.008100\n",
      "global step: 101, learning rate: 0.008083\n",
      "global step: 102, learning rate: 0.008066\n",
      "global step: 103, learning rate: 0.008049\n",
      "global step: 104, learning rate: 0.008032\n",
      "global step: 105, learning rate: 0.008015\n",
      "global step: 106, learning rate: 0.007998\n",
      "global step: 107, learning rate: 0.007981\n",
      "global step: 108, learning rate: 0.007965\n",
      "global step: 109, learning rate: 0.007948\n",
      "global step: 110, learning rate: 0.007931\n",
      "global step: 111, learning rate: 0.007914\n",
      "global step: 112, learning rate: 0.007898\n",
      "global step: 113, learning rate: 0.007881\n",
      "global step: 114, learning rate: 0.007865\n",
      "global step: 115, learning rate: 0.007848\n",
      "global step: 116, learning rate: 0.007831\n",
      "global step: 117, learning rate: 0.007815\n",
      "global step: 118, learning rate: 0.007799\n",
      "global step: 119, learning rate: 0.007782\n",
      "global step: 120, learning rate: 0.007766\n",
      "global step: 121, learning rate: 0.007749\n",
      "global step: 122, learning rate: 0.007733\n",
      "global step: 123, learning rate: 0.007717\n",
      "global step: 124, learning rate: 0.007701\n",
      "global step: 125, learning rate: 0.007684\n",
      "global step: 126, learning rate: 0.007668\n",
      "global step: 127, learning rate: 0.007652\n",
      "global step: 128, learning rate: 0.007636\n",
      "global step: 129, learning rate: 0.007620\n",
      "global step: 130, learning rate: 0.007604\n",
      "global step: 131, learning rate: 0.007588\n",
      "global step: 132, learning rate: 0.007572\n",
      "global step: 133, learning rate: 0.007556\n",
      "global step: 134, learning rate: 0.007540\n",
      "global step: 135, learning rate: 0.007524\n",
      "global step: 136, learning rate: 0.007508\n",
      "global step: 137, learning rate: 0.007492\n",
      "global step: 138, learning rate: 0.007477\n",
      "global step: 139, learning rate: 0.007461\n",
      "global step: 140, learning rate: 0.007445\n",
      "global step: 141, learning rate: 0.007430\n",
      "global step: 142, learning rate: 0.007414\n",
      "global step: 143, learning rate: 0.007398\n",
      "global step: 144, learning rate: 0.007383\n",
      "global step: 145, learning rate: 0.007367\n",
      "global step: 146, learning rate: 0.007352\n",
      "global step: 147, learning rate: 0.007336\n",
      "global step: 148, learning rate: 0.007321\n",
      "global step: 149, learning rate: 0.007305\n",
      "global step: 150, learning rate: 0.007290\n",
      "global step: 151, learning rate: 0.007275\n",
      "global step: 152, learning rate: 0.007259\n",
      "global step: 153, learning rate: 0.007244\n",
      "global step: 154, learning rate: 0.007229\n",
      "global step: 155, learning rate: 0.007214\n",
      "global step: 156, learning rate: 0.007198\n",
      "global step: 157, learning rate: 0.007183\n",
      "global step: 158, learning rate: 0.007168\n",
      "global step: 159, learning rate: 0.007153\n",
      "global step: 160, learning rate: 0.007138\n",
      "global step: 161, learning rate: 0.007123\n",
      "global step: 162, learning rate: 0.007108\n",
      "global step: 163, learning rate: 0.007093\n",
      "global step: 164, learning rate: 0.007078\n",
      "global step: 165, learning rate: 0.007063\n",
      "global step: 166, learning rate: 0.007048\n",
      "global step: 167, learning rate: 0.007033\n",
      "global step: 168, learning rate: 0.007019\n",
      "global step: 169, learning rate: 0.007004\n",
      "global step: 170, learning rate: 0.006989\n",
      "global step: 171, learning rate: 0.006974\n",
      "global step: 172, learning rate: 0.006960\n",
      "global step: 173, learning rate: 0.006945\n",
      "global step: 174, learning rate: 0.006930\n",
      "global step: 175, learning rate: 0.006916\n",
      "global step: 176, learning rate: 0.006901\n",
      "global step: 177, learning rate: 0.006887\n",
      "global step: 178, learning rate: 0.006872\n",
      "global step: 179, learning rate: 0.006858\n",
      "global step: 180, learning rate: 0.006843\n",
      "global step: 181, learning rate: 0.006829\n",
      "global step: 182, learning rate: 0.006815\n",
      "global step: 183, learning rate: 0.006800\n",
      "global step: 184, learning rate: 0.006786\n",
      "global step: 185, learning rate: 0.006772\n",
      "global step: 186, learning rate: 0.006757\n",
      "global step: 187, learning rate: 0.006743\n",
      "global step: 188, learning rate: 0.006729\n",
      "global step: 189, learning rate: 0.006715\n",
      "global step: 190, learning rate: 0.006701\n",
      "global step: 191, learning rate: 0.006687\n",
      "global step: 192, learning rate: 0.006673\n",
      "global step: 193, learning rate: 0.006658\n",
      "global step: 194, learning rate: 0.006644\n",
      "global step: 195, learning rate: 0.006630\n",
      "global step: 196, learning rate: 0.006617\n",
      "global step: 197, learning rate: 0.006603\n",
      "global step: 198, learning rate: 0.006589\n",
      "global step: 199, learning rate: 0.006575\n",
      "global step: 200, learning rate: 0.006561\n",
      "global step: 201, learning rate: 0.006547\n",
      "global step: 202, learning rate: 0.006533\n",
      "global step: 203, learning rate: 0.006520\n",
      "global step: 204, learning rate: 0.006506\n",
      "global step: 205, learning rate: 0.006492\n",
      "global step: 206, learning rate: 0.006479\n",
      "global step: 207, learning rate: 0.006465\n",
      "global step: 208, learning rate: 0.006451\n",
      "global step: 209, learning rate: 0.006438\n",
      "global step: 210, learning rate: 0.006424\n",
      "global step: 211, learning rate: 0.006411\n",
      "global step: 212, learning rate: 0.006397\n",
      "global step: 213, learning rate: 0.006384\n",
      "global step: 214, learning rate: 0.006370\n",
      "global step: 215, learning rate: 0.006357\n",
      "global step: 216, learning rate: 0.006343\n",
      "global step: 217, learning rate: 0.006330\n",
      "global step: 218, learning rate: 0.006317\n",
      "global step: 219, learning rate: 0.006304\n",
      "global step: 220, learning rate: 0.006290\n",
      "global step: 221, learning rate: 0.006277\n",
      "global step: 222, learning rate: 0.006264\n",
      "global step: 223, learning rate: 0.006251\n",
      "global step: 224, learning rate: 0.006237\n",
      "global step: 225, learning rate: 0.006224\n",
      "global step: 226, learning rate: 0.006211\n",
      "global step: 227, learning rate: 0.006198\n",
      "global step: 228, learning rate: 0.006185\n",
      "global step: 229, learning rate: 0.006172\n",
      "global step: 230, learning rate: 0.006159\n",
      "global step: 231, learning rate: 0.006146\n",
      "global step: 232, learning rate: 0.006133\n",
      "global step: 233, learning rate: 0.006120\n",
      "global step: 234, learning rate: 0.006107\n",
      "global step: 235, learning rate: 0.006095\n",
      "global step: 236, learning rate: 0.006082\n",
      "global step: 237, learning rate: 0.006069\n",
      "global step: 238, learning rate: 0.006056\n",
      "global step: 239, learning rate: 0.006043\n",
      "global step: 240, learning rate: 0.006031\n",
      "global step: 241, learning rate: 0.006018\n",
      "global step: 242, learning rate: 0.006005\n",
      "global step: 243, learning rate: 0.005993\n",
      "global step: 244, learning rate: 0.005980\n",
      "global step: 245, learning rate: 0.005967\n",
      "global step: 246, learning rate: 0.005955\n",
      "global step: 247, learning rate: 0.005942\n",
      "global step: 248, learning rate: 0.005930\n",
      "global step: 249, learning rate: 0.005917\n",
      "global step: 250, learning rate: 0.005905\n",
      "global step: 251, learning rate: 0.005892\n",
      "global step: 252, learning rate: 0.005880\n",
      "global step: 253, learning rate: 0.005868\n",
      "global step: 254, learning rate: 0.005855\n",
      "global step: 255, learning rate: 0.005843\n",
      "global step: 256, learning rate: 0.005831\n",
      "global step: 257, learning rate: 0.005818\n",
      "global step: 258, learning rate: 0.005806\n",
      "global step: 259, learning rate: 0.005794\n",
      "global step: 260, learning rate: 0.005782\n",
      "global step: 261, learning rate: 0.005770\n",
      "global step: 262, learning rate: 0.005757\n",
      "global step: 263, learning rate: 0.005745\n",
      "global step: 264, learning rate: 0.005733\n",
      "global step: 265, learning rate: 0.005721\n",
      "global step: 266, learning rate: 0.005709\n",
      "global step: 267, learning rate: 0.005697\n",
      "global step: 268, learning rate: 0.005685\n",
      "global step: 269, learning rate: 0.005673\n",
      "global step: 270, learning rate: 0.005661\n",
      "global step: 271, learning rate: 0.005649\n",
      "global step: 272, learning rate: 0.005637\n",
      "global step: 273, learning rate: 0.005626\n",
      "global step: 274, learning rate: 0.005614\n",
      "global step: 275, learning rate: 0.005602\n",
      "global step: 276, learning rate: 0.005590\n",
      "global step: 277, learning rate: 0.005578\n",
      "global step: 278, learning rate: 0.005567\n",
      "global step: 279, learning rate: 0.005555\n",
      "global step: 280, learning rate: 0.005543\n",
      "global step: 281, learning rate: 0.005531\n",
      "global step: 282, learning rate: 0.005520\n",
      "global step: 283, learning rate: 0.005508\n",
      "global step: 284, learning rate: 0.005497\n",
      "global step: 285, learning rate: 0.005485\n",
      "global step: 286, learning rate: 0.005474\n",
      "global step: 287, learning rate: 0.005462\n",
      "global step: 288, learning rate: 0.005451\n",
      "global step: 289, learning rate: 0.005439\n",
      "global step: 290, learning rate: 0.005428\n",
      "global step: 291, learning rate: 0.005416\n",
      "global step: 292, learning rate: 0.005405\n",
      "global step: 293, learning rate: 0.005393\n",
      "global step: 294, learning rate: 0.005382\n",
      "global step: 295, learning rate: 0.005371\n",
      "global step: 296, learning rate: 0.005359\n",
      "global step: 297, learning rate: 0.005348\n",
      "global step: 298, learning rate: 0.005337\n",
      "global step: 299, learning rate: 0.005326\n",
      "global step: 300, learning rate: 0.005314\n",
      "global step: 301, learning rate: 0.005303\n",
      "global step: 302, learning rate: 0.005292\n",
      "global step: 303, learning rate: 0.005281\n",
      "global step: 304, learning rate: 0.005270\n",
      "global step: 305, learning rate: 0.005259\n",
      "global step: 306, learning rate: 0.005248\n",
      "global step: 307, learning rate: 0.005237\n",
      "global step: 308, learning rate: 0.005226\n",
      "global step: 309, learning rate: 0.005215\n",
      "global step: 310, learning rate: 0.005204\n",
      "global step: 311, learning rate: 0.005193\n",
      "global step: 312, learning rate: 0.005182\n",
      "global step: 313, learning rate: 0.005171\n",
      "global step: 314, learning rate: 0.005160\n",
      "global step: 315, learning rate: 0.005149\n",
      "global step: 316, learning rate: 0.005138\n",
      "global step: 317, learning rate: 0.005127\n",
      "global step: 318, learning rate: 0.005117\n",
      "global step: 319, learning rate: 0.005106\n",
      "global step: 320, learning rate: 0.005095\n",
      "global step: 321, learning rate: 0.005084\n",
      "global step: 322, learning rate: 0.005074\n",
      "global step: 323, learning rate: 0.005063\n",
      "global step: 324, learning rate: 0.005052\n",
      "global step: 325, learning rate: 0.005042\n",
      "global step: 326, learning rate: 0.005031\n",
      "global step: 327, learning rate: 0.005020\n",
      "global step: 328, learning rate: 0.005010\n",
      "global step: 329, learning rate: 0.004999\n",
      "global step: 330, learning rate: 0.004989\n",
      "global step: 331, learning rate: 0.004978\n",
      "global step: 332, learning rate: 0.004968\n",
      "global step: 333, learning rate: 0.004957\n",
      "global step: 334, learning rate: 0.004947\n",
      "global step: 335, learning rate: 0.004937\n",
      "global step: 336, learning rate: 0.004926\n",
      "global step: 337, learning rate: 0.004916\n",
      "global step: 338, learning rate: 0.004905\n",
      "global step: 339, learning rate: 0.004895\n",
      "global step: 340, learning rate: 0.004885\n",
      "global step: 341, learning rate: 0.004875\n",
      "global step: 342, learning rate: 0.004864\n",
      "global step: 343, learning rate: 0.004854\n",
      "global step: 344, learning rate: 0.004844\n",
      "global step: 345, learning rate: 0.004834\n",
      "global step: 346, learning rate: 0.004823\n",
      "global step: 347, learning rate: 0.004813\n",
      "global step: 348, learning rate: 0.004803\n",
      "global step: 349, learning rate: 0.004793\n",
      "global step: 350, learning rate: 0.004783\n",
      "global step: 351, learning rate: 0.004773\n",
      "global step: 352, learning rate: 0.004763\n",
      "global step: 353, learning rate: 0.004753\n",
      "global step: 354, learning rate: 0.004743\n",
      "global step: 355, learning rate: 0.004733\n",
      "global step: 356, learning rate: 0.004723\n",
      "global step: 357, learning rate: 0.004713\n",
      "global step: 358, learning rate: 0.004703\n",
      "global step: 359, learning rate: 0.004693\n",
      "global step: 360, learning rate: 0.004683\n",
      "global step: 361, learning rate: 0.004673\n",
      "global step: 362, learning rate: 0.004664\n",
      "global step: 363, learning rate: 0.004654\n",
      "global step: 364, learning rate: 0.004644\n",
      "global step: 365, learning rate: 0.004634\n",
      "global step: 366, learning rate: 0.004624\n",
      "global step: 367, learning rate: 0.004615\n",
      "global step: 368, learning rate: 0.004605\n",
      "global step: 369, learning rate: 0.004595\n",
      "global step: 370, learning rate: 0.004586\n",
      "global step: 371, learning rate: 0.004576\n",
      "global step: 372, learning rate: 0.004566\n",
      "global step: 373, learning rate: 0.004557\n",
      "global step: 374, learning rate: 0.004547\n",
      "global step: 375, learning rate: 0.004538\n",
      "global step: 376, learning rate: 0.004528\n",
      "global step: 377, learning rate: 0.004518\n",
      "global step: 378, learning rate: 0.004509\n",
      "global step: 379, learning rate: 0.004499\n",
      "global step: 380, learning rate: 0.004490\n",
      "global step: 381, learning rate: 0.004481\n",
      "global step: 382, learning rate: 0.004471\n",
      "global step: 383, learning rate: 0.004462\n",
      "global step: 384, learning rate: 0.004452\n",
      "global step: 385, learning rate: 0.004443\n",
      "global step: 386, learning rate: 0.004434\n",
      "global step: 387, learning rate: 0.004424\n",
      "global step: 388, learning rate: 0.004415\n",
      "global step: 389, learning rate: 0.004406\n",
      "global step: 390, learning rate: 0.004396\n",
      "global step: 391, learning rate: 0.004387\n",
      "global step: 392, learning rate: 0.004378\n",
      "global step: 393, learning rate: 0.004369\n",
      "global step: 394, learning rate: 0.004359\n",
      "global step: 395, learning rate: 0.004350\n",
      "global step: 396, learning rate: 0.004341\n",
      "global step: 397, learning rate: 0.004332\n",
      "global step: 398, learning rate: 0.004323\n",
      "global step: 399, learning rate: 0.004314\n",
      "global step: 400, learning rate: 0.004305\n",
      "global step: 401, learning rate: 0.004296\n",
      "global step: 402, learning rate: 0.004287\n",
      "global step: 403, learning rate: 0.004278\n",
      "global step: 404, learning rate: 0.004269\n",
      "global step: 405, learning rate: 0.004260\n",
      "global step: 406, learning rate: 0.004251\n",
      "global step: 407, learning rate: 0.004242\n",
      "global step: 408, learning rate: 0.004233\n",
      "global step: 409, learning rate: 0.004224\n",
      "global step: 410, learning rate: 0.004215\n",
      "global step: 411, learning rate: 0.004206\n",
      "global step: 412, learning rate: 0.004197\n",
      "global step: 413, learning rate: 0.004188\n",
      "global step: 414, learning rate: 0.004180\n",
      "global step: 415, learning rate: 0.004171\n",
      "global step: 416, learning rate: 0.004162\n",
      "global step: 417, learning rate: 0.004153\n",
      "global step: 418, learning rate: 0.004144\n",
      "global step: 419, learning rate: 0.004136\n",
      "global step: 420, learning rate: 0.004127\n",
      "global step: 421, learning rate: 0.004118\n",
      "global step: 422, learning rate: 0.004110\n",
      "global step: 423, learning rate: 0.004101\n",
      "global step: 424, learning rate: 0.004092\n",
      "global step: 425, learning rate: 0.004084\n",
      "global step: 426, learning rate: 0.004075\n",
      "global step: 427, learning rate: 0.004067\n",
      "global step: 428, learning rate: 0.004058\n",
      "global step: 429, learning rate: 0.004049\n",
      "global step: 430, learning rate: 0.004041\n",
      "global step: 431, learning rate: 0.004032\n",
      "global step: 432, learning rate: 0.004024\n",
      "global step: 433, learning rate: 0.004016\n",
      "global step: 434, learning rate: 0.004007\n",
      "global step: 435, learning rate: 0.003999\n",
      "global step: 436, learning rate: 0.003990\n",
      "global step: 437, learning rate: 0.003982\n",
      "global step: 438, learning rate: 0.003973\n",
      "global step: 439, learning rate: 0.003965\n",
      "global step: 440, learning rate: 0.003957\n",
      "global step: 441, learning rate: 0.003948\n",
      "global step: 442, learning rate: 0.003940\n",
      "global step: 443, learning rate: 0.003932\n",
      "global step: 444, learning rate: 0.003923\n",
      "global step: 445, learning rate: 0.003915\n",
      "global step: 446, learning rate: 0.003907\n",
      "global step: 447, learning rate: 0.003899\n",
      "global step: 448, learning rate: 0.003891\n",
      "global step: 449, learning rate: 0.003882\n",
      "global step: 450, learning rate: 0.003874\n",
      "global step: 451, learning rate: 0.003866\n",
      "global step: 452, learning rate: 0.003858\n",
      "global step: 453, learning rate: 0.003850\n",
      "global step: 454, learning rate: 0.003842\n",
      "global step: 455, learning rate: 0.003834\n",
      "global step: 456, learning rate: 0.003826\n",
      "global step: 457, learning rate: 0.003817\n",
      "global step: 458, learning rate: 0.003809\n",
      "global step: 459, learning rate: 0.003801\n",
      "global step: 460, learning rate: 0.003793\n",
      "global step: 461, learning rate: 0.003785\n",
      "global step: 462, learning rate: 0.003777\n",
      "global step: 463, learning rate: 0.003770\n",
      "global step: 464, learning rate: 0.003762\n",
      "global step: 465, learning rate: 0.003754\n",
      "global step: 466, learning rate: 0.003746\n",
      "global step: 467, learning rate: 0.003738\n",
      "global step: 468, learning rate: 0.003730\n",
      "global step: 469, learning rate: 0.003722\n",
      "global step: 470, learning rate: 0.003714\n",
      "global step: 471, learning rate: 0.003707\n",
      "global step: 472, learning rate: 0.003699\n",
      "global step: 473, learning rate: 0.003691\n",
      "global step: 474, learning rate: 0.003683\n",
      "global step: 475, learning rate: 0.003675\n",
      "global step: 476, learning rate: 0.003668\n",
      "global step: 477, learning rate: 0.003660\n",
      "global step: 478, learning rate: 0.003652\n",
      "global step: 479, learning rate: 0.003645\n",
      "global step: 480, learning rate: 0.003637\n",
      "global step: 481, learning rate: 0.003629\n",
      "global step: 482, learning rate: 0.003622\n",
      "global step: 483, learning rate: 0.003614\n",
      "global step: 484, learning rate: 0.003606\n",
      "global step: 485, learning rate: 0.003599\n",
      "global step: 486, learning rate: 0.003591\n",
      "global step: 487, learning rate: 0.003584\n",
      "global step: 488, learning rate: 0.003576\n",
      "global step: 489, learning rate: 0.003569\n",
      "global step: 490, learning rate: 0.003561\n",
      "global step: 491, learning rate: 0.003554\n",
      "global step: 492, learning rate: 0.003546\n",
      "global step: 493, learning rate: 0.003539\n",
      "global step: 494, learning rate: 0.003531\n",
      "global step: 495, learning rate: 0.003524\n",
      "global step: 496, learning rate: 0.003516\n",
      "global step: 497, learning rate: 0.003509\n",
      "global step: 498, learning rate: 0.003502\n",
      "global step: 499, learning rate: 0.003494\n",
      "global step: 500, learning rate: 0.003487\n",
      "global step: 501, learning rate: 0.003479\n",
      "global step: 502, learning rate: 0.003472\n",
      "global step: 503, learning rate: 0.003465\n",
      "global step: 504, learning rate: 0.003458\n",
      "global step: 505, learning rate: 0.003450\n",
      "global step: 506, learning rate: 0.003443\n",
      "global step: 507, learning rate: 0.003436\n",
      "global step: 508, learning rate: 0.003428\n",
      "global step: 509, learning rate: 0.003421\n",
      "global step: 510, learning rate: 0.003414\n",
      "global step: 511, learning rate: 0.003407\n",
      "global step: 512, learning rate: 0.003400\n",
      "global step: 513, learning rate: 0.003393\n",
      "global step: 514, learning rate: 0.003385\n",
      "global step: 515, learning rate: 0.003378\n",
      "global step: 516, learning rate: 0.003371\n",
      "global step: 517, learning rate: 0.003364\n",
      "global step: 518, learning rate: 0.003357\n",
      "global step: 519, learning rate: 0.003350\n",
      "global step: 520, learning rate: 0.003343\n",
      "global step: 521, learning rate: 0.003336\n",
      "global step: 522, learning rate: 0.003329\n",
      "global step: 523, learning rate: 0.003322\n",
      "global step: 524, learning rate: 0.003315\n",
      "global step: 525, learning rate: 0.003308\n",
      "global step: 526, learning rate: 0.003301\n",
      "global step: 527, learning rate: 0.003294\n",
      "global step: 528, learning rate: 0.003287\n",
      "global step: 529, learning rate: 0.003280\n",
      "global step: 530, learning rate: 0.003273\n",
      "global step: 531, learning rate: 0.003266\n",
      "global step: 532, learning rate: 0.003259\n",
      "global step: 533, learning rate: 0.003253\n",
      "global step: 534, learning rate: 0.003246\n",
      "global step: 535, learning rate: 0.003239\n",
      "global step: 536, learning rate: 0.003232\n",
      "global step: 537, learning rate: 0.003225\n",
      "global step: 538, learning rate: 0.003218\n",
      "global step: 539, learning rate: 0.003212\n",
      "global step: 540, learning rate: 0.003205\n",
      "global step: 541, learning rate: 0.003198\n",
      "global step: 542, learning rate: 0.003191\n",
      "global step: 543, learning rate: 0.003185\n",
      "global step: 544, learning rate: 0.003178\n",
      "global step: 545, learning rate: 0.003171\n",
      "global step: 546, learning rate: 0.003165\n",
      "global step: 547, learning rate: 0.003158\n",
      "global step: 548, learning rate: 0.003151\n",
      "global step: 549, learning rate: 0.003145\n",
      "global step: 550, learning rate: 0.003138\n",
      "global step: 551, learning rate: 0.003132\n",
      "global step: 552, learning rate: 0.003125\n",
      "global step: 553, learning rate: 0.003118\n",
      "global step: 554, learning rate: 0.003112\n",
      "global step: 555, learning rate: 0.003105\n",
      "global step: 556, learning rate: 0.003099\n",
      "global step: 557, learning rate: 0.003092\n",
      "global step: 558, learning rate: 0.003086\n",
      "global step: 559, learning rate: 0.003079\n",
      "global step: 560, learning rate: 0.003073\n",
      "global step: 561, learning rate: 0.003066\n",
      "global step: 562, learning rate: 0.003060\n",
      "global step: 563, learning rate: 0.003053\n",
      "global step: 564, learning rate: 0.003047\n",
      "global step: 565, learning rate: 0.003040\n",
      "global step: 566, learning rate: 0.003034\n",
      "global step: 567, learning rate: 0.003028\n",
      "global step: 568, learning rate: 0.003021\n",
      "global step: 569, learning rate: 0.003015\n",
      "global step: 570, learning rate: 0.003009\n",
      "global step: 571, learning rate: 0.003002\n",
      "global step: 572, learning rate: 0.002996\n",
      "global step: 573, learning rate: 0.002990\n",
      "global step: 574, learning rate: 0.002983\n",
      "global step: 575, learning rate: 0.002977\n",
      "global step: 576, learning rate: 0.002971\n",
      "global step: 577, learning rate: 0.002965\n",
      "global step: 578, learning rate: 0.002958\n",
      "global step: 579, learning rate: 0.002952\n",
      "global step: 580, learning rate: 0.002946\n",
      "global step: 581, learning rate: 0.002940\n",
      "global step: 582, learning rate: 0.002933\n",
      "global step: 583, learning rate: 0.002927\n",
      "global step: 584, learning rate: 0.002921\n",
      "global step: 585, learning rate: 0.002915\n",
      "global step: 586, learning rate: 0.002909\n",
      "global step: 587, learning rate: 0.002903\n",
      "global step: 588, learning rate: 0.002897\n",
      "global step: 589, learning rate: 0.002891\n",
      "global step: 590, learning rate: 0.002884\n",
      "global step: 591, learning rate: 0.002878\n",
      "global step: 592, learning rate: 0.002872\n",
      "global step: 593, learning rate: 0.002866\n",
      "global step: 594, learning rate: 0.002860\n",
      "global step: 595, learning rate: 0.002854\n",
      "global step: 596, learning rate: 0.002848\n",
      "global step: 597, learning rate: 0.002842\n",
      "global step: 598, learning rate: 0.002836\n",
      "global step: 599, learning rate: 0.002830\n",
      "global step: 600, learning rate: 0.002824\n",
      "global step: 601, learning rate: 0.002818\n",
      "global step: 602, learning rate: 0.002812\n",
      "global step: 603, learning rate: 0.002806\n",
      "global step: 604, learning rate: 0.002801\n",
      "global step: 605, learning rate: 0.002795\n",
      "global step: 606, learning rate: 0.002789\n",
      "global step: 607, learning rate: 0.002783\n",
      "global step: 608, learning rate: 0.002777\n",
      "global step: 609, learning rate: 0.002771\n",
      "global step: 610, learning rate: 0.002765\n",
      "global step: 611, learning rate: 0.002760\n",
      "global step: 612, learning rate: 0.002754\n",
      "global step: 613, learning rate: 0.002748\n",
      "global step: 614, learning rate: 0.002742\n",
      "global step: 615, learning rate: 0.002736\n",
      "global step: 616, learning rate: 0.002731\n",
      "global step: 617, learning rate: 0.002725\n",
      "global step: 618, learning rate: 0.002719\n",
      "global step: 619, learning rate: 0.002713\n",
      "global step: 620, learning rate: 0.002708\n",
      "global step: 621, learning rate: 0.002702\n",
      "global step: 622, learning rate: 0.002696\n",
      "global step: 623, learning rate: 0.002691\n",
      "global step: 624, learning rate: 0.002685\n",
      "global step: 625, learning rate: 0.002679\n",
      "global step: 626, learning rate: 0.002674\n",
      "global step: 627, learning rate: 0.002668\n",
      "global step: 628, learning rate: 0.002662\n",
      "global step: 629, learning rate: 0.002657\n",
      "global step: 630, learning rate: 0.002651\n",
      "global step: 631, learning rate: 0.002646\n",
      "global step: 632, learning rate: 0.002640\n",
      "global step: 633, learning rate: 0.002635\n",
      "global step: 634, learning rate: 0.002629\n",
      "global step: 635, learning rate: 0.002623\n",
      "global step: 636, learning rate: 0.002618\n",
      "global step: 637, learning rate: 0.002612\n",
      "global step: 638, learning rate: 0.002607\n",
      "global step: 639, learning rate: 0.002601\n",
      "global step: 640, learning rate: 0.002596\n",
      "global step: 641, learning rate: 0.002591\n",
      "global step: 642, learning rate: 0.002585\n",
      "global step: 643, learning rate: 0.002580\n",
      "global step: 644, learning rate: 0.002574\n",
      "global step: 645, learning rate: 0.002569\n",
      "global step: 646, learning rate: 0.002563\n",
      "global step: 647, learning rate: 0.002558\n",
      "global step: 648, learning rate: 0.002553\n",
      "global step: 649, learning rate: 0.002547\n",
      "global step: 650, learning rate: 0.002542\n",
      "global step: 651, learning rate: 0.002537\n",
      "global step: 652, learning rate: 0.002531\n",
      "global step: 653, learning rate: 0.002526\n",
      "global step: 654, learning rate: 0.002521\n",
      "global step: 655, learning rate: 0.002515\n",
      "global step: 656, learning rate: 0.002510\n",
      "global step: 657, learning rate: 0.002505\n",
      "global step: 658, learning rate: 0.002499\n",
      "global step: 659, learning rate: 0.002494\n",
      "global step: 660, learning rate: 0.002489\n",
      "global step: 661, learning rate: 0.002484\n",
      "global step: 662, learning rate: 0.002478\n",
      "global step: 663, learning rate: 0.002473\n",
      "global step: 664, learning rate: 0.002468\n",
      "global step: 665, learning rate: 0.002463\n",
      "global step: 666, learning rate: 0.002458\n",
      "global step: 667, learning rate: 0.002452\n",
      "global step: 668, learning rate: 0.002447\n",
      "global step: 669, learning rate: 0.002442\n",
      "global step: 670, learning rate: 0.002437\n",
      "global step: 671, learning rate: 0.002432\n",
      "global step: 672, learning rate: 0.002427\n",
      "global step: 673, learning rate: 0.002422\n",
      "global step: 674, learning rate: 0.002417\n",
      "global step: 675, learning rate: 0.002411\n",
      "global step: 676, learning rate: 0.002406\n",
      "global step: 677, learning rate: 0.002401\n",
      "global step: 678, learning rate: 0.002396\n",
      "global step: 679, learning rate: 0.002391\n",
      "global step: 680, learning rate: 0.002386\n",
      "global step: 681, learning rate: 0.002381\n",
      "global step: 682, learning rate: 0.002376\n",
      "global step: 683, learning rate: 0.002371\n",
      "global step: 684, learning rate: 0.002366\n",
      "global step: 685, learning rate: 0.002361\n",
      "global step: 686, learning rate: 0.002356\n",
      "global step: 687, learning rate: 0.002351\n",
      "global step: 688, learning rate: 0.002346\n",
      "global step: 689, learning rate: 0.002341\n",
      "global step: 690, learning rate: 0.002336\n",
      "global step: 691, learning rate: 0.002331\n",
      "global step: 692, learning rate: 0.002327\n",
      "global step: 693, learning rate: 0.002322\n",
      "global step: 694, learning rate: 0.002317\n",
      "global step: 695, learning rate: 0.002312\n",
      "global step: 696, learning rate: 0.002307\n",
      "global step: 697, learning rate: 0.002302\n",
      "global step: 698, learning rate: 0.002297\n",
      "global step: 699, learning rate: 0.002293\n",
      "global step: 700, learning rate: 0.002288\n",
      "global step: 701, learning rate: 0.002283\n",
      "global step: 702, learning rate: 0.002278\n",
      "global step: 703, learning rate: 0.002273\n",
      "global step: 704, learning rate: 0.002268\n",
      "global step: 705, learning rate: 0.002264\n",
      "global step: 706, learning rate: 0.002259\n",
      "global step: 707, learning rate: 0.002254\n",
      "global step: 708, learning rate: 0.002249\n",
      "global step: 709, learning rate: 0.002245\n",
      "global step: 710, learning rate: 0.002240\n",
      "global step: 711, learning rate: 0.002235\n",
      "global step: 712, learning rate: 0.002231\n",
      "global step: 713, learning rate: 0.002226\n",
      "global step: 714, learning rate: 0.002221\n",
      "global step: 715, learning rate: 0.002217\n",
      "global step: 716, learning rate: 0.002212\n",
      "global step: 717, learning rate: 0.002207\n",
      "global step: 718, learning rate: 0.002203\n",
      "global step: 719, learning rate: 0.002198\n",
      "global step: 720, learning rate: 0.002193\n",
      "global step: 721, learning rate: 0.002189\n",
      "global step: 722, learning rate: 0.002184\n",
      "global step: 723, learning rate: 0.002179\n",
      "global step: 724, learning rate: 0.002175\n",
      "global step: 725, learning rate: 0.002170\n",
      "global step: 726, learning rate: 0.002166\n",
      "global step: 727, learning rate: 0.002161\n",
      "global step: 728, learning rate: 0.002157\n",
      "global step: 729, learning rate: 0.002152\n",
      "global step: 730, learning rate: 0.002148\n",
      "global step: 731, learning rate: 0.002143\n",
      "global step: 732, learning rate: 0.002139\n",
      "global step: 733, learning rate: 0.002134\n",
      "global step: 734, learning rate: 0.002130\n",
      "global step: 735, learning rate: 0.002125\n",
      "global step: 736, learning rate: 0.002121\n",
      "global step: 737, learning rate: 0.002116\n",
      "global step: 738, learning rate: 0.002112\n",
      "global step: 739, learning rate: 0.002107\n",
      "global step: 740, learning rate: 0.002103\n",
      "global step: 741, learning rate: 0.002098\n",
      "global step: 742, learning rate: 0.002094\n",
      "global step: 743, learning rate: 0.002090\n",
      "global step: 744, learning rate: 0.002085\n",
      "global step: 745, learning rate: 0.002081\n",
      "global step: 746, learning rate: 0.002076\n",
      "global step: 747, learning rate: 0.002072\n",
      "global step: 748, learning rate: 0.002068\n",
      "global step: 749, learning rate: 0.002063\n",
      "global step: 750, learning rate: 0.002059\n",
      "global step: 751, learning rate: 0.002055\n",
      "global step: 752, learning rate: 0.002050\n",
      "global step: 753, learning rate: 0.002046\n",
      "global step: 754, learning rate: 0.002042\n",
      "global step: 755, learning rate: 0.002037\n",
      "global step: 756, learning rate: 0.002033\n",
      "global step: 757, learning rate: 0.002029\n",
      "global step: 758, learning rate: 0.002024\n",
      "global step: 759, learning rate: 0.002020\n",
      "global step: 760, learning rate: 0.002016\n",
      "global step: 761, learning rate: 0.002012\n",
      "global step: 762, learning rate: 0.002008\n",
      "global step: 763, learning rate: 0.002003\n",
      "global step: 764, learning rate: 0.001999\n",
      "global step: 765, learning rate: 0.001995\n",
      "global step: 766, learning rate: 0.001991\n",
      "global step: 767, learning rate: 0.001986\n",
      "global step: 768, learning rate: 0.001982\n",
      "global step: 769, learning rate: 0.001978\n",
      "global step: 770, learning rate: 0.001974\n",
      "global step: 771, learning rate: 0.001970\n",
      "global step: 772, learning rate: 0.001966\n",
      "global step: 773, learning rate: 0.001962\n",
      "global step: 774, learning rate: 0.001957\n",
      "global step: 775, learning rate: 0.001953\n",
      "global step: 776, learning rate: 0.001949\n",
      "global step: 777, learning rate: 0.001945\n",
      "global step: 778, learning rate: 0.001941\n",
      "global step: 779, learning rate: 0.001937\n",
      "global step: 780, learning rate: 0.001933\n",
      "global step: 781, learning rate: 0.001929\n",
      "global step: 782, learning rate: 0.001925\n",
      "global step: 783, learning rate: 0.001921\n",
      "global step: 784, learning rate: 0.001917\n",
      "global step: 785, learning rate: 0.001913\n",
      "global step: 786, learning rate: 0.001909\n",
      "global step: 787, learning rate: 0.001904\n",
      "global step: 788, learning rate: 0.001900\n",
      "global step: 789, learning rate: 0.001896\n",
      "global step: 790, learning rate: 0.001892\n",
      "global step: 791, learning rate: 0.001888\n",
      "global step: 792, learning rate: 0.001885\n",
      "global step: 793, learning rate: 0.001881\n",
      "global step: 794, learning rate: 0.001877\n",
      "global step: 795, learning rate: 0.001873\n",
      "global step: 796, learning rate: 0.001869\n",
      "global step: 797, learning rate: 0.001865\n",
      "global step: 798, learning rate: 0.001861\n",
      "global step: 799, learning rate: 0.001857\n",
      "global step: 800, learning rate: 0.001853\n",
      "global step: 801, learning rate: 0.001849\n",
      "global step: 802, learning rate: 0.001845\n",
      "global step: 803, learning rate: 0.001841\n",
      "global step: 804, learning rate: 0.001837\n",
      "global step: 805, learning rate: 0.001834\n",
      "global step: 806, learning rate: 0.001830\n",
      "global step: 807, learning rate: 0.001826\n",
      "global step: 808, learning rate: 0.001822\n",
      "global step: 809, learning rate: 0.001818\n",
      "global step: 810, learning rate: 0.001814\n",
      "global step: 811, learning rate: 0.001811\n",
      "global step: 812, learning rate: 0.001807\n",
      "global step: 813, learning rate: 0.001803\n",
      "global step: 814, learning rate: 0.001799\n",
      "global step: 815, learning rate: 0.001795\n",
      "global step: 816, learning rate: 0.001792\n",
      "global step: 817, learning rate: 0.001788\n",
      "global step: 818, learning rate: 0.001784\n",
      "global step: 819, learning rate: 0.001780\n",
      "global step: 820, learning rate: 0.001777\n",
      "global step: 821, learning rate: 0.001773\n",
      "global step: 822, learning rate: 0.001769\n",
      "global step: 823, learning rate: 0.001765\n",
      "global step: 824, learning rate: 0.001762\n",
      "global step: 825, learning rate: 0.001758\n",
      "global step: 826, learning rate: 0.001754\n",
      "global step: 827, learning rate: 0.001751\n",
      "global step: 828, learning rate: 0.001747\n",
      "global step: 829, learning rate: 0.001743\n",
      "global step: 830, learning rate: 0.001740\n",
      "global step: 831, learning rate: 0.001736\n",
      "global step: 832, learning rate: 0.001732\n",
      "global step: 833, learning rate: 0.001729\n",
      "global step: 834, learning rate: 0.001725\n",
      "global step: 835, learning rate: 0.001721\n",
      "global step: 836, learning rate: 0.001718\n",
      "global step: 837, learning rate: 0.001714\n",
      "global step: 838, learning rate: 0.001710\n",
      "global step: 839, learning rate: 0.001707\n",
      "global step: 840, learning rate: 0.001703\n",
      "global step: 841, learning rate: 0.001700\n",
      "global step: 842, learning rate: 0.001696\n",
      "global step: 843, learning rate: 0.001693\n",
      "global step: 844, learning rate: 0.001689\n",
      "global step: 845, learning rate: 0.001685\n",
      "global step: 846, learning rate: 0.001682\n",
      "global step: 847, learning rate: 0.001678\n",
      "global step: 848, learning rate: 0.001675\n",
      "global step: 849, learning rate: 0.001671\n",
      "global step: 850, learning rate: 0.001668\n",
      "global step: 851, learning rate: 0.001664\n",
      "global step: 852, learning rate: 0.001661\n",
      "global step: 853, learning rate: 0.001657\n",
      "global step: 854, learning rate: 0.001654\n",
      "global step: 855, learning rate: 0.001650\n",
      "global step: 856, learning rate: 0.001647\n",
      "global step: 857, learning rate: 0.001643\n",
      "global step: 858, learning rate: 0.001640\n",
      "global step: 859, learning rate: 0.001636\n",
      "global step: 860, learning rate: 0.001633\n",
      "global step: 861, learning rate: 0.001630\n",
      "global step: 862, learning rate: 0.001626\n",
      "global step: 863, learning rate: 0.001623\n",
      "global step: 864, learning rate: 0.001619\n",
      "global step: 865, learning rate: 0.001616\n",
      "global step: 866, learning rate: 0.001612\n",
      "global step: 867, learning rate: 0.001609\n",
      "global step: 868, learning rate: 0.001606\n",
      "global step: 869, learning rate: 0.001602\n",
      "global step: 870, learning rate: 0.001599\n",
      "global step: 871, learning rate: 0.001596\n",
      "global step: 872, learning rate: 0.001592\n",
      "global step: 873, learning rate: 0.001589\n",
      "global step: 874, learning rate: 0.001585\n",
      "global step: 875, learning rate: 0.001582\n",
      "global step: 876, learning rate: 0.001579\n",
      "global step: 877, learning rate: 0.001575\n",
      "global step: 878, learning rate: 0.001572\n",
      "global step: 879, learning rate: 0.001569\n",
      "global step: 880, learning rate: 0.001566\n",
      "global step: 881, learning rate: 0.001562\n",
      "global step: 882, learning rate: 0.001559\n",
      "global step: 883, learning rate: 0.001556\n",
      "global step: 884, learning rate: 0.001552\n",
      "global step: 885, learning rate: 0.001549\n",
      "global step: 886, learning rate: 0.001546\n",
      "global step: 887, learning rate: 0.001543\n",
      "global step: 888, learning rate: 0.001539\n",
      "global step: 889, learning rate: 0.001536\n",
      "global step: 890, learning rate: 0.001533\n",
      "global step: 891, learning rate: 0.001530\n",
      "global step: 892, learning rate: 0.001526\n",
      "global step: 893, learning rate: 0.001523\n",
      "global step: 894, learning rate: 0.001520\n",
      "global step: 895, learning rate: 0.001517\n",
      "global step: 896, learning rate: 0.001514\n",
      "global step: 897, learning rate: 0.001510\n",
      "global step: 898, learning rate: 0.001507\n",
      "global step: 899, learning rate: 0.001504\n",
      "global step: 900, learning rate: 0.001501\n",
      "global step: 901, learning rate: 0.001498\n",
      "global step: 902, learning rate: 0.001495\n",
      "global step: 903, learning rate: 0.001491\n",
      "global step: 904, learning rate: 0.001488\n",
      "global step: 905, learning rate: 0.001485\n",
      "global step: 906, learning rate: 0.001482\n",
      "global step: 907, learning rate: 0.001479\n",
      "global step: 908, learning rate: 0.001476\n",
      "global step: 909, learning rate: 0.001473\n",
      "global step: 910, learning rate: 0.001470\n",
      "global step: 911, learning rate: 0.001467\n",
      "global step: 912, learning rate: 0.001463\n",
      "global step: 913, learning rate: 0.001460\n",
      "global step: 914, learning rate: 0.001457\n",
      "global step: 915, learning rate: 0.001454\n",
      "global step: 916, learning rate: 0.001451\n",
      "global step: 917, learning rate: 0.001448\n",
      "global step: 918, learning rate: 0.001445\n",
      "global step: 919, learning rate: 0.001442\n",
      "global step: 920, learning rate: 0.001439\n",
      "global step: 921, learning rate: 0.001436\n",
      "global step: 922, learning rate: 0.001433\n",
      "global step: 923, learning rate: 0.001430\n",
      "global step: 924, learning rate: 0.001427\n",
      "global step: 925, learning rate: 0.001424\n",
      "global step: 926, learning rate: 0.001421\n",
      "global step: 927, learning rate: 0.001418\n",
      "global step: 928, learning rate: 0.001415\n",
      "global step: 929, learning rate: 0.001412\n",
      "global step: 930, learning rate: 0.001409\n",
      "global step: 931, learning rate: 0.001406\n",
      "global step: 932, learning rate: 0.001403\n",
      "global step: 933, learning rate: 0.001400\n",
      "global step: 934, learning rate: 0.001397\n",
      "global step: 935, learning rate: 0.001394\n",
      "global step: 936, learning rate: 0.001391\n",
      "global step: 937, learning rate: 0.001388\n",
      "global step: 938, learning rate: 0.001385\n",
      "global step: 939, learning rate: 0.001383\n",
      "global step: 940, learning rate: 0.001380\n",
      "global step: 941, learning rate: 0.001377\n",
      "global step: 942, learning rate: 0.001374\n",
      "global step: 943, learning rate: 0.001371\n",
      "global step: 944, learning rate: 0.001368\n",
      "global step: 945, learning rate: 0.001365\n",
      "global step: 946, learning rate: 0.001362\n",
      "global step: 947, learning rate: 0.001359\n",
      "global step: 948, learning rate: 0.001357\n",
      "global step: 949, learning rate: 0.001354\n",
      "global step: 950, learning rate: 0.001351\n",
      "global step: 951, learning rate: 0.001348\n",
      "global step: 952, learning rate: 0.001345\n",
      "global step: 953, learning rate: 0.001342\n",
      "global step: 954, learning rate: 0.001340\n",
      "global step: 955, learning rate: 0.001337\n",
      "global step: 956, learning rate: 0.001334\n",
      "global step: 957, learning rate: 0.001331\n",
      "global step: 958, learning rate: 0.001328\n",
      "global step: 959, learning rate: 0.001325\n",
      "global step: 960, learning rate: 0.001323\n",
      "global step: 961, learning rate: 0.001320\n",
      "global step: 962, learning rate: 0.001317\n",
      "global step: 963, learning rate: 0.001314\n",
      "global step: 964, learning rate: 0.001312\n",
      "global step: 965, learning rate: 0.001309\n",
      "global step: 966, learning rate: 0.001306\n",
      "global step: 967, learning rate: 0.001303\n",
      "global step: 968, learning rate: 0.001301\n",
      "global step: 969, learning rate: 0.001298\n",
      "global step: 970, learning rate: 0.001295\n",
      "global step: 971, learning rate: 0.001292\n",
      "global step: 972, learning rate: 0.001290\n",
      "global step: 973, learning rate: 0.001287\n",
      "global step: 974, learning rate: 0.001284\n",
      "global step: 975, learning rate: 0.001282\n",
      "global step: 976, learning rate: 0.001279\n",
      "global step: 977, learning rate: 0.001276\n",
      "global step: 978, learning rate: 0.001273\n",
      "global step: 979, learning rate: 0.001271\n",
      "global step: 980, learning rate: 0.001268\n",
      "global step: 981, learning rate: 0.001265\n",
      "global step: 982, learning rate: 0.001263\n",
      "global step: 983, learning rate: 0.001260\n",
      "global step: 984, learning rate: 0.001257\n",
      "global step: 985, learning rate: 0.001255\n",
      "global step: 986, learning rate: 0.001252\n",
      "global step: 987, learning rate: 0.001250\n",
      "global step: 988, learning rate: 0.001247\n",
      "global step: 989, learning rate: 0.001244\n",
      "global step: 990, learning rate: 0.001242\n",
      "global step: 991, learning rate: 0.001239\n",
      "global step: 992, learning rate: 0.001236\n",
      "global step: 993, learning rate: 0.001234\n",
      "global step: 994, learning rate: 0.001231\n",
      "global step: 995, learning rate: 0.001229\n",
      "global step: 996, learning rate: 0.001226\n",
      "global step: 997, learning rate: 0.001223\n",
      "global step: 998, learning rate: 0.001221\n",
      "global step: 999, learning rate: 0.001218\n",
      "global step: 1000, learning rate: 0.001216\n",
      "global step: 1001, learning rate: 0.001213\n",
      "global step: 1002, learning rate: 0.001211\n",
      "global step: 1003, learning rate: 0.001208\n",
      "global step: 1004, learning rate: 0.001206\n",
      "global step: 1005, learning rate: 0.001203\n",
      "global step: 1006, learning rate: 0.001200\n",
      "global step: 1007, learning rate: 0.001198\n",
      "global step: 1008, learning rate: 0.001195\n",
      "global step: 1009, learning rate: 0.001193\n",
      "global step: 1010, learning rate: 0.001190\n",
      "global step: 1011, learning rate: 0.001188\n",
      "global step: 1012, learning rate: 0.001185\n",
      "global step: 1013, learning rate: 0.001183\n",
      "global step: 1014, learning rate: 0.001180\n",
      "global step: 1015, learning rate: 0.001178\n",
      "global step: 1016, learning rate: 0.001175\n",
      "global step: 1017, learning rate: 0.001173\n",
      "global step: 1018, learning rate: 0.001171\n",
      "global step: 1019, learning rate: 0.001168\n",
      "global step: 1020, learning rate: 0.001166\n",
      "global step: 1021, learning rate: 0.001163\n",
      "global step: 1022, learning rate: 0.001161\n",
      "global step: 1023, learning rate: 0.001158\n",
      "global step: 1024, learning rate: 0.001156\n",
      "global step: 1025, learning rate: 0.001153\n",
      "global step: 1026, learning rate: 0.001151\n",
      "global step: 1027, learning rate: 0.001149\n",
      "global step: 1028, learning rate: 0.001146\n",
      "global step: 1029, learning rate: 0.001144\n",
      "global step: 1030, learning rate: 0.001141\n",
      "global step: 1031, learning rate: 0.001139\n",
      "global step: 1032, learning rate: 0.001136\n",
      "global step: 1033, learning rate: 0.001134\n",
      "global step: 1034, learning rate: 0.001132\n",
      "global step: 1035, learning rate: 0.001129\n",
      "global step: 1036, learning rate: 0.001127\n",
      "global step: 1037, learning rate: 0.001125\n",
      "global step: 1038, learning rate: 0.001122\n",
      "global step: 1039, learning rate: 0.001120\n",
      "global step: 1040, learning rate: 0.001117\n",
      "global step: 1041, learning rate: 0.001115\n",
      "global step: 1042, learning rate: 0.001113\n",
      "global step: 1043, learning rate: 0.001110\n",
      "global step: 1044, learning rate: 0.001108\n",
      "global step: 1045, learning rate: 0.001106\n",
      "global step: 1046, learning rate: 0.001103\n",
      "global step: 1047, learning rate: 0.001101\n",
      "global step: 1048, learning rate: 0.001099\n",
      "global step: 1049, learning rate: 0.001096\n",
      "global step: 1050, learning rate: 0.001094\n",
      "global step: 1051, learning rate: 0.001092\n",
      "global step: 1052, learning rate: 0.001090\n",
      "global step: 1053, learning rate: 0.001087\n",
      "global step: 1054, learning rate: 0.001085\n",
      "global step: 1055, learning rate: 0.001083\n",
      "global step: 1056, learning rate: 0.001080\n",
      "global step: 1057, learning rate: 0.001078\n",
      "global step: 1058, learning rate: 0.001076\n",
      "global step: 1059, learning rate: 0.001074\n",
      "global step: 1060, learning rate: 0.001071\n",
      "global step: 1061, learning rate: 0.001069\n",
      "global step: 1062, learning rate: 0.001067\n",
      "global step: 1063, learning rate: 0.001065\n",
      "global step: 1064, learning rate: 0.001062\n",
      "global step: 1065, learning rate: 0.001060\n",
      "global step: 1066, learning rate: 0.001058\n",
      "global step: 1067, learning rate: 0.001056\n",
      "global step: 1068, learning rate: 0.001053\n",
      "global step: 1069, learning rate: 0.001051\n",
      "global step: 1070, learning rate: 0.001049\n",
      "global step: 1071, learning rate: 0.001047\n",
      "global step: 1072, learning rate: 0.001045\n",
      "global step: 1073, learning rate: 0.001042\n",
      "global step: 1074, learning rate: 0.001040\n",
      "global step: 1075, learning rate: 0.001038\n",
      "global step: 1076, learning rate: 0.001036\n",
      "global step: 1077, learning rate: 0.001034\n",
      "global step: 1078, learning rate: 0.001031\n",
      "global step: 1079, learning rate: 0.001029\n",
      "global step: 1080, learning rate: 0.001027\n",
      "global step: 1081, learning rate: 0.001025\n",
      "global step: 1082, learning rate: 0.001023\n",
      "global step: 1083, learning rate: 0.001021\n",
      "global step: 1084, learning rate: 0.001019\n",
      "global step: 1085, learning rate: 0.001016\n",
      "global step: 1086, learning rate: 0.001014\n",
      "global step: 1087, learning rate: 0.001012\n",
      "global step: 1088, learning rate: 0.001010\n",
      "global step: 1089, learning rate: 0.001008\n",
      "global step: 1090, learning rate: 0.001006\n",
      "global step: 1091, learning rate: 0.001004\n",
      "global step: 1092, learning rate: 0.001002\n",
      "global step: 1093, learning rate: 0.000999\n",
      "global step: 1094, learning rate: 0.000997\n",
      "global step: 1095, learning rate: 0.000995\n",
      "global step: 1096, learning rate: 0.000993\n",
      "global step: 1097, learning rate: 0.000991\n",
      "global step: 1098, learning rate: 0.000989\n",
      "global step: 1099, learning rate: 0.000987\n",
      "global step: 1100, learning rate: 0.000985\n",
      "global step: 1101, learning rate: 0.000983\n",
      "global step: 1102, learning rate: 0.000981\n",
      "global step: 1103, learning rate: 0.000979\n",
      "global step: 1104, learning rate: 0.000977\n",
      "global step: 1105, learning rate: 0.000974\n",
      "global step: 1106, learning rate: 0.000972\n",
      "global step: 1107, learning rate: 0.000970\n",
      "global step: 1108, learning rate: 0.000968\n",
      "global step: 1109, learning rate: 0.000966\n",
      "global step: 1110, learning rate: 0.000964\n",
      "global step: 1111, learning rate: 0.000962\n",
      "global step: 1112, learning rate: 0.000960\n",
      "global step: 1113, learning rate: 0.000958\n",
      "global step: 1114, learning rate: 0.000956\n",
      "global step: 1115, learning rate: 0.000954\n",
      "global step: 1116, learning rate: 0.000952\n",
      "global step: 1117, learning rate: 0.000950\n",
      "global step: 1118, learning rate: 0.000948\n",
      "global step: 1119, learning rate: 0.000946\n",
      "global step: 1120, learning rate: 0.000944\n",
      "global step: 1121, learning rate: 0.000942\n",
      "global step: 1122, learning rate: 0.000940\n",
      "global step: 1123, learning rate: 0.000938\n",
      "global step: 1124, learning rate: 0.000936\n",
      "global step: 1125, learning rate: 0.000934\n",
      "global step: 1126, learning rate: 0.000932\n",
      "global step: 1127, learning rate: 0.000930\n",
      "global step: 1128, learning rate: 0.000928\n",
      "global step: 1129, learning rate: 0.000926\n",
      "global step: 1130, learning rate: 0.000924\n",
      "global step: 1131, learning rate: 0.000922\n",
      "global step: 1132, learning rate: 0.000921\n",
      "global step: 1133, learning rate: 0.000919\n",
      "global step: 1134, learning rate: 0.000917\n",
      "global step: 1135, learning rate: 0.000915\n",
      "global step: 1136, learning rate: 0.000913\n",
      "global step: 1137, learning rate: 0.000911\n",
      "global step: 1138, learning rate: 0.000909\n",
      "global step: 1139, learning rate: 0.000907\n",
      "global step: 1140, learning rate: 0.000905\n",
      "global step: 1141, learning rate: 0.000903\n",
      "global step: 1142, learning rate: 0.000901\n",
      "global step: 1143, learning rate: 0.000899\n",
      "global step: 1144, learning rate: 0.000898\n",
      "global step: 1145, learning rate: 0.000896\n",
      "global step: 1146, learning rate: 0.000894\n",
      "global step: 1147, learning rate: 0.000892\n",
      "global step: 1148, learning rate: 0.000890\n",
      "global step: 1149, learning rate: 0.000888\n",
      "global step: 1150, learning rate: 0.000886\n",
      "global step: 1151, learning rate: 0.000884\n",
      "global step: 1152, learning rate: 0.000883\n",
      "global step: 1153, learning rate: 0.000881\n",
      "global step: 1154, learning rate: 0.000879\n",
      "global step: 1155, learning rate: 0.000877\n",
      "global step: 1156, learning rate: 0.000875\n",
      "global step: 1157, learning rate: 0.000873\n",
      "global step: 1158, learning rate: 0.000871\n",
      "global step: 1159, learning rate: 0.000870\n",
      "global step: 1160, learning rate: 0.000868\n",
      "global step: 1161, learning rate: 0.000866\n",
      "global step: 1162, learning rate: 0.000864\n",
      "global step: 1163, learning rate: 0.000862\n",
      "global step: 1164, learning rate: 0.000861\n",
      "global step: 1165, learning rate: 0.000859\n",
      "global step: 1166, learning rate: 0.000857\n",
      "global step: 1167, learning rate: 0.000855\n",
      "global step: 1168, learning rate: 0.000853\n",
      "global step: 1169, learning rate: 0.000852\n",
      "global step: 1170, learning rate: 0.000850\n",
      "global step: 1171, learning rate: 0.000848\n",
      "global step: 1172, learning rate: 0.000846\n",
      "global step: 1173, learning rate: 0.000844\n",
      "global step: 1174, learning rate: 0.000843\n",
      "global step: 1175, learning rate: 0.000841\n",
      "global step: 1176, learning rate: 0.000839\n",
      "global step: 1177, learning rate: 0.000837\n",
      "global step: 1178, learning rate: 0.000836\n",
      "global step: 1179, learning rate: 0.000834\n",
      "global step: 1180, learning rate: 0.000832\n",
      "global step: 1181, learning rate: 0.000830\n",
      "global step: 1182, learning rate: 0.000829\n",
      "global step: 1183, learning rate: 0.000827\n",
      "global step: 1184, learning rate: 0.000825\n",
      "global step: 1185, learning rate: 0.000823\n",
      "global step: 1186, learning rate: 0.000822\n",
      "global step: 1187, learning rate: 0.000820\n",
      "global step: 1188, learning rate: 0.000818\n",
      "global step: 1189, learning rate: 0.000816\n",
      "global step: 1190, learning rate: 0.000815\n",
      "global step: 1191, learning rate: 0.000813\n",
      "global step: 1192, learning rate: 0.000811\n",
      "global step: 1193, learning rate: 0.000810\n",
      "global step: 1194, learning rate: 0.000808\n",
      "global step: 1195, learning rate: 0.000806\n",
      "global step: 1196, learning rate: 0.000804\n",
      "global step: 1197, learning rate: 0.000803\n",
      "global step: 1198, learning rate: 0.000801\n",
      "global step: 1199, learning rate: 0.000799\n",
      "global step: 1200, learning rate: 0.000798\n",
      "global step: 1201, learning rate: 0.000796\n",
      "global step: 1202, learning rate: 0.000794\n",
      "global step: 1203, learning rate: 0.000793\n",
      "global step: 1204, learning rate: 0.000791\n",
      "global step: 1205, learning rate: 0.000789\n",
      "global step: 1206, learning rate: 0.000788\n",
      "global step: 1207, learning rate: 0.000786\n",
      "global step: 1208, learning rate: 0.000784\n",
      "global step: 1209, learning rate: 0.000783\n",
      "global step: 1210, learning rate: 0.000781\n",
      "global step: 1211, learning rate: 0.000779\n",
      "global step: 1212, learning rate: 0.000778\n",
      "global step: 1213, learning rate: 0.000776\n",
      "global step: 1214, learning rate: 0.000774\n",
      "global step: 1215, learning rate: 0.000773\n",
      "global step: 1216, learning rate: 0.000771\n",
      "global step: 1217, learning rate: 0.000770\n",
      "global step: 1218, learning rate: 0.000768\n",
      "global step: 1219, learning rate: 0.000766\n",
      "global step: 1220, learning rate: 0.000765\n",
      "global step: 1221, learning rate: 0.000763\n",
      "global step: 1222, learning rate: 0.000762\n",
      "global step: 1223, learning rate: 0.000760\n",
      "global step: 1224, learning rate: 0.000758\n",
      "global step: 1225, learning rate: 0.000757\n",
      "global step: 1226, learning rate: 0.000755\n",
      "global step: 1227, learning rate: 0.000754\n",
      "global step: 1228, learning rate: 0.000752\n",
      "global step: 1229, learning rate: 0.000750\n",
      "global step: 1230, learning rate: 0.000749\n",
      "global step: 1231, learning rate: 0.000747\n",
      "global step: 1232, learning rate: 0.000746\n",
      "global step: 1233, learning rate: 0.000744\n",
      "global step: 1234, learning rate: 0.000743\n",
      "global step: 1235, learning rate: 0.000741\n",
      "global step: 1236, learning rate: 0.000739\n",
      "global step: 1237, learning rate: 0.000738\n",
      "global step: 1238, learning rate: 0.000736\n",
      "global step: 1239, learning rate: 0.000735\n",
      "global step: 1240, learning rate: 0.000733\n",
      "global step: 1241, learning rate: 0.000732\n",
      "global step: 1242, learning rate: 0.000730\n",
      "global step: 1243, learning rate: 0.000729\n",
      "global step: 1244, learning rate: 0.000727\n",
      "global step: 1245, learning rate: 0.000726\n",
      "global step: 1246, learning rate: 0.000724\n",
      "global step: 1247, learning rate: 0.000722\n",
      "global step: 1248, learning rate: 0.000721\n",
      "global step: 1249, learning rate: 0.000719\n",
      "global step: 1250, learning rate: 0.000718\n",
      "global step: 1251, learning rate: 0.000716\n",
      "global step: 1252, learning rate: 0.000715\n",
      "global step: 1253, learning rate: 0.000713\n",
      "global step: 1254, learning rate: 0.000712\n",
      "global step: 1255, learning rate: 0.000710\n",
      "global step: 1256, learning rate: 0.000709\n",
      "global step: 1257, learning rate: 0.000707\n",
      "global step: 1258, learning rate: 0.000706\n",
      "global step: 1259, learning rate: 0.000704\n",
      "global step: 1260, learning rate: 0.000703\n",
      "global step: 1261, learning rate: 0.000701\n",
      "global step: 1262, learning rate: 0.000700\n",
      "global step: 1263, learning rate: 0.000698\n",
      "global step: 1264, learning rate: 0.000697\n",
      "global step: 1265, learning rate: 0.000696\n",
      "global step: 1266, learning rate: 0.000694\n",
      "global step: 1267, learning rate: 0.000693\n",
      "global step: 1268, learning rate: 0.000691\n",
      "global step: 1269, learning rate: 0.000690\n",
      "global step: 1270, learning rate: 0.000688\n",
      "global step: 1271, learning rate: 0.000687\n",
      "global step: 1272, learning rate: 0.000685\n",
      "global step: 1273, learning rate: 0.000684\n",
      "global step: 1274, learning rate: 0.000682\n",
      "global step: 1275, learning rate: 0.000681\n",
      "global step: 1276, learning rate: 0.000680\n",
      "global step: 1277, learning rate: 0.000678\n",
      "global step: 1278, learning rate: 0.000677\n",
      "global step: 1279, learning rate: 0.000675\n",
      "global step: 1280, learning rate: 0.000674\n",
      "global step: 1281, learning rate: 0.000673\n",
      "global step: 1282, learning rate: 0.000671\n",
      "global step: 1283, learning rate: 0.000670\n",
      "global step: 1284, learning rate: 0.000668\n",
      "global step: 1285, learning rate: 0.000667\n",
      "global step: 1286, learning rate: 0.000665\n",
      "global step: 1287, learning rate: 0.000664\n",
      "global step: 1288, learning rate: 0.000663\n",
      "global step: 1289, learning rate: 0.000661\n",
      "global step: 1290, learning rate: 0.000660\n",
      "global step: 1291, learning rate: 0.000658\n",
      "global step: 1292, learning rate: 0.000657\n",
      "global step: 1293, learning rate: 0.000656\n",
      "global step: 1294, learning rate: 0.000654\n",
      "global step: 1295, learning rate: 0.000653\n",
      "global step: 1296, learning rate: 0.000652\n",
      "global step: 1297, learning rate: 0.000650\n",
      "global step: 1298, learning rate: 0.000649\n",
      "global step: 1299, learning rate: 0.000647\n",
      "global step: 1300, learning rate: 0.000646\n",
      "global step: 1301, learning rate: 0.000645\n",
      "global step: 1302, learning rate: 0.000643\n",
      "global step: 1303, learning rate: 0.000642\n",
      "global step: 1304, learning rate: 0.000641\n",
      "global step: 1305, learning rate: 0.000639\n",
      "global step: 1306, learning rate: 0.000638\n",
      "global step: 1307, learning rate: 0.000637\n",
      "global step: 1308, learning rate: 0.000635\n",
      "global step: 1309, learning rate: 0.000634\n",
      "global step: 1310, learning rate: 0.000633\n",
      "global step: 1311, learning rate: 0.000631\n",
      "global step: 1312, learning rate: 0.000630\n",
      "global step: 1313, learning rate: 0.000629\n",
      "global step: 1314, learning rate: 0.000627\n",
      "global step: 1315, learning rate: 0.000626\n",
      "global step: 1316, learning rate: 0.000625\n",
      "global step: 1317, learning rate: 0.000623\n",
      "global step: 1318, learning rate: 0.000622\n",
      "global step: 1319, learning rate: 0.000621\n",
      "global step: 1320, learning rate: 0.000619\n",
      "global step: 1321, learning rate: 0.000618\n",
      "global step: 1322, learning rate: 0.000617\n",
      "global step: 1323, learning rate: 0.000616\n",
      "global step: 1324, learning rate: 0.000614\n",
      "global step: 1325, learning rate: 0.000613\n",
      "global step: 1326, learning rate: 0.000612\n",
      "global step: 1327, learning rate: 0.000610\n",
      "global step: 1328, learning rate: 0.000609\n",
      "global step: 1329, learning rate: 0.000608\n",
      "global step: 1330, learning rate: 0.000607\n",
      "global step: 1331, learning rate: 0.000605\n",
      "global step: 1332, learning rate: 0.000604\n",
      "global step: 1333, learning rate: 0.000603\n",
      "global step: 1334, learning rate: 0.000601\n",
      "global step: 1335, learning rate: 0.000600\n",
      "global step: 1336, learning rate: 0.000599\n",
      "global step: 1337, learning rate: 0.000598\n",
      "global step: 1338, learning rate: 0.000596\n",
      "global step: 1339, learning rate: 0.000595\n",
      "global step: 1340, learning rate: 0.000594\n",
      "global step: 1341, learning rate: 0.000593\n",
      "global step: 1342, learning rate: 0.000591\n",
      "global step: 1343, learning rate: 0.000590\n",
      "global step: 1344, learning rate: 0.000589\n",
      "global step: 1345, learning rate: 0.000588\n",
      "global step: 1346, learning rate: 0.000586\n",
      "global step: 1347, learning rate: 0.000585\n",
      "global step: 1348, learning rate: 0.000584\n",
      "global step: 1349, learning rate: 0.000583\n",
      "global step: 1350, learning rate: 0.000581\n",
      "global step: 1351, learning rate: 0.000580\n",
      "global step: 1352, learning rate: 0.000579\n",
      "global step: 1353, learning rate: 0.000578\n",
      "global step: 1354, learning rate: 0.000577\n",
      "global step: 1355, learning rate: 0.000575\n",
      "global step: 1356, learning rate: 0.000574\n",
      "global step: 1357, learning rate: 0.000573\n",
      "global step: 1358, learning rate: 0.000572\n",
      "global step: 1359, learning rate: 0.000571\n",
      "global step: 1360, learning rate: 0.000569\n",
      "global step: 1361, learning rate: 0.000568\n",
      "global step: 1362, learning rate: 0.000567\n",
      "global step: 1363, learning rate: 0.000566\n",
      "global step: 1364, learning rate: 0.000565\n",
      "global step: 1365, learning rate: 0.000563\n",
      "global step: 1366, learning rate: 0.000562\n",
      "global step: 1367, learning rate: 0.000561\n",
      "global step: 1368, learning rate: 0.000560\n",
      "global step: 1369, learning rate: 0.000559\n",
      "global step: 1370, learning rate: 0.000557\n",
      "global step: 1371, learning rate: 0.000556\n",
      "global step: 1372, learning rate: 0.000555\n",
      "global step: 1373, learning rate: 0.000554\n",
      "global step: 1374, learning rate: 0.000553\n",
      "global step: 1375, learning rate: 0.000552\n",
      "global step: 1376, learning rate: 0.000550\n",
      "global step: 1377, learning rate: 0.000549\n",
      "global step: 1378, learning rate: 0.000548\n",
      "global step: 1379, learning rate: 0.000547\n",
      "global step: 1380, learning rate: 0.000546\n",
      "global step: 1381, learning rate: 0.000545\n",
      "global step: 1382, learning rate: 0.000544\n",
      "global step: 1383, learning rate: 0.000542\n",
      "global step: 1384, learning rate: 0.000541\n",
      "global step: 1385, learning rate: 0.000540\n",
      "global step: 1386, learning rate: 0.000539\n",
      "global step: 1387, learning rate: 0.000538\n",
      "global step: 1388, learning rate: 0.000537\n",
      "global step: 1389, learning rate: 0.000536\n",
      "global step: 1390, learning rate: 0.000534\n",
      "global step: 1391, learning rate: 0.000533\n",
      "global step: 1392, learning rate: 0.000532\n",
      "global step: 1393, learning rate: 0.000531\n",
      "global step: 1394, learning rate: 0.000530\n",
      "global step: 1395, learning rate: 0.000529\n",
      "global step: 1396, learning rate: 0.000528\n",
      "global step: 1397, learning rate: 0.000527\n",
      "global step: 1398, learning rate: 0.000526\n",
      "global step: 1399, learning rate: 0.000524\n",
      "global step: 1400, learning rate: 0.000523\n",
      "global step: 1401, learning rate: 0.000522\n",
      "global step: 1402, learning rate: 0.000521\n",
      "global step: 1403, learning rate: 0.000520\n",
      "global step: 1404, learning rate: 0.000519\n",
      "global step: 1405, learning rate: 0.000518\n",
      "global step: 1406, learning rate: 0.000517\n",
      "global step: 1407, learning rate: 0.000516\n",
      "global step: 1408, learning rate: 0.000515\n",
      "global step: 1409, learning rate: 0.000514\n",
      "global step: 1410, learning rate: 0.000512\n",
      "global step: 1411, learning rate: 0.000511\n",
      "global step: 1412, learning rate: 0.000510\n",
      "global step: 1413, learning rate: 0.000509\n",
      "global step: 1414, learning rate: 0.000508\n",
      "global step: 1415, learning rate: 0.000507\n",
      "global step: 1416, learning rate: 0.000506\n",
      "global step: 1417, learning rate: 0.000505\n",
      "global step: 1418, learning rate: 0.000504\n",
      "global step: 1419, learning rate: 0.000503\n",
      "global step: 1420, learning rate: 0.000502\n",
      "global step: 1421, learning rate: 0.000501\n",
      "global step: 1422, learning rate: 0.000500\n",
      "global step: 1423, learning rate: 0.000499\n",
      "global step: 1424, learning rate: 0.000498\n",
      "global step: 1425, learning rate: 0.000496\n",
      "global step: 1426, learning rate: 0.000495\n",
      "global step: 1427, learning rate: 0.000494\n",
      "global step: 1428, learning rate: 0.000493\n",
      "global step: 1429, learning rate: 0.000492\n",
      "global step: 1430, learning rate: 0.000491\n",
      "global step: 1431, learning rate: 0.000490\n",
      "global step: 1432, learning rate: 0.000489\n",
      "global step: 1433, learning rate: 0.000488\n",
      "global step: 1434, learning rate: 0.000487\n",
      "global step: 1435, learning rate: 0.000486\n",
      "global step: 1436, learning rate: 0.000485\n",
      "global step: 1437, learning rate: 0.000484\n",
      "global step: 1438, learning rate: 0.000483\n",
      "global step: 1439, learning rate: 0.000482\n",
      "global step: 1440, learning rate: 0.000481\n",
      "global step: 1441, learning rate: 0.000480\n",
      "global step: 1442, learning rate: 0.000479\n",
      "global step: 1443, learning rate: 0.000478\n",
      "global step: 1444, learning rate: 0.000477\n",
      "global step: 1445, learning rate: 0.000476\n",
      "global step: 1446, learning rate: 0.000475\n",
      "global step: 1447, learning rate: 0.000474\n",
      "global step: 1448, learning rate: 0.000473\n",
      "global step: 1449, learning rate: 0.000472\n",
      "global step: 1450, learning rate: 0.000471\n",
      "global step: 1451, learning rate: 0.000470\n",
      "global step: 1452, learning rate: 0.000469\n",
      "global step: 1453, learning rate: 0.000468\n",
      "global step: 1454, learning rate: 0.000467\n",
      "global step: 1455, learning rate: 0.000466\n",
      "global step: 1456, learning rate: 0.000465\n",
      "global step: 1457, learning rate: 0.000464\n",
      "global step: 1458, learning rate: 0.000463\n",
      "global step: 1459, learning rate: 0.000462\n",
      "global step: 1460, learning rate: 0.000461\n",
      "global step: 1461, learning rate: 0.000460\n",
      "global step: 1462, learning rate: 0.000459\n",
      "global step: 1463, learning rate: 0.000458\n",
      "global step: 1464, learning rate: 0.000457\n",
      "global step: 1465, learning rate: 0.000456\n",
      "global step: 1466, learning rate: 0.000455\n",
      "global step: 1467, learning rate: 0.000454\n",
      "global step: 1468, learning rate: 0.000453\n",
      "global step: 1469, learning rate: 0.000453\n",
      "global step: 1470, learning rate: 0.000452\n",
      "global step: 1471, learning rate: 0.000451\n",
      "global step: 1472, learning rate: 0.000450\n",
      "global step: 1473, learning rate: 0.000449\n",
      "global step: 1474, learning rate: 0.000448\n",
      "global step: 1475, learning rate: 0.000447\n",
      "global step: 1476, learning rate: 0.000446\n",
      "global step: 1477, learning rate: 0.000445\n",
      "global step: 1478, learning rate: 0.000444\n",
      "global step: 1479, learning rate: 0.000443\n",
      "global step: 1480, learning rate: 0.000442\n",
      "global step: 1481, learning rate: 0.000441\n",
      "global step: 1482, learning rate: 0.000440\n",
      "global step: 1483, learning rate: 0.000439\n",
      "global step: 1484, learning rate: 0.000438\n",
      "global step: 1485, learning rate: 0.000438\n",
      "global step: 1486, learning rate: 0.000437\n",
      "global step: 1487, learning rate: 0.000436\n",
      "global step: 1488, learning rate: 0.000435\n",
      "global step: 1489, learning rate: 0.000434\n",
      "global step: 1490, learning rate: 0.000433\n",
      "global step: 1491, learning rate: 0.000432\n",
      "global step: 1492, learning rate: 0.000431\n",
      "global step: 1493, learning rate: 0.000430\n",
      "global step: 1494, learning rate: 0.000429\n",
      "global step: 1495, learning rate: 0.000428\n",
      "global step: 1496, learning rate: 0.000427\n",
      "global step: 1497, learning rate: 0.000427\n",
      "global step: 1498, learning rate: 0.000426\n",
      "global step: 1499, learning rate: 0.000425\n",
      "global step: 1500, learning rate: 0.000424\n",
      "global step: 1501, learning rate: 0.000423\n",
      "global step: 1502, learning rate: 0.000422\n",
      "global step: 1503, learning rate: 0.000421\n",
      "global step: 1504, learning rate: 0.000420\n",
      "global step: 1505, learning rate: 0.000419\n",
      "global step: 1506, learning rate: 0.000419\n",
      "global step: 1507, learning rate: 0.000418\n",
      "global step: 1508, learning rate: 0.000417\n",
      "global step: 1509, learning rate: 0.000416\n",
      "global step: 1510, learning rate: 0.000415\n",
      "global step: 1511, learning rate: 0.000414\n",
      "global step: 1512, learning rate: 0.000413\n",
      "global step: 1513, learning rate: 0.000412\n",
      "global step: 1514, learning rate: 0.000412\n",
      "global step: 1515, learning rate: 0.000411\n",
      "global step: 1516, learning rate: 0.000410\n",
      "global step: 1517, learning rate: 0.000409\n",
      "global step: 1518, learning rate: 0.000408\n",
      "global step: 1519, learning rate: 0.000407\n",
      "global step: 1520, learning rate: 0.000406\n",
      "global step: 1521, learning rate: 0.000406\n",
      "global step: 1522, learning rate: 0.000405\n",
      "global step: 1523, learning rate: 0.000404\n",
      "global step: 1524, learning rate: 0.000403\n",
      "global step: 1525, learning rate: 0.000402\n",
      "global step: 1526, learning rate: 0.000401\n",
      "global step: 1527, learning rate: 0.000400\n",
      "global step: 1528, learning rate: 0.000400\n",
      "global step: 1529, learning rate: 0.000399\n",
      "global step: 1530, learning rate: 0.000398\n",
      "global step: 1531, learning rate: 0.000397\n",
      "global step: 1532, learning rate: 0.000396\n",
      "global step: 1533, learning rate: 0.000395\n",
      "global step: 1534, learning rate: 0.000395\n",
      "global step: 1535, learning rate: 0.000394\n",
      "global step: 1536, learning rate: 0.000393\n",
      "global step: 1537, learning rate: 0.000392\n",
      "global step: 1538, learning rate: 0.000391\n",
      "global step: 1539, learning rate: 0.000390\n",
      "global step: 1540, learning rate: 0.000390\n",
      "global step: 1541, learning rate: 0.000389\n",
      "global step: 1542, learning rate: 0.000388\n",
      "global step: 1543, learning rate: 0.000387\n",
      "global step: 1544, learning rate: 0.000386\n",
      "global step: 1545, learning rate: 0.000386\n",
      "global step: 1546, learning rate: 0.000385\n",
      "global step: 1547, learning rate: 0.000384\n",
      "global step: 1548, learning rate: 0.000383\n",
      "global step: 1549, learning rate: 0.000382\n",
      "global step: 1550, learning rate: 0.000382\n",
      "global step: 1551, learning rate: 0.000381\n",
      "global step: 1552, learning rate: 0.000380\n",
      "global step: 1553, learning rate: 0.000379\n",
      "global step: 1554, learning rate: 0.000378\n",
      "global step: 1555, learning rate: 0.000378\n",
      "global step: 1556, learning rate: 0.000377\n",
      "global step: 1557, learning rate: 0.000376\n",
      "global step: 1558, learning rate: 0.000375\n",
      "global step: 1559, learning rate: 0.000374\n",
      "global step: 1560, learning rate: 0.000374\n",
      "global step: 1561, learning rate: 0.000373\n",
      "global step: 1562, learning rate: 0.000372\n",
      "global step: 1563, learning rate: 0.000371\n",
      "global step: 1564, learning rate: 0.000370\n",
      "global step: 1565, learning rate: 0.000370\n",
      "global step: 1566, learning rate: 0.000369\n",
      "global step: 1567, learning rate: 0.000368\n",
      "global step: 1568, learning rate: 0.000367\n",
      "global step: 1569, learning rate: 0.000367\n",
      "global step: 1570, learning rate: 0.000366\n",
      "global step: 1571, learning rate: 0.000365\n",
      "global step: 1572, learning rate: 0.000364\n",
      "global step: 1573, learning rate: 0.000363\n",
      "global step: 1574, learning rate: 0.000363\n",
      "global step: 1575, learning rate: 0.000362\n",
      "global step: 1576, learning rate: 0.000361\n",
      "global step: 1577, learning rate: 0.000360\n",
      "global step: 1578, learning rate: 0.000360\n",
      "global step: 1579, learning rate: 0.000359\n",
      "global step: 1580, learning rate: 0.000358\n",
      "global step: 1581, learning rate: 0.000357\n",
      "global step: 1582, learning rate: 0.000357\n",
      "global step: 1583, learning rate: 0.000356\n",
      "global step: 1584, learning rate: 0.000355\n",
      "global step: 1585, learning rate: 0.000354\n",
      "global step: 1586, learning rate: 0.000354\n",
      "global step: 1587, learning rate: 0.000353\n",
      "global step: 1588, learning rate: 0.000352\n",
      "global step: 1589, learning rate: 0.000351\n",
      "global step: 1590, learning rate: 0.000351\n",
      "global step: 1591, learning rate: 0.000350\n",
      "global step: 1592, learning rate: 0.000349\n",
      "global step: 1593, learning rate: 0.000348\n",
      "global step: 1594, learning rate: 0.000348\n",
      "global step: 1595, learning rate: 0.000347\n",
      "global step: 1596, learning rate: 0.000346\n",
      "global step: 1597, learning rate: 0.000346\n",
      "global step: 1598, learning rate: 0.000345\n",
      "global step: 1599, learning rate: 0.000344\n",
      "global step: 1600, learning rate: 0.000343\n",
      "global step: 1601, learning rate: 0.000343\n",
      "global step: 1602, learning rate: 0.000342\n",
      "global step: 1603, learning rate: 0.000341\n",
      "global step: 1604, learning rate: 0.000340\n",
      "global step: 1605, learning rate: 0.000340\n",
      "global step: 1606, learning rate: 0.000339\n",
      "global step: 1607, learning rate: 0.000338\n",
      "global step: 1608, learning rate: 0.000338\n",
      "global step: 1609, learning rate: 0.000337\n",
      "global step: 1610, learning rate: 0.000336\n",
      "global step: 1611, learning rate: 0.000336\n",
      "global step: 1612, learning rate: 0.000335\n",
      "global step: 1613, learning rate: 0.000334\n",
      "global step: 1614, learning rate: 0.000333\n",
      "global step: 1615, learning rate: 0.000333\n",
      "global step: 1616, learning rate: 0.000332\n",
      "global step: 1617, learning rate: 0.000331\n",
      "global step: 1618, learning rate: 0.000331\n",
      "global step: 1619, learning rate: 0.000330\n",
      "global step: 1620, learning rate: 0.000329\n",
      "global step: 1621, learning rate: 0.000329\n",
      "global step: 1622, learning rate: 0.000328\n",
      "global step: 1623, learning rate: 0.000327\n",
      "global step: 1624, learning rate: 0.000326\n",
      "global step: 1625, learning rate: 0.000326\n",
      "global step: 1626, learning rate: 0.000325\n",
      "global step: 1627, learning rate: 0.000324\n",
      "global step: 1628, learning rate: 0.000324\n",
      "global step: 1629, learning rate: 0.000323\n",
      "global step: 1630, learning rate: 0.000322\n",
      "global step: 1631, learning rate: 0.000322\n",
      "global step: 1632, learning rate: 0.000321\n",
      "global step: 1633, learning rate: 0.000320\n",
      "global step: 1634, learning rate: 0.000320\n",
      "global step: 1635, learning rate: 0.000319\n",
      "global step: 1636, learning rate: 0.000318\n",
      "global step: 1637, learning rate: 0.000318\n",
      "global step: 1638, learning rate: 0.000317\n",
      "global step: 1639, learning rate: 0.000316\n",
      "global step: 1640, learning rate: 0.000316\n",
      "global step: 1641, learning rate: 0.000315\n",
      "global step: 1642, learning rate: 0.000314\n",
      "global step: 1643, learning rate: 0.000314\n",
      "global step: 1644, learning rate: 0.000313\n",
      "global step: 1645, learning rate: 0.000312\n",
      "global step: 1646, learning rate: 0.000312\n",
      "global step: 1647, learning rate: 0.000311\n",
      "global step: 1648, learning rate: 0.000310\n",
      "global step: 1649, learning rate: 0.000310\n",
      "global step: 1650, learning rate: 0.000309\n",
      "global step: 1651, learning rate: 0.000308\n",
      "global step: 1652, learning rate: 0.000308\n",
      "global step: 1653, learning rate: 0.000307\n",
      "global step: 1654, learning rate: 0.000306\n",
      "global step: 1655, learning rate: 0.000306\n",
      "global step: 1656, learning rate: 0.000305\n",
      "global step: 1657, learning rate: 0.000305\n",
      "global step: 1658, learning rate: 0.000304\n",
      "global step: 1659, learning rate: 0.000303\n",
      "global step: 1660, learning rate: 0.000303\n",
      "global step: 1661, learning rate: 0.000302\n",
      "global step: 1662, learning rate: 0.000301\n",
      "global step: 1663, learning rate: 0.000301\n",
      "global step: 1664, learning rate: 0.000300\n",
      "global step: 1665, learning rate: 0.000299\n",
      "global step: 1666, learning rate: 0.000299\n",
      "global step: 1667, learning rate: 0.000298\n",
      "global step: 1668, learning rate: 0.000298\n",
      "global step: 1669, learning rate: 0.000297\n",
      "global step: 1670, learning rate: 0.000296\n",
      "global step: 1671, learning rate: 0.000296\n",
      "global step: 1672, learning rate: 0.000295\n",
      "global step: 1673, learning rate: 0.000294\n",
      "global step: 1674, learning rate: 0.000294\n",
      "global step: 1675, learning rate: 0.000293\n",
      "global step: 1676, learning rate: 0.000293\n",
      "global step: 1677, learning rate: 0.000292\n",
      "global step: 1678, learning rate: 0.000291\n",
      "global step: 1679, learning rate: 0.000291\n",
      "global step: 1680, learning rate: 0.000290\n",
      "global step: 1681, learning rate: 0.000289\n",
      "global step: 1682, learning rate: 0.000289\n",
      "global step: 1683, learning rate: 0.000288\n",
      "global step: 1684, learning rate: 0.000288\n",
      "global step: 1685, learning rate: 0.000287\n",
      "global step: 1686, learning rate: 0.000286\n",
      "global step: 1687, learning rate: 0.000286\n",
      "global step: 1688, learning rate: 0.000285\n",
      "global step: 1689, learning rate: 0.000285\n",
      "global step: 1690, learning rate: 0.000284\n",
      "global step: 1691, learning rate: 0.000283\n",
      "global step: 1692, learning rate: 0.000283\n",
      "global step: 1693, learning rate: 0.000282\n",
      "global step: 1694, learning rate: 0.000282\n",
      "global step: 1695, learning rate: 0.000281\n",
      "global step: 1696, learning rate: 0.000280\n",
      "global step: 1697, learning rate: 0.000280\n",
      "global step: 1698, learning rate: 0.000279\n",
      "global step: 1699, learning rate: 0.000279\n",
      "global step: 1700, learning rate: 0.000278\n",
      "global step: 1701, learning rate: 0.000278\n",
      "global step: 1702, learning rate: 0.000277\n",
      "global step: 1703, learning rate: 0.000276\n",
      "global step: 1704, learning rate: 0.000276\n",
      "global step: 1705, learning rate: 0.000275\n",
      "global step: 1706, learning rate: 0.000275\n",
      "global step: 1707, learning rate: 0.000274\n",
      "global step: 1708, learning rate: 0.000273\n",
      "global step: 1709, learning rate: 0.000273\n",
      "global step: 1710, learning rate: 0.000272\n",
      "global step: 1711, learning rate: 0.000272\n",
      "global step: 1712, learning rate: 0.000271\n",
      "global step: 1713, learning rate: 0.000271\n",
      "global step: 1714, learning rate: 0.000270\n",
      "global step: 1715, learning rate: 0.000269\n",
      "global step: 1716, learning rate: 0.000269\n",
      "global step: 1717, learning rate: 0.000268\n",
      "global step: 1718, learning rate: 0.000268\n",
      "global step: 1719, learning rate: 0.000267\n",
      "global step: 1720, learning rate: 0.000267\n",
      "global step: 1721, learning rate: 0.000266\n",
      "global step: 1722, learning rate: 0.000266\n",
      "global step: 1723, learning rate: 0.000265\n",
      "global step: 1724, learning rate: 0.000264\n",
      "global step: 1725, learning rate: 0.000264\n",
      "global step: 1726, learning rate: 0.000263\n",
      "global step: 1727, learning rate: 0.000263\n",
      "global step: 1728, learning rate: 0.000262\n",
      "global step: 1729, learning rate: 0.000262\n",
      "global step: 1730, learning rate: 0.000261\n",
      "global step: 1731, learning rate: 0.000261\n",
      "global step: 1732, learning rate: 0.000260\n",
      "global step: 1733, learning rate: 0.000259\n",
      "global step: 1734, learning rate: 0.000259\n",
      "global step: 1735, learning rate: 0.000258\n",
      "global step: 1736, learning rate: 0.000258\n",
      "global step: 1737, learning rate: 0.000257\n",
      "global step: 1738, learning rate: 0.000257\n",
      "global step: 1739, learning rate: 0.000256\n",
      "global step: 1740, learning rate: 0.000256\n",
      "global step: 1741, learning rate: 0.000255\n",
      "global step: 1742, learning rate: 0.000255\n",
      "global step: 1743, learning rate: 0.000254\n",
      "global step: 1744, learning rate: 0.000254\n",
      "global step: 1745, learning rate: 0.000253\n",
      "global step: 1746, learning rate: 0.000252\n",
      "global step: 1747, learning rate: 0.000252\n",
      "global step: 1748, learning rate: 0.000251\n",
      "global step: 1749, learning rate: 0.000251\n",
      "global step: 1750, learning rate: 0.000250\n",
      "global step: 1751, learning rate: 0.000250\n",
      "global step: 1752, learning rate: 0.000249\n",
      "global step: 1753, learning rate: 0.000249\n",
      "global step: 1754, learning rate: 0.000248\n",
      "global step: 1755, learning rate: 0.000248\n",
      "global step: 1756, learning rate: 0.000247\n",
      "global step: 1757, learning rate: 0.000247\n",
      "global step: 1758, learning rate: 0.000246\n",
      "global step: 1759, learning rate: 0.000246\n",
      "global step: 1760, learning rate: 0.000245\n",
      "global step: 1761, learning rate: 0.000245\n",
      "global step: 1762, learning rate: 0.000244\n",
      "global step: 1763, learning rate: 0.000244\n",
      "global step: 1764, learning rate: 0.000243\n",
      "global step: 1765, learning rate: 0.000243\n",
      "global step: 1766, learning rate: 0.000242\n",
      "global step: 1767, learning rate: 0.000242\n",
      "global step: 1768, learning rate: 0.000241\n",
      "global step: 1769, learning rate: 0.000240\n",
      "global step: 1770, learning rate: 0.000240\n",
      "global step: 1771, learning rate: 0.000239\n",
      "global step: 1772, learning rate: 0.000239\n",
      "global step: 1773, learning rate: 0.000238\n",
      "global step: 1774, learning rate: 0.000238\n",
      "global step: 1775, learning rate: 0.000237\n",
      "global step: 1776, learning rate: 0.000237\n",
      "global step: 1777, learning rate: 0.000236\n",
      "global step: 1778, learning rate: 0.000236\n",
      "global step: 1779, learning rate: 0.000235\n",
      "global step: 1780, learning rate: 0.000235\n",
      "global step: 1781, learning rate: 0.000234\n",
      "global step: 1782, learning rate: 0.000234\n",
      "global step: 1783, learning rate: 0.000234\n",
      "global step: 1784, learning rate: 0.000233\n",
      "global step: 1785, learning rate: 0.000233\n",
      "global step: 1786, learning rate: 0.000232\n",
      "global step: 1787, learning rate: 0.000232\n",
      "global step: 1788, learning rate: 0.000231\n",
      "global step: 1789, learning rate: 0.000231\n",
      "global step: 1790, learning rate: 0.000230\n",
      "global step: 1791, learning rate: 0.000230\n",
      "global step: 1792, learning rate: 0.000229\n",
      "global step: 1793, learning rate: 0.000229\n",
      "global step: 1794, learning rate: 0.000228\n",
      "global step: 1795, learning rate: 0.000228\n",
      "global step: 1796, learning rate: 0.000227\n",
      "global step: 1797, learning rate: 0.000227\n",
      "global step: 1798, learning rate: 0.000226\n",
      "global step: 1799, learning rate: 0.000226\n",
      "global step: 1800, learning rate: 0.000225\n",
      "global step: 1801, learning rate: 0.000225\n",
      "global step: 1802, learning rate: 0.000224\n",
      "global step: 1803, learning rate: 0.000224\n",
      "global step: 1804, learning rate: 0.000223\n",
      "global step: 1805, learning rate: 0.000223\n",
      "global step: 1806, learning rate: 0.000222\n",
      "global step: 1807, learning rate: 0.000222\n",
      "global step: 1808, learning rate: 0.000222\n",
      "global step: 1809, learning rate: 0.000221\n",
      "global step: 1810, learning rate: 0.000221\n",
      "global step: 1811, learning rate: 0.000220\n",
      "global step: 1812, learning rate: 0.000220\n",
      "global step: 1813, learning rate: 0.000219\n",
      "global step: 1814, learning rate: 0.000219\n",
      "global step: 1815, learning rate: 0.000218\n",
      "global step: 1816, learning rate: 0.000218\n",
      "global step: 1817, learning rate: 0.000217\n",
      "global step: 1818, learning rate: 0.000217\n",
      "global step: 1819, learning rate: 0.000216\n",
      "global step: 1820, learning rate: 0.000216\n",
      "global step: 1821, learning rate: 0.000216\n",
      "global step: 1822, learning rate: 0.000215\n",
      "global step: 1823, learning rate: 0.000215\n",
      "global step: 1824, learning rate: 0.000214\n",
      "global step: 1825, learning rate: 0.000214\n",
      "global step: 1826, learning rate: 0.000213\n",
      "global step: 1827, learning rate: 0.000213\n",
      "global step: 1828, learning rate: 0.000212\n",
      "global step: 1829, learning rate: 0.000212\n",
      "global step: 1830, learning rate: 0.000211\n",
      "global step: 1831, learning rate: 0.000211\n",
      "global step: 1832, learning rate: 0.000211\n",
      "global step: 1833, learning rate: 0.000210\n",
      "global step: 1834, learning rate: 0.000210\n",
      "global step: 1835, learning rate: 0.000209\n",
      "global step: 1836, learning rate: 0.000209\n",
      "global step: 1837, learning rate: 0.000208\n",
      "global step: 1838, learning rate: 0.000208\n",
      "global step: 1839, learning rate: 0.000208\n",
      "global step: 1840, learning rate: 0.000207\n",
      "global step: 1841, learning rate: 0.000207\n",
      "global step: 1842, learning rate: 0.000206\n",
      "global step: 1843, learning rate: 0.000206\n",
      "global step: 1844, learning rate: 0.000205\n",
      "global step: 1845, learning rate: 0.000205\n",
      "global step: 1846, learning rate: 0.000204\n",
      "global step: 1847, learning rate: 0.000204\n",
      "global step: 1848, learning rate: 0.000204\n",
      "global step: 1849, learning rate: 0.000203\n",
      "global step: 1850, learning rate: 0.000203\n",
      "global step: 1851, learning rate: 0.000202\n",
      "global step: 1852, learning rate: 0.000202\n",
      "global step: 1853, learning rate: 0.000201\n",
      "global step: 1854, learning rate: 0.000201\n",
      "global step: 1855, learning rate: 0.000201\n",
      "global step: 1856, learning rate: 0.000200\n",
      "global step: 1857, learning rate: 0.000200\n",
      "global step: 1858, learning rate: 0.000199\n",
      "global step: 1859, learning rate: 0.000199\n",
      "global step: 1860, learning rate: 0.000199\n",
      "global step: 1861, learning rate: 0.000198\n",
      "global step: 1862, learning rate: 0.000198\n",
      "global step: 1863, learning rate: 0.000197\n",
      "global step: 1864, learning rate: 0.000197\n",
      "global step: 1865, learning rate: 0.000196\n",
      "global step: 1866, learning rate: 0.000196\n",
      "global step: 1867, learning rate: 0.000196\n",
      "global step: 1868, learning rate: 0.000195\n",
      "global step: 1869, learning rate: 0.000195\n",
      "global step: 1870, learning rate: 0.000194\n",
      "global step: 1871, learning rate: 0.000194\n",
      "global step: 1872, learning rate: 0.000194\n",
      "global step: 1873, learning rate: 0.000193\n",
      "global step: 1874, learning rate: 0.000193\n",
      "global step: 1875, learning rate: 0.000192\n",
      "global step: 1876, learning rate: 0.000192\n",
      "global step: 1877, learning rate: 0.000192\n",
      "global step: 1878, learning rate: 0.000191\n",
      "global step: 1879, learning rate: 0.000191\n",
      "global step: 1880, learning rate: 0.000190\n",
      "global step: 1881, learning rate: 0.000190\n",
      "global step: 1882, learning rate: 0.000190\n",
      "global step: 1883, learning rate: 0.000189\n",
      "global step: 1884, learning rate: 0.000189\n",
      "global step: 1885, learning rate: 0.000188\n",
      "global step: 1886, learning rate: 0.000188\n",
      "global step: 1887, learning rate: 0.000188\n",
      "global step: 1888, learning rate: 0.000187\n",
      "global step: 1889, learning rate: 0.000187\n",
      "global step: 1890, learning rate: 0.000186\n",
      "global step: 1891, learning rate: 0.000186\n",
      "global step: 1892, learning rate: 0.000186\n",
      "global step: 1893, learning rate: 0.000185\n",
      "global step: 1894, learning rate: 0.000185\n",
      "global step: 1895, learning rate: 0.000184\n",
      "global step: 1896, learning rate: 0.000184\n",
      "global step: 1897, learning rate: 0.000184\n",
      "global step: 1898, learning rate: 0.000183\n",
      "global step: 1899, learning rate: 0.000183\n",
      "global step: 1900, learning rate: 0.000182\n",
      "global step: 1901, learning rate: 0.000182\n",
      "global step: 1902, learning rate: 0.000182\n",
      "global step: 1903, learning rate: 0.000181\n",
      "global step: 1904, learning rate: 0.000181\n",
      "global step: 1905, learning rate: 0.000181\n",
      "global step: 1906, learning rate: 0.000180\n",
      "global step: 1907, learning rate: 0.000180\n",
      "global step: 1908, learning rate: 0.000179\n",
      "global step: 1909, learning rate: 0.000179\n",
      "global step: 1910, learning rate: 0.000179\n",
      "global step: 1911, learning rate: 0.000178\n",
      "global step: 1912, learning rate: 0.000178\n",
      "global step: 1913, learning rate: 0.000178\n",
      "global step: 1914, learning rate: 0.000177\n",
      "global step: 1915, learning rate: 0.000177\n",
      "global step: 1916, learning rate: 0.000176\n",
      "global step: 1917, learning rate: 0.000176\n",
      "global step: 1918, learning rate: 0.000176\n",
      "global step: 1919, learning rate: 0.000175\n",
      "global step: 1920, learning rate: 0.000175\n",
      "global step: 1921, learning rate: 0.000175\n",
      "global step: 1922, learning rate: 0.000174\n",
      "global step: 1923, learning rate: 0.000174\n",
      "global step: 1924, learning rate: 0.000173\n",
      "global step: 1925, learning rate: 0.000173\n",
      "global step: 1926, learning rate: 0.000173\n",
      "global step: 1927, learning rate: 0.000172\n",
      "global step: 1928, learning rate: 0.000172\n",
      "global step: 1929, learning rate: 0.000172\n",
      "global step: 1930, learning rate: 0.000171\n",
      "global step: 1931, learning rate: 0.000171\n",
      "global step: 1932, learning rate: 0.000171\n",
      "global step: 1933, learning rate: 0.000170\n",
      "global step: 1934, learning rate: 0.000170\n",
      "global step: 1935, learning rate: 0.000170\n",
      "global step: 1936, learning rate: 0.000169\n",
      "global step: 1937, learning rate: 0.000169\n",
      "global step: 1938, learning rate: 0.000168\n",
      "global step: 1939, learning rate: 0.000168\n",
      "global step: 1940, learning rate: 0.000168\n",
      "global step: 1941, learning rate: 0.000167\n",
      "global step: 1942, learning rate: 0.000167\n",
      "global step: 1943, learning rate: 0.000167\n",
      "global step: 1944, learning rate: 0.000166\n",
      "global step: 1945, learning rate: 0.000166\n",
      "global step: 1946, learning rate: 0.000166\n",
      "global step: 1947, learning rate: 0.000165\n",
      "global step: 1948, learning rate: 0.000165\n",
      "global step: 1949, learning rate: 0.000165\n",
      "global step: 1950, learning rate: 0.000164\n",
      "global step: 1951, learning rate: 0.000164\n",
      "global step: 1952, learning rate: 0.000164\n",
      "global step: 1953, learning rate: 0.000163\n",
      "global step: 1954, learning rate: 0.000163\n",
      "global step: 1955, learning rate: 0.000163\n",
      "global step: 1956, learning rate: 0.000162\n",
      "global step: 1957, learning rate: 0.000162\n",
      "global step: 1958, learning rate: 0.000161\n",
      "global step: 1959, learning rate: 0.000161\n",
      "global step: 1960, learning rate: 0.000161\n",
      "global step: 1961, learning rate: 0.000160\n",
      "global step: 1962, learning rate: 0.000160\n",
      "global step: 1963, learning rate: 0.000160\n",
      "global step: 1964, learning rate: 0.000159\n",
      "global step: 1965, learning rate: 0.000159\n",
      "global step: 1966, learning rate: 0.000159\n",
      "global step: 1967, learning rate: 0.000158\n",
      "global step: 1968, learning rate: 0.000158\n",
      "global step: 1969, learning rate: 0.000158\n",
      "global step: 1970, learning rate: 0.000157\n",
      "global step: 1971, learning rate: 0.000157\n",
      "global step: 1972, learning rate: 0.000157\n",
      "global step: 1973, learning rate: 0.000156\n",
      "global step: 1974, learning rate: 0.000156\n",
      "global step: 1975, learning rate: 0.000156\n",
      "global step: 1976, learning rate: 0.000155\n",
      "global step: 1977, learning rate: 0.000155\n",
      "global step: 1978, learning rate: 0.000155\n",
      "global step: 1979, learning rate: 0.000154\n",
      "global step: 1980, learning rate: 0.000154\n",
      "global step: 1981, learning rate: 0.000154\n",
      "global step: 1982, learning rate: 0.000154\n",
      "global step: 1983, learning rate: 0.000153\n",
      "global step: 1984, learning rate: 0.000153\n",
      "global step: 1985, learning rate: 0.000153\n",
      "global step: 1986, learning rate: 0.000152\n",
      "global step: 1987, learning rate: 0.000152\n",
      "global step: 1988, learning rate: 0.000152\n",
      "global step: 1989, learning rate: 0.000151\n",
      "global step: 1990, learning rate: 0.000151\n",
      "global step: 1991, learning rate: 0.000151\n",
      "global step: 1992, learning rate: 0.000150\n",
      "global step: 1993, learning rate: 0.000150\n",
      "global step: 1994, learning rate: 0.000150\n",
      "global step: 1995, learning rate: 0.000149\n",
      "global step: 1996, learning rate: 0.000149\n",
      "global step: 1997, learning rate: 0.000149\n",
      "global step: 1998, learning rate: 0.000148\n",
      "global step: 1999, learning rate: 0.000148\n",
      "global step: 2000, learning rate: 0.000148\n",
      "global step: 2001, learning rate: 0.000147\n",
      "global step: 2002, learning rate: 0.000147\n",
      "global step: 2003, learning rate: 0.000147\n",
      "global step: 2004, learning rate: 0.000147\n",
      "global step: 2005, learning rate: 0.000146\n",
      "global step: 2006, learning rate: 0.000146\n",
      "global step: 2007, learning rate: 0.000146\n",
      "global step: 2008, learning rate: 0.000145\n",
      "global step: 2009, learning rate: 0.000145\n",
      "global step: 2010, learning rate: 0.000145\n",
      "global step: 2011, learning rate: 0.000144\n",
      "global step: 2012, learning rate: 0.000144\n",
      "global step: 2013, learning rate: 0.000144\n",
      "global step: 2014, learning rate: 0.000144\n",
      "global step: 2015, learning rate: 0.000143\n",
      "global step: 2016, learning rate: 0.000143\n",
      "global step: 2017, learning rate: 0.000143\n",
      "global step: 2018, learning rate: 0.000142\n",
      "global step: 2019, learning rate: 0.000142\n",
      "global step: 2020, learning rate: 0.000142\n",
      "global step: 2021, learning rate: 0.000141\n",
      "global step: 2022, learning rate: 0.000141\n",
      "global step: 2023, learning rate: 0.000141\n",
      "global step: 2024, learning rate: 0.000141\n",
      "global step: 2025, learning rate: 0.000140\n",
      "global step: 2026, learning rate: 0.000140\n",
      "global step: 2027, learning rate: 0.000140\n",
      "global step: 2028, learning rate: 0.000139\n",
      "global step: 2029, learning rate: 0.000139\n",
      "global step: 2030, learning rate: 0.000139\n",
      "global step: 2031, learning rate: 0.000138\n",
      "global step: 2032, learning rate: 0.000138\n",
      "global step: 2033, learning rate: 0.000138\n",
      "global step: 2034, learning rate: 0.000138\n",
      "global step: 2035, learning rate: 0.000137\n",
      "global step: 2036, learning rate: 0.000137\n",
      "global step: 2037, learning rate: 0.000137\n",
      "global step: 2038, learning rate: 0.000136\n",
      "global step: 2039, learning rate: 0.000136\n",
      "global step: 2040, learning rate: 0.000136\n",
      "global step: 2041, learning rate: 0.000136\n",
      "global step: 2042, learning rate: 0.000135\n",
      "global step: 2043, learning rate: 0.000135\n",
      "global step: 2044, learning rate: 0.000135\n",
      "global step: 2045, learning rate: 0.000134\n",
      "global step: 2046, learning rate: 0.000134\n",
      "global step: 2047, learning rate: 0.000134\n",
      "global step: 2048, learning rate: 0.000134\n",
      "global step: 2049, learning rate: 0.000133\n",
      "global step: 2050, learning rate: 0.000133\n",
      "global step: 2051, learning rate: 0.000133\n",
      "global step: 2052, learning rate: 0.000132\n",
      "global step: 2053, learning rate: 0.000132\n",
      "global step: 2054, learning rate: 0.000132\n",
      "global step: 2055, learning rate: 0.000132\n",
      "global step: 2056, learning rate: 0.000131\n",
      "global step: 2057, learning rate: 0.000131\n",
      "global step: 2058, learning rate: 0.000131\n",
      "global step: 2059, learning rate: 0.000131\n",
      "global step: 2060, learning rate: 0.000130\n",
      "global step: 2061, learning rate: 0.000130\n",
      "global step: 2062, learning rate: 0.000130\n",
      "global step: 2063, learning rate: 0.000129\n",
      "global step: 2064, learning rate: 0.000129\n",
      "global step: 2065, learning rate: 0.000129\n",
      "global step: 2066, learning rate: 0.000129\n",
      "global step: 2067, learning rate: 0.000128\n",
      "global step: 2068, learning rate: 0.000128\n",
      "global step: 2069, learning rate: 0.000128\n",
      "global step: 2070, learning rate: 0.000128\n",
      "global step: 2071, learning rate: 0.000127\n",
      "global step: 2072, learning rate: 0.000127\n",
      "global step: 2073, learning rate: 0.000127\n",
      "global step: 2074, learning rate: 0.000126\n",
      "global step: 2075, learning rate: 0.000126\n",
      "global step: 2076, learning rate: 0.000126\n",
      "global step: 2077, learning rate: 0.000126\n",
      "global step: 2078, learning rate: 0.000125\n",
      "global step: 2079, learning rate: 0.000125\n",
      "global step: 2080, learning rate: 0.000125\n",
      "global step: 2081, learning rate: 0.000125\n",
      "global step: 2082, learning rate: 0.000124\n",
      "global step: 2083, learning rate: 0.000124\n",
      "global step: 2084, learning rate: 0.000124\n",
      "global step: 2085, learning rate: 0.000124\n",
      "global step: 2086, learning rate: 0.000123\n",
      "global step: 2087, learning rate: 0.000123\n",
      "global step: 2088, learning rate: 0.000123\n",
      "global step: 2089, learning rate: 0.000123\n",
      "global step: 2090, learning rate: 0.000122\n",
      "global step: 2091, learning rate: 0.000122\n",
      "global step: 2092, learning rate: 0.000122\n",
      "global step: 2093, learning rate: 0.000122\n",
      "global step: 2094, learning rate: 0.000121\n",
      "global step: 2095, learning rate: 0.000121\n",
      "global step: 2096, learning rate: 0.000121\n",
      "global step: 2097, learning rate: 0.000120\n",
      "global step: 2098, learning rate: 0.000120\n",
      "global step: 2099, learning rate: 0.000120\n",
      "global step: 2100, learning rate: 0.000120\n",
      "global step: 2101, learning rate: 0.000119\n",
      "global step: 2102, learning rate: 0.000119\n",
      "global step: 2103, learning rate: 0.000119\n",
      "global step: 2104, learning rate: 0.000119\n",
      "global step: 2105, learning rate: 0.000118\n",
      "global step: 2106, learning rate: 0.000118\n",
      "global step: 2107, learning rate: 0.000118\n",
      "global step: 2108, learning rate: 0.000118\n",
      "global step: 2109, learning rate: 0.000117\n",
      "global step: 2110, learning rate: 0.000117\n",
      "global step: 2111, learning rate: 0.000117\n",
      "global step: 2112, learning rate: 0.000117\n",
      "global step: 2113, learning rate: 0.000116\n",
      "global step: 2114, learning rate: 0.000116\n",
      "global step: 2115, learning rate: 0.000116\n",
      "global step: 2116, learning rate: 0.000116\n",
      "global step: 2117, learning rate: 0.000116\n",
      "global step: 2118, learning rate: 0.000115\n",
      "global step: 2119, learning rate: 0.000115\n",
      "global step: 2120, learning rate: 0.000115\n",
      "global step: 2121, learning rate: 0.000115\n",
      "global step: 2122, learning rate: 0.000114\n",
      "global step: 2123, learning rate: 0.000114\n",
      "global step: 2124, learning rate: 0.000114\n",
      "global step: 2125, learning rate: 0.000114\n",
      "global step: 2126, learning rate: 0.000113\n",
      "global step: 2127, learning rate: 0.000113\n",
      "global step: 2128, learning rate: 0.000113\n",
      "global step: 2129, learning rate: 0.000113\n",
      "global step: 2130, learning rate: 0.000112\n",
      "global step: 2131, learning rate: 0.000112\n",
      "global step: 2132, learning rate: 0.000112\n",
      "global step: 2133, learning rate: 0.000112\n",
      "global step: 2134, learning rate: 0.000111\n",
      "global step: 2135, learning rate: 0.000111\n",
      "global step: 2136, learning rate: 0.000111\n",
      "global step: 2137, learning rate: 0.000111\n",
      "global step: 2138, learning rate: 0.000111\n",
      "global step: 2139, learning rate: 0.000110\n",
      "global step: 2140, learning rate: 0.000110\n",
      "global step: 2141, learning rate: 0.000110\n",
      "global step: 2142, learning rate: 0.000110\n",
      "global step: 2143, learning rate: 0.000109\n",
      "global step: 2144, learning rate: 0.000109\n",
      "global step: 2145, learning rate: 0.000109\n",
      "global step: 2146, learning rate: 0.000109\n",
      "global step: 2147, learning rate: 0.000108\n",
      "global step: 2148, learning rate: 0.000108\n",
      "global step: 2149, learning rate: 0.000108\n",
      "global step: 2150, learning rate: 0.000108\n",
      "global step: 2151, learning rate: 0.000108\n",
      "global step: 2152, learning rate: 0.000107\n",
      "global step: 2153, learning rate: 0.000107\n",
      "global step: 2154, learning rate: 0.000107\n",
      "global step: 2155, learning rate: 0.000107\n",
      "global step: 2156, learning rate: 0.000106\n",
      "global step: 2157, learning rate: 0.000106\n",
      "global step: 2158, learning rate: 0.000106\n",
      "global step: 2159, learning rate: 0.000106\n",
      "global step: 2160, learning rate: 0.000106\n",
      "global step: 2161, learning rate: 0.000105\n",
      "global step: 2162, learning rate: 0.000105\n",
      "global step: 2163, learning rate: 0.000105\n",
      "global step: 2164, learning rate: 0.000105\n",
      "global step: 2165, learning rate: 0.000104\n",
      "global step: 2166, learning rate: 0.000104\n",
      "global step: 2167, learning rate: 0.000104\n",
      "global step: 2168, learning rate: 0.000104\n",
      "global step: 2169, learning rate: 0.000104\n",
      "global step: 2170, learning rate: 0.000103\n",
      "global step: 2171, learning rate: 0.000103\n",
      "global step: 2172, learning rate: 0.000103\n",
      "global step: 2173, learning rate: 0.000103\n",
      "global step: 2174, learning rate: 0.000102\n",
      "global step: 2175, learning rate: 0.000102\n",
      "global step: 2176, learning rate: 0.000102\n",
      "global step: 2177, learning rate: 0.000102\n",
      "global step: 2178, learning rate: 0.000102\n",
      "global step: 2179, learning rate: 0.000101\n",
      "global step: 2180, learning rate: 0.000101\n",
      "global step: 2181, learning rate: 0.000101\n",
      "global step: 2182, learning rate: 0.000101\n",
      "global step: 2183, learning rate: 0.000101\n",
      "global step: 2184, learning rate: 0.000100\n",
      "global step: 2185, learning rate: 0.000100\n",
      "global step: 2186, learning rate: 0.000100\n",
      "global step: 2187, learning rate: 0.000100\n",
      "global step: 2188, learning rate: 0.000099\n",
      "global step: 2189, learning rate: 0.000099\n",
      "global step: 2190, learning rate: 0.000099\n",
      "global step: 2191, learning rate: 0.000099\n",
      "global step: 2192, learning rate: 0.000099\n",
      "global step: 2193, learning rate: 0.000098\n",
      "global step: 2194, learning rate: 0.000098\n",
      "global step: 2195, learning rate: 0.000098\n",
      "global step: 2196, learning rate: 0.000098\n",
      "global step: 2197, learning rate: 0.000098\n",
      "global step: 2198, learning rate: 0.000097\n",
      "global step: 2199, learning rate: 0.000097\n",
      "global step: 2200, learning rate: 0.000097\n",
      "global step: 2201, learning rate: 0.000097\n",
      "global step: 2202, learning rate: 0.000097\n",
      "global step: 2203, learning rate: 0.000096\n",
      "global step: 2204, learning rate: 0.000096\n",
      "global step: 2205, learning rate: 0.000096\n",
      "global step: 2206, learning rate: 0.000096\n",
      "global step: 2207, learning rate: 0.000096\n",
      "global step: 2208, learning rate: 0.000095\n",
      "global step: 2209, learning rate: 0.000095\n",
      "global step: 2210, learning rate: 0.000095\n",
      "global step: 2211, learning rate: 0.000095\n",
      "global step: 2212, learning rate: 0.000095\n",
      "global step: 2213, learning rate: 0.000094\n",
      "global step: 2214, learning rate: 0.000094\n",
      "global step: 2215, learning rate: 0.000094\n",
      "global step: 2216, learning rate: 0.000094\n",
      "global step: 2217, learning rate: 0.000094\n",
      "global step: 2218, learning rate: 0.000093\n",
      "global step: 2219, learning rate: 0.000093\n",
      "global step: 2220, learning rate: 0.000093\n",
      "global step: 2221, learning rate: 0.000093\n",
      "global step: 2222, learning rate: 0.000093\n",
      "global step: 2223, learning rate: 0.000092\n",
      "global step: 2224, learning rate: 0.000092\n",
      "global step: 2225, learning rate: 0.000092\n",
      "global step: 2226, learning rate: 0.000092\n",
      "global step: 2227, learning rate: 0.000092\n",
      "global step: 2228, learning rate: 0.000091\n",
      "global step: 2229, learning rate: 0.000091\n",
      "global step: 2230, learning rate: 0.000091\n",
      "global step: 2231, learning rate: 0.000091\n",
      "global step: 2232, learning rate: 0.000091\n",
      "global step: 2233, learning rate: 0.000090\n",
      "global step: 2234, learning rate: 0.000090\n",
      "global step: 2235, learning rate: 0.000090\n",
      "global step: 2236, learning rate: 0.000090\n",
      "global step: 2237, learning rate: 0.000090\n",
      "global step: 2238, learning rate: 0.000090\n",
      "global step: 2239, learning rate: 0.000089\n",
      "global step: 2240, learning rate: 0.000089\n",
      "global step: 2241, learning rate: 0.000089\n",
      "global step: 2242, learning rate: 0.000089\n",
      "global step: 2243, learning rate: 0.000089\n",
      "global step: 2244, learning rate: 0.000088\n",
      "global step: 2245, learning rate: 0.000088\n",
      "global step: 2246, learning rate: 0.000088\n",
      "global step: 2247, learning rate: 0.000088\n",
      "global step: 2248, learning rate: 0.000088\n",
      "global step: 2249, learning rate: 0.000087\n",
      "global step: 2250, learning rate: 0.000087\n",
      "global step: 2251, learning rate: 0.000087\n",
      "global step: 2252, learning rate: 0.000087\n",
      "global step: 2253, learning rate: 0.000087\n",
      "global step: 2254, learning rate: 0.000087\n",
      "global step: 2255, learning rate: 0.000086\n",
      "global step: 2256, learning rate: 0.000086\n",
      "global step: 2257, learning rate: 0.000086\n",
      "global step: 2258, learning rate: 0.000086\n",
      "global step: 2259, learning rate: 0.000086\n",
      "global step: 2260, learning rate: 0.000085\n",
      "global step: 2261, learning rate: 0.000085\n",
      "global step: 2262, learning rate: 0.000085\n",
      "global step: 2263, learning rate: 0.000085\n",
      "global step: 2264, learning rate: 0.000085\n",
      "global step: 2265, learning rate: 0.000085\n",
      "global step: 2266, learning rate: 0.000084\n",
      "global step: 2267, learning rate: 0.000084\n",
      "global step: 2268, learning rate: 0.000084\n",
      "global step: 2269, learning rate: 0.000084\n",
      "global step: 2270, learning rate: 0.000084\n",
      "global step: 2271, learning rate: 0.000084\n",
      "global step: 2272, learning rate: 0.000083\n",
      "global step: 2273, learning rate: 0.000083\n",
      "global step: 2274, learning rate: 0.000083\n",
      "global step: 2275, learning rate: 0.000083\n",
      "global step: 2276, learning rate: 0.000083\n",
      "global step: 2277, learning rate: 0.000082\n",
      "global step: 2278, learning rate: 0.000082\n",
      "global step: 2279, learning rate: 0.000082\n",
      "global step: 2280, learning rate: 0.000082\n",
      "global step: 2281, learning rate: 0.000082\n",
      "global step: 2282, learning rate: 0.000082\n",
      "global step: 2283, learning rate: 0.000081\n",
      "global step: 2284, learning rate: 0.000081\n",
      "global step: 2285, learning rate: 0.000081\n",
      "global step: 2286, learning rate: 0.000081\n",
      "global step: 2287, learning rate: 0.000081\n",
      "global step: 2288, learning rate: 0.000081\n",
      "global step: 2289, learning rate: 0.000080\n",
      "global step: 2290, learning rate: 0.000080\n",
      "global step: 2291, learning rate: 0.000080\n",
      "global step: 2292, learning rate: 0.000080\n",
      "global step: 2293, learning rate: 0.000080\n",
      "global step: 2294, learning rate: 0.000080\n",
      "global step: 2295, learning rate: 0.000079\n",
      "global step: 2296, learning rate: 0.000079\n",
      "global step: 2297, learning rate: 0.000079\n",
      "global step: 2298, learning rate: 0.000079\n",
      "global step: 2299, learning rate: 0.000079\n",
      "global step: 2300, learning rate: 0.000079\n",
      "global step: 2301, learning rate: 0.000078\n",
      "global step: 2302, learning rate: 0.000078\n",
      "global step: 2303, learning rate: 0.000078\n",
      "global step: 2304, learning rate: 0.000078\n",
      "global step: 2305, learning rate: 0.000078\n",
      "global step: 2306, learning rate: 0.000078\n",
      "global step: 2307, learning rate: 0.000077\n",
      "global step: 2308, learning rate: 0.000077\n",
      "global step: 2309, learning rate: 0.000077\n",
      "global step: 2310, learning rate: 0.000077\n",
      "global step: 2311, learning rate: 0.000077\n",
      "global step: 2312, learning rate: 0.000077\n",
      "global step: 2313, learning rate: 0.000076\n",
      "global step: 2314, learning rate: 0.000076\n",
      "global step: 2315, learning rate: 0.000076\n",
      "global step: 2316, learning rate: 0.000076\n",
      "global step: 2317, learning rate: 0.000076\n",
      "global step: 2318, learning rate: 0.000076\n",
      "global step: 2319, learning rate: 0.000075\n",
      "global step: 2320, learning rate: 0.000075\n",
      "global step: 2321, learning rate: 0.000075\n",
      "global step: 2322, learning rate: 0.000075\n",
      "global step: 2323, learning rate: 0.000075\n",
      "global step: 2324, learning rate: 0.000075\n",
      "global step: 2325, learning rate: 0.000075\n",
      "global step: 2326, learning rate: 0.000074\n",
      "global step: 2327, learning rate: 0.000074\n",
      "global step: 2328, learning rate: 0.000074\n",
      "global step: 2329, learning rate: 0.000074\n",
      "global step: 2330, learning rate: 0.000074\n",
      "global step: 2331, learning rate: 0.000074\n",
      "global step: 2332, learning rate: 0.000073\n",
      "global step: 2333, learning rate: 0.000073\n",
      "global step: 2334, learning rate: 0.000073\n",
      "global step: 2335, learning rate: 0.000073\n",
      "global step: 2336, learning rate: 0.000073\n",
      "global step: 2337, learning rate: 0.000073\n",
      "global step: 2338, learning rate: 0.000073\n",
      "global step: 2339, learning rate: 0.000072\n",
      "global step: 2340, learning rate: 0.000072\n",
      "global step: 2341, learning rate: 0.000072\n",
      "global step: 2342, learning rate: 0.000072\n",
      "global step: 2343, learning rate: 0.000072\n",
      "global step: 2344, learning rate: 0.000072\n",
      "global step: 2345, learning rate: 0.000071\n",
      "global step: 2346, learning rate: 0.000071\n",
      "global step: 2347, learning rate: 0.000071\n",
      "global step: 2348, learning rate: 0.000071\n",
      "global step: 2349, learning rate: 0.000071\n",
      "global step: 2350, learning rate: 0.000071\n",
      "global step: 2351, learning rate: 0.000071\n",
      "global step: 2352, learning rate: 0.000070\n",
      "global step: 2353, learning rate: 0.000070\n",
      "global step: 2354, learning rate: 0.000070\n",
      "global step: 2355, learning rate: 0.000070\n",
      "global step: 2356, learning rate: 0.000070\n",
      "global step: 2357, learning rate: 0.000070\n",
      "global step: 2358, learning rate: 0.000070\n",
      "global step: 2359, learning rate: 0.000069\n",
      "global step: 2360, learning rate: 0.000069\n",
      "global step: 2361, learning rate: 0.000069\n",
      "global step: 2362, learning rate: 0.000069\n",
      "global step: 2363, learning rate: 0.000069\n",
      "global step: 2364, learning rate: 0.000069\n",
      "global step: 2365, learning rate: 0.000068\n",
      "global step: 2366, learning rate: 0.000068\n",
      "global step: 2367, learning rate: 0.000068\n",
      "global step: 2368, learning rate: 0.000068\n",
      "global step: 2369, learning rate: 0.000068\n",
      "global step: 2370, learning rate: 0.000068\n",
      "global step: 2371, learning rate: 0.000068\n",
      "global step: 2372, learning rate: 0.000067\n",
      "global step: 2373, learning rate: 0.000067\n",
      "global step: 2374, learning rate: 0.000067\n",
      "global step: 2375, learning rate: 0.000067\n",
      "global step: 2376, learning rate: 0.000067\n",
      "global step: 2377, learning rate: 0.000067\n",
      "global step: 2378, learning rate: 0.000067\n",
      "global step: 2379, learning rate: 0.000067\n",
      "global step: 2380, learning rate: 0.000066\n",
      "global step: 2381, learning rate: 0.000066\n",
      "global step: 2382, learning rate: 0.000066\n",
      "global step: 2383, learning rate: 0.000066\n",
      "global step: 2384, learning rate: 0.000066\n",
      "global step: 2385, learning rate: 0.000066\n",
      "global step: 2386, learning rate: 0.000066\n",
      "global step: 2387, learning rate: 0.000065\n",
      "global step: 2388, learning rate: 0.000065\n",
      "global step: 2389, learning rate: 0.000065\n",
      "global step: 2390, learning rate: 0.000065\n",
      "global step: 2391, learning rate: 0.000065\n",
      "global step: 2392, learning rate: 0.000065\n",
      "global step: 2393, learning rate: 0.000065\n",
      "global step: 2394, learning rate: 0.000064\n",
      "global step: 2395, learning rate: 0.000064\n",
      "global step: 2396, learning rate: 0.000064\n",
      "global step: 2397, learning rate: 0.000064\n",
      "global step: 2398, learning rate: 0.000064\n",
      "global step: 2399, learning rate: 0.000064\n",
      "global step: 2400, learning rate: 0.000064\n",
      "global step: 2401, learning rate: 0.000063\n",
      "global step: 2402, learning rate: 0.000063\n",
      "global step: 2403, learning rate: 0.000063\n",
      "global step: 2404, learning rate: 0.000063\n",
      "global step: 2405, learning rate: 0.000063\n",
      "global step: 2406, learning rate: 0.000063\n",
      "global step: 2407, learning rate: 0.000063\n",
      "global step: 2408, learning rate: 0.000063\n",
      "global step: 2409, learning rate: 0.000062\n",
      "global step: 2410, learning rate: 0.000062\n",
      "global step: 2411, learning rate: 0.000062\n",
      "global step: 2412, learning rate: 0.000062\n",
      "global step: 2413, learning rate: 0.000062\n",
      "global step: 2414, learning rate: 0.000062\n",
      "global step: 2415, learning rate: 0.000062\n",
      "global step: 2416, learning rate: 0.000062\n",
      "global step: 2417, learning rate: 0.000061\n",
      "global step: 2418, learning rate: 0.000061\n",
      "global step: 2419, learning rate: 0.000061\n",
      "global step: 2420, learning rate: 0.000061\n",
      "global step: 2421, learning rate: 0.000061\n",
      "global step: 2422, learning rate: 0.000061\n",
      "global step: 2423, learning rate: 0.000061\n",
      "global step: 2424, learning rate: 0.000060\n",
      "global step: 2425, learning rate: 0.000060\n",
      "global step: 2426, learning rate: 0.000060\n",
      "global step: 2427, learning rate: 0.000060\n",
      "global step: 2428, learning rate: 0.000060\n",
      "global step: 2429, learning rate: 0.000060\n",
      "global step: 2430, learning rate: 0.000060\n",
      "global step: 2431, learning rate: 0.000060\n",
      "global step: 2432, learning rate: 0.000059\n",
      "global step: 2433, learning rate: 0.000059\n",
      "global step: 2434, learning rate: 0.000059\n",
      "global step: 2435, learning rate: 0.000059\n",
      "global step: 2436, learning rate: 0.000059\n",
      "global step: 2437, learning rate: 0.000059\n",
      "global step: 2438, learning rate: 0.000059\n",
      "global step: 2439, learning rate: 0.000059\n",
      "global step: 2440, learning rate: 0.000058\n",
      "global step: 2441, learning rate: 0.000058\n",
      "global step: 2442, learning rate: 0.000058\n",
      "global step: 2443, learning rate: 0.000058\n",
      "global step: 2444, learning rate: 0.000058\n",
      "global step: 2445, learning rate: 0.000058\n",
      "global step: 2446, learning rate: 0.000058\n",
      "global step: 2447, learning rate: 0.000058\n",
      "global step: 2448, learning rate: 0.000058\n",
      "global step: 2449, learning rate: 0.000057\n",
      "global step: 2450, learning rate: 0.000057\n",
      "global step: 2451, learning rate: 0.000057\n",
      "global step: 2452, learning rate: 0.000057\n",
      "global step: 2453, learning rate: 0.000057\n",
      "global step: 2454, learning rate: 0.000057\n",
      "global step: 2455, learning rate: 0.000057\n",
      "global step: 2456, learning rate: 0.000057\n",
      "global step: 2457, learning rate: 0.000056\n",
      "global step: 2458, learning rate: 0.000056\n",
      "global step: 2459, learning rate: 0.000056\n",
      "global step: 2460, learning rate: 0.000056\n",
      "global step: 2461, learning rate: 0.000056\n",
      "global step: 2462, learning rate: 0.000056\n",
      "global step: 2463, learning rate: 0.000056\n",
      "global step: 2464, learning rate: 0.000056\n",
      "global step: 2465, learning rate: 0.000055\n",
      "global step: 2466, learning rate: 0.000055\n",
      "global step: 2467, learning rate: 0.000055\n",
      "global step: 2468, learning rate: 0.000055\n",
      "global step: 2469, learning rate: 0.000055\n",
      "global step: 2470, learning rate: 0.000055\n",
      "global step: 2471, learning rate: 0.000055\n",
      "global step: 2472, learning rate: 0.000055\n",
      "global step: 2473, learning rate: 0.000055\n",
      "global step: 2474, learning rate: 0.000054\n",
      "global step: 2475, learning rate: 0.000054\n",
      "global step: 2476, learning rate: 0.000054\n",
      "global step: 2477, learning rate: 0.000054\n",
      "global step: 2478, learning rate: 0.000054\n",
      "global step: 2479, learning rate: 0.000054\n",
      "global step: 2480, learning rate: 0.000054\n",
      "global step: 2481, learning rate: 0.000054\n",
      "global step: 2482, learning rate: 0.000054\n",
      "global step: 2483, learning rate: 0.000053\n",
      "global step: 2484, learning rate: 0.000053\n",
      "global step: 2485, learning rate: 0.000053\n",
      "global step: 2486, learning rate: 0.000053\n",
      "global step: 2487, learning rate: 0.000053\n",
      "global step: 2488, learning rate: 0.000053\n",
      "global step: 2489, learning rate: 0.000053\n",
      "global step: 2490, learning rate: 0.000053\n",
      "global step: 2491, learning rate: 0.000053\n",
      "global step: 2492, learning rate: 0.000052\n",
      "global step: 2493, learning rate: 0.000052\n",
      "global step: 2494, learning rate: 0.000052\n",
      "global step: 2495, learning rate: 0.000052\n",
      "global step: 2496, learning rate: 0.000052\n",
      "global step: 2497, learning rate: 0.000052\n",
      "global step: 2498, learning rate: 0.000052\n",
      "global step: 2499, learning rate: 0.000052\n",
      "global step: 2500, learning rate: 0.000052\n",
      "global step: 2501, learning rate: 0.000051\n",
      "global step: 2502, learning rate: 0.000051\n",
      "global step: 2503, learning rate: 0.000051\n",
      "global step: 2504, learning rate: 0.000051\n",
      "global step: 2505, learning rate: 0.000051\n",
      "global step: 2506, learning rate: 0.000051\n",
      "global step: 2507, learning rate: 0.000051\n",
      "global step: 2508, learning rate: 0.000051\n",
      "global step: 2509, learning rate: 0.000051\n",
      "global step: 2510, learning rate: 0.000050\n",
      "global step: 2511, learning rate: 0.000050\n",
      "global step: 2512, learning rate: 0.000050\n",
      "global step: 2513, learning rate: 0.000050\n",
      "global step: 2514, learning rate: 0.000050\n",
      "global step: 2515, learning rate: 0.000050\n",
      "global step: 2516, learning rate: 0.000050\n",
      "global step: 2517, learning rate: 0.000050\n",
      "global step: 2518, learning rate: 0.000050\n",
      "global step: 2519, learning rate: 0.000050\n",
      "global step: 2520, learning rate: 0.000049\n",
      "global step: 2521, learning rate: 0.000049\n",
      "global step: 2522, learning rate: 0.000049\n",
      "global step: 2523, learning rate: 0.000049\n",
      "global step: 2524, learning rate: 0.000049\n",
      "global step: 2525, learning rate: 0.000049\n",
      "global step: 2526, learning rate: 0.000049\n",
      "global step: 2527, learning rate: 0.000049\n",
      "global step: 2528, learning rate: 0.000049\n",
      "global step: 2529, learning rate: 0.000048\n",
      "global step: 2530, learning rate: 0.000048\n",
      "global step: 2531, learning rate: 0.000048\n",
      "global step: 2532, learning rate: 0.000048\n",
      "global step: 2533, learning rate: 0.000048\n",
      "global step: 2534, learning rate: 0.000048\n",
      "global step: 2535, learning rate: 0.000048\n",
      "global step: 2536, learning rate: 0.000048\n",
      "global step: 2537, learning rate: 0.000048\n",
      "global step: 2538, learning rate: 0.000048\n",
      "global step: 2539, learning rate: 0.000047\n",
      "global step: 2540, learning rate: 0.000047\n",
      "global step: 2541, learning rate: 0.000047\n",
      "global step: 2542, learning rate: 0.000047\n",
      "global step: 2543, learning rate: 0.000047\n",
      "global step: 2544, learning rate: 0.000047\n",
      "global step: 2545, learning rate: 0.000047\n",
      "global step: 2546, learning rate: 0.000047\n",
      "global step: 2547, learning rate: 0.000047\n",
      "global step: 2548, learning rate: 0.000047\n",
      "global step: 2549, learning rate: 0.000046\n",
      "global step: 2550, learning rate: 0.000046\n",
      "global step: 2551, learning rate: 0.000046\n",
      "global step: 2552, learning rate: 0.000046\n",
      "global step: 2553, learning rate: 0.000046\n",
      "global step: 2554, learning rate: 0.000046\n",
      "global step: 2555, learning rate: 0.000046\n",
      "global step: 2556, learning rate: 0.000046\n",
      "global step: 2557, learning rate: 0.000046\n",
      "global step: 2558, learning rate: 0.000046\n",
      "global step: 2559, learning rate: 0.000046\n",
      "global step: 2560, learning rate: 0.000045\n",
      "global step: 2561, learning rate: 0.000045\n",
      "global step: 2562, learning rate: 0.000045\n",
      "global step: 2563, learning rate: 0.000045\n",
      "global step: 2564, learning rate: 0.000045\n",
      "global step: 2565, learning rate: 0.000045\n",
      "global step: 2566, learning rate: 0.000045\n",
      "global step: 2567, learning rate: 0.000045\n",
      "global step: 2568, learning rate: 0.000045\n",
      "global step: 2569, learning rate: 0.000045\n",
      "global step: 2570, learning rate: 0.000044\n",
      "global step: 2571, learning rate: 0.000044\n",
      "global step: 2572, learning rate: 0.000044\n",
      "global step: 2573, learning rate: 0.000044\n",
      "global step: 2574, learning rate: 0.000044\n",
      "global step: 2575, learning rate: 0.000044\n",
      "global step: 2576, learning rate: 0.000044\n",
      "global step: 2577, learning rate: 0.000044\n",
      "global step: 2578, learning rate: 0.000044\n",
      "global step: 2579, learning rate: 0.000044\n",
      "global step: 2580, learning rate: 0.000044\n",
      "global step: 2581, learning rate: 0.000043\n",
      "global step: 2582, learning rate: 0.000043\n",
      "global step: 2583, learning rate: 0.000043\n",
      "global step: 2584, learning rate: 0.000043\n",
      "global step: 2585, learning rate: 0.000043\n",
      "global step: 2586, learning rate: 0.000043\n",
      "global step: 2587, learning rate: 0.000043\n",
      "global step: 2588, learning rate: 0.000043\n",
      "global step: 2589, learning rate: 0.000043\n",
      "global step: 2590, learning rate: 0.000043\n",
      "global step: 2591, learning rate: 0.000043\n",
      "global step: 2592, learning rate: 0.000042\n",
      "global step: 2593, learning rate: 0.000042\n",
      "global step: 2594, learning rate: 0.000042\n",
      "global step: 2595, learning rate: 0.000042\n",
      "global step: 2596, learning rate: 0.000042\n",
      "global step: 2597, learning rate: 0.000042\n",
      "global step: 2598, learning rate: 0.000042\n",
      "global step: 2599, learning rate: 0.000042\n",
      "global step: 2600, learning rate: 0.000042\n",
      "global step: 2601, learning rate: 0.000042\n",
      "global step: 2602, learning rate: 0.000042\n",
      "global step: 2603, learning rate: 0.000041\n",
      "global step: 2604, learning rate: 0.000041\n",
      "global step: 2605, learning rate: 0.000041\n",
      "global step: 2606, learning rate: 0.000041\n",
      "global step: 2607, learning rate: 0.000041\n",
      "global step: 2608, learning rate: 0.000041\n",
      "global step: 2609, learning rate: 0.000041\n",
      "global step: 2610, learning rate: 0.000041\n",
      "global step: 2611, learning rate: 0.000041\n",
      "global step: 2612, learning rate: 0.000041\n",
      "global step: 2613, learning rate: 0.000041\n",
      "global step: 2614, learning rate: 0.000041\n",
      "global step: 2615, learning rate: 0.000040\n",
      "global step: 2616, learning rate: 0.000040\n",
      "global step: 2617, learning rate: 0.000040\n",
      "global step: 2618, learning rate: 0.000040\n",
      "global step: 2619, learning rate: 0.000040\n",
      "global step: 2620, learning rate: 0.000040\n",
      "global step: 2621, learning rate: 0.000040\n",
      "global step: 2622, learning rate: 0.000040\n",
      "global step: 2623, learning rate: 0.000040\n",
      "global step: 2624, learning rate: 0.000040\n",
      "global step: 2625, learning rate: 0.000040\n",
      "global step: 2626, learning rate: 0.000040\n",
      "global step: 2627, learning rate: 0.000039\n",
      "global step: 2628, learning rate: 0.000039\n",
      "global step: 2629, learning rate: 0.000039\n",
      "global step: 2630, learning rate: 0.000039\n",
      "global step: 2631, learning rate: 0.000039\n",
      "global step: 2632, learning rate: 0.000039\n",
      "global step: 2633, learning rate: 0.000039\n",
      "global step: 2634, learning rate: 0.000039\n",
      "global step: 2635, learning rate: 0.000039\n",
      "global step: 2636, learning rate: 0.000039\n",
      "global step: 2637, learning rate: 0.000039\n",
      "global step: 2638, learning rate: 0.000039\n",
      "global step: 2639, learning rate: 0.000038\n",
      "global step: 2640, learning rate: 0.000038\n",
      "global step: 2641, learning rate: 0.000038\n",
      "global step: 2642, learning rate: 0.000038\n",
      "global step: 2643, learning rate: 0.000038\n",
      "global step: 2644, learning rate: 0.000038\n",
      "global step: 2645, learning rate: 0.000038\n",
      "global step: 2646, learning rate: 0.000038\n",
      "global step: 2647, learning rate: 0.000038\n",
      "global step: 2648, learning rate: 0.000038\n",
      "global step: 2649, learning rate: 0.000038\n",
      "global step: 2650, learning rate: 0.000038\n",
      "global step: 2651, learning rate: 0.000037\n",
      "global step: 2652, learning rate: 0.000037\n",
      "global step: 2653, learning rate: 0.000037\n",
      "global step: 2654, learning rate: 0.000037\n",
      "global step: 2655, learning rate: 0.000037\n",
      "global step: 2656, learning rate: 0.000037\n",
      "global step: 2657, learning rate: 0.000037\n",
      "global step: 2658, learning rate: 0.000037\n",
      "global step: 2659, learning rate: 0.000037\n",
      "global step: 2660, learning rate: 0.000037\n",
      "global step: 2661, learning rate: 0.000037\n",
      "global step: 2662, learning rate: 0.000037\n",
      "global step: 2663, learning rate: 0.000037\n",
      "global step: 2664, learning rate: 0.000036\n",
      "global step: 2665, learning rate: 0.000036\n",
      "global step: 2666, learning rate: 0.000036\n",
      "global step: 2667, learning rate: 0.000036\n",
      "global step: 2668, learning rate: 0.000036\n",
      "global step: 2669, learning rate: 0.000036\n",
      "global step: 2670, learning rate: 0.000036\n",
      "global step: 2671, learning rate: 0.000036\n",
      "global step: 2672, learning rate: 0.000036\n",
      "global step: 2673, learning rate: 0.000036\n",
      "global step: 2674, learning rate: 0.000036\n",
      "global step: 2675, learning rate: 0.000036\n",
      "global step: 2676, learning rate: 0.000036\n",
      "global step: 2677, learning rate: 0.000035\n",
      "global step: 2678, learning rate: 0.000035\n",
      "global step: 2679, learning rate: 0.000035\n",
      "global step: 2680, learning rate: 0.000035\n",
      "global step: 2681, learning rate: 0.000035\n",
      "global step: 2682, learning rate: 0.000035\n",
      "global step: 2683, learning rate: 0.000035\n",
      "global step: 2684, learning rate: 0.000035\n",
      "global step: 2685, learning rate: 0.000035\n",
      "global step: 2686, learning rate: 0.000035\n",
      "global step: 2687, learning rate: 0.000035\n",
      "global step: 2688, learning rate: 0.000035\n",
      "global step: 2689, learning rate: 0.000035\n",
      "global step: 2690, learning rate: 0.000035\n",
      "global step: 2691, learning rate: 0.000034\n",
      "global step: 2692, learning rate: 0.000034\n",
      "global step: 2693, learning rate: 0.000034\n",
      "global step: 2694, learning rate: 0.000034\n",
      "global step: 2695, learning rate: 0.000034\n",
      "global step: 2696, learning rate: 0.000034\n",
      "global step: 2697, learning rate: 0.000034\n",
      "global step: 2698, learning rate: 0.000034\n",
      "global step: 2699, learning rate: 0.000034\n",
      "global step: 2700, learning rate: 0.000034\n",
      "global step: 2701, learning rate: 0.000034\n",
      "global step: 2702, learning rate: 0.000034\n",
      "global step: 2703, learning rate: 0.000034\n",
      "global step: 2704, learning rate: 0.000034\n",
      "global step: 2705, learning rate: 0.000033\n",
      "global step: 2706, learning rate: 0.000033\n",
      "global step: 2707, learning rate: 0.000033\n",
      "global step: 2708, learning rate: 0.000033\n",
      "global step: 2709, learning rate: 0.000033\n",
      "global step: 2710, learning rate: 0.000033\n",
      "global step: 2711, learning rate: 0.000033\n",
      "global step: 2712, learning rate: 0.000033\n",
      "global step: 2713, learning rate: 0.000033\n",
      "global step: 2714, learning rate: 0.000033\n",
      "global step: 2715, learning rate: 0.000033\n",
      "global step: 2716, learning rate: 0.000033\n",
      "global step: 2717, learning rate: 0.000033\n",
      "global step: 2718, learning rate: 0.000033\n",
      "global step: 2719, learning rate: 0.000032\n",
      "global step: 2720, learning rate: 0.000032\n",
      "global step: 2721, learning rate: 0.000032\n",
      "global step: 2722, learning rate: 0.000032\n",
      "global step: 2723, learning rate: 0.000032\n",
      "global step: 2724, learning rate: 0.000032\n",
      "global step: 2725, learning rate: 0.000032\n",
      "global step: 2726, learning rate: 0.000032\n",
      "global step: 2727, learning rate: 0.000032\n",
      "global step: 2728, learning rate: 0.000032\n",
      "global step: 2729, learning rate: 0.000032\n",
      "global step: 2730, learning rate: 0.000032\n",
      "global step: 2731, learning rate: 0.000032\n",
      "global step: 2732, learning rate: 0.000032\n",
      "global step: 2733, learning rate: 0.000032\n",
      "global step: 2734, learning rate: 0.000031\n",
      "global step: 2735, learning rate: 0.000031\n",
      "global step: 2736, learning rate: 0.000031\n",
      "global step: 2737, learning rate: 0.000031\n",
      "global step: 2738, learning rate: 0.000031\n",
      "global step: 2739, learning rate: 0.000031\n",
      "global step: 2740, learning rate: 0.000031\n",
      "global step: 2741, learning rate: 0.000031\n",
      "global step: 2742, learning rate: 0.000031\n",
      "global step: 2743, learning rate: 0.000031\n",
      "global step: 2744, learning rate: 0.000031\n",
      "global step: 2745, learning rate: 0.000031\n",
      "global step: 2746, learning rate: 0.000031\n",
      "global step: 2747, learning rate: 0.000031\n",
      "global step: 2748, learning rate: 0.000031\n",
      "global step: 2749, learning rate: 0.000030\n",
      "global step: 2750, learning rate: 0.000030\n",
      "global step: 2751, learning rate: 0.000030\n",
      "global step: 2752, learning rate: 0.000030\n",
      "global step: 2753, learning rate: 0.000030\n",
      "global step: 2754, learning rate: 0.000030\n",
      "global step: 2755, learning rate: 0.000030\n",
      "global step: 2756, learning rate: 0.000030\n",
      "global step: 2757, learning rate: 0.000030\n",
      "global step: 2758, learning rate: 0.000030\n",
      "global step: 2759, learning rate: 0.000030\n",
      "global step: 2760, learning rate: 0.000030\n",
      "global step: 2761, learning rate: 0.000030\n",
      "global step: 2762, learning rate: 0.000030\n",
      "global step: 2763, learning rate: 0.000030\n",
      "global step: 2764, learning rate: 0.000030\n",
      "global step: 2765, learning rate: 0.000029\n",
      "global step: 2766, learning rate: 0.000029\n",
      "global step: 2767, learning rate: 0.000029\n",
      "global step: 2768, learning rate: 0.000029\n",
      "global step: 2769, learning rate: 0.000029\n",
      "global step: 2770, learning rate: 0.000029\n",
      "global step: 2771, learning rate: 0.000029\n",
      "global step: 2772, learning rate: 0.000029\n",
      "global step: 2773, learning rate: 0.000029\n",
      "global step: 2774, learning rate: 0.000029\n",
      "global step: 2775, learning rate: 0.000029\n",
      "global step: 2776, learning rate: 0.000029\n",
      "global step: 2777, learning rate: 0.000029\n",
      "global step: 2778, learning rate: 0.000029\n",
      "global step: 2779, learning rate: 0.000029\n",
      "global step: 2780, learning rate: 0.000029\n",
      "global step: 2781, learning rate: 0.000029\n",
      "global step: 2782, learning rate: 0.000028\n",
      "global step: 2783, learning rate: 0.000028\n",
      "global step: 2784, learning rate: 0.000028\n",
      "global step: 2785, learning rate: 0.000028\n",
      "global step: 2786, learning rate: 0.000028\n",
      "global step: 2787, learning rate: 0.000028\n",
      "global step: 2788, learning rate: 0.000028\n",
      "global step: 2789, learning rate: 0.000028\n",
      "global step: 2790, learning rate: 0.000028\n",
      "global step: 2791, learning rate: 0.000028\n",
      "global step: 2792, learning rate: 0.000028\n",
      "global step: 2793, learning rate: 0.000028\n",
      "global step: 2794, learning rate: 0.000028\n",
      "global step: 2795, learning rate: 0.000028\n",
      "global step: 2796, learning rate: 0.000028\n",
      "global step: 2797, learning rate: 0.000028\n",
      "global step: 2798, learning rate: 0.000028\n",
      "global step: 2799, learning rate: 0.000027\n",
      "global step: 2800, learning rate: 0.000027\n",
      "global step: 2801, learning rate: 0.000027\n",
      "global step: 2802, learning rate: 0.000027\n",
      "global step: 2803, learning rate: 0.000027\n",
      "global step: 2804, learning rate: 0.000027\n",
      "global step: 2805, learning rate: 0.000027\n",
      "global step: 2806, learning rate: 0.000027\n",
      "global step: 2807, learning rate: 0.000027\n",
      "global step: 2808, learning rate: 0.000027\n",
      "global step: 2809, learning rate: 0.000027\n",
      "global step: 2810, learning rate: 0.000027\n",
      "global step: 2811, learning rate: 0.000027\n",
      "global step: 2812, learning rate: 0.000027\n",
      "global step: 2813, learning rate: 0.000027\n",
      "global step: 2814, learning rate: 0.000027\n",
      "global step: 2815, learning rate: 0.000027\n",
      "global step: 2816, learning rate: 0.000026\n",
      "global step: 2817, learning rate: 0.000026\n",
      "global step: 2818, learning rate: 0.000026\n",
      "global step: 2819, learning rate: 0.000026\n",
      "global step: 2820, learning rate: 0.000026\n",
      "global step: 2821, learning rate: 0.000026\n",
      "global step: 2822, learning rate: 0.000026\n",
      "global step: 2823, learning rate: 0.000026\n",
      "global step: 2824, learning rate: 0.000026\n",
      "global step: 2825, learning rate: 0.000026\n",
      "global step: 2826, learning rate: 0.000026\n",
      "global step: 2827, learning rate: 0.000026\n",
      "global step: 2828, learning rate: 0.000026\n",
      "global step: 2829, learning rate: 0.000026\n",
      "global step: 2830, learning rate: 0.000026\n",
      "global step: 2831, learning rate: 0.000026\n",
      "global step: 2832, learning rate: 0.000026\n",
      "global step: 2833, learning rate: 0.000026\n",
      "global step: 2834, learning rate: 0.000025\n",
      "global step: 2835, learning rate: 0.000025\n",
      "global step: 2836, learning rate: 0.000025\n",
      "global step: 2837, learning rate: 0.000025\n",
      "global step: 2838, learning rate: 0.000025\n",
      "global step: 2839, learning rate: 0.000025\n",
      "global step: 2840, learning rate: 0.000025\n",
      "global step: 2841, learning rate: 0.000025\n",
      "global step: 2842, learning rate: 0.000025\n",
      "global step: 2843, learning rate: 0.000025\n",
      "global step: 2844, learning rate: 0.000025\n",
      "global step: 2845, learning rate: 0.000025\n",
      "global step: 2846, learning rate: 0.000025\n",
      "global step: 2847, learning rate: 0.000025\n",
      "global step: 2848, learning rate: 0.000025\n",
      "global step: 2849, learning rate: 0.000025\n",
      "global step: 2850, learning rate: 0.000025\n",
      "global step: 2851, learning rate: 0.000025\n",
      "global step: 2852, learning rate: 0.000025\n",
      "global step: 2853, learning rate: 0.000024\n",
      "global step: 2854, learning rate: 0.000024\n",
      "global step: 2855, learning rate: 0.000024\n",
      "global step: 2856, learning rate: 0.000024\n",
      "global step: 2857, learning rate: 0.000024\n",
      "global step: 2858, learning rate: 0.000024\n",
      "global step: 2859, learning rate: 0.000024\n",
      "global step: 2860, learning rate: 0.000024\n",
      "global step: 2861, learning rate: 0.000024\n",
      "global step: 2862, learning rate: 0.000024\n",
      "global step: 2863, learning rate: 0.000024\n",
      "global step: 2864, learning rate: 0.000024\n",
      "global step: 2865, learning rate: 0.000024\n",
      "global step: 2866, learning rate: 0.000024\n",
      "global step: 2867, learning rate: 0.000024\n",
      "global step: 2868, learning rate: 0.000024\n",
      "global step: 2869, learning rate: 0.000024\n",
      "global step: 2870, learning rate: 0.000024\n",
      "global step: 2871, learning rate: 0.000024\n",
      "global step: 2872, learning rate: 0.000024\n",
      "global step: 2873, learning rate: 0.000023\n",
      "global step: 2874, learning rate: 0.000023\n",
      "global step: 2875, learning rate: 0.000023\n",
      "global step: 2876, learning rate: 0.000023\n",
      "global step: 2877, learning rate: 0.000023\n",
      "global step: 2878, learning rate: 0.000023\n",
      "global step: 2879, learning rate: 0.000023\n",
      "global step: 2880, learning rate: 0.000023\n",
      "global step: 2881, learning rate: 0.000023\n",
      "global step: 2882, learning rate: 0.000023\n",
      "global step: 2883, learning rate: 0.000023\n",
      "global step: 2884, learning rate: 0.000023\n",
      "global step: 2885, learning rate: 0.000023\n",
      "global step: 2886, learning rate: 0.000023\n",
      "global step: 2887, learning rate: 0.000023\n",
      "global step: 2888, learning rate: 0.000023\n",
      "global step: 2889, learning rate: 0.000023\n",
      "global step: 2890, learning rate: 0.000023\n",
      "global step: 2891, learning rate: 0.000023\n",
      "global step: 2892, learning rate: 0.000023\n",
      "global step: 2893, learning rate: 0.000023\n",
      "global step: 2894, learning rate: 0.000022\n",
      "global step: 2895, learning rate: 0.000022\n",
      "global step: 2896, learning rate: 0.000022\n",
      "global step: 2897, learning rate: 0.000022\n",
      "global step: 2898, learning rate: 0.000022\n",
      "global step: 2899, learning rate: 0.000022\n",
      "global step: 2900, learning rate: 0.000022\n",
      "global step: 2901, learning rate: 0.000022\n",
      "global step: 2902, learning rate: 0.000022\n",
      "global step: 2903, learning rate: 0.000022\n",
      "global step: 2904, learning rate: 0.000022\n",
      "global step: 2905, learning rate: 0.000022\n",
      "global step: 2906, learning rate: 0.000022\n",
      "global step: 2907, learning rate: 0.000022\n",
      "global step: 2908, learning rate: 0.000022\n",
      "global step: 2909, learning rate: 0.000022\n",
      "global step: 2910, learning rate: 0.000022\n",
      "global step: 2911, learning rate: 0.000022\n",
      "global step: 2912, learning rate: 0.000022\n",
      "global step: 2913, learning rate: 0.000022\n",
      "global step: 2914, learning rate: 0.000022\n",
      "global step: 2915, learning rate: 0.000021\n",
      "global step: 2916, learning rate: 0.000021\n",
      "global step: 2917, learning rate: 0.000021\n",
      "global step: 2918, learning rate: 0.000021\n",
      "global step: 2919, learning rate: 0.000021\n",
      "global step: 2920, learning rate: 0.000021\n",
      "global step: 2921, learning rate: 0.000021\n",
      "global step: 2922, learning rate: 0.000021\n",
      "global step: 2923, learning rate: 0.000021\n",
      "global step: 2924, learning rate: 0.000021\n",
      "global step: 2925, learning rate: 0.000021\n",
      "global step: 2926, learning rate: 0.000021\n",
      "global step: 2927, learning rate: 0.000021\n",
      "global step: 2928, learning rate: 0.000021\n",
      "global step: 2929, learning rate: 0.000021\n",
      "global step: 2930, learning rate: 0.000021\n",
      "global step: 2931, learning rate: 0.000021\n",
      "global step: 2932, learning rate: 0.000021\n",
      "global step: 2933, learning rate: 0.000021\n",
      "global step: 2934, learning rate: 0.000021\n",
      "global step: 2935, learning rate: 0.000021\n",
      "global step: 2936, learning rate: 0.000021\n",
      "global step: 2937, learning rate: 0.000021\n",
      "global step: 2938, learning rate: 0.000020\n",
      "global step: 2939, learning rate: 0.000020\n",
      "global step: 2940, learning rate: 0.000020\n",
      "global step: 2941, learning rate: 0.000020\n",
      "global step: 2942, learning rate: 0.000020\n",
      "global step: 2943, learning rate: 0.000020\n",
      "global step: 2944, learning rate: 0.000020\n",
      "global step: 2945, learning rate: 0.000020\n",
      "global step: 2946, learning rate: 0.000020\n",
      "global step: 2947, learning rate: 0.000020\n",
      "global step: 2948, learning rate: 0.000020\n",
      "global step: 2949, learning rate: 0.000020\n",
      "global step: 2950, learning rate: 0.000020\n",
      "global step: 2951, learning rate: 0.000020\n",
      "global step: 2952, learning rate: 0.000020\n",
      "global step: 2953, learning rate: 0.000020\n",
      "global step: 2954, learning rate: 0.000020\n",
      "global step: 2955, learning rate: 0.000020\n",
      "global step: 2956, learning rate: 0.000020\n",
      "global step: 2957, learning rate: 0.000020\n",
      "global step: 2958, learning rate: 0.000020\n",
      "global step: 2959, learning rate: 0.000020\n",
      "global step: 2960, learning rate: 0.000020\n",
      "global step: 2961, learning rate: 0.000020\n",
      "global step: 2962, learning rate: 0.000019\n",
      "global step: 2963, learning rate: 0.000019\n",
      "global step: 2964, learning rate: 0.000019\n",
      "global step: 2965, learning rate: 0.000019\n",
      "global step: 2966, learning rate: 0.000019\n",
      "global step: 2967, learning rate: 0.000019\n",
      "global step: 2968, learning rate: 0.000019\n",
      "global step: 2969, learning rate: 0.000019\n",
      "global step: 2970, learning rate: 0.000019\n",
      "global step: 2971, learning rate: 0.000019\n",
      "global step: 2972, learning rate: 0.000019\n",
      "global step: 2973, learning rate: 0.000019\n",
      "global step: 2974, learning rate: 0.000019\n",
      "global step: 2975, learning rate: 0.000019\n",
      "global step: 2976, learning rate: 0.000019\n",
      "global step: 2977, learning rate: 0.000019\n",
      "global step: 2978, learning rate: 0.000019\n",
      "global step: 2979, learning rate: 0.000019\n",
      "global step: 2980, learning rate: 0.000019\n",
      "global step: 2981, learning rate: 0.000019\n",
      "global step: 2982, learning rate: 0.000019\n",
      "global step: 2983, learning rate: 0.000019\n",
      "global step: 2984, learning rate: 0.000019\n",
      "global step: 2985, learning rate: 0.000019\n",
      "global step: 2986, learning rate: 0.000019\n",
      "global step: 2987, learning rate: 0.000018\n",
      "global step: 2988, learning rate: 0.000018\n",
      "global step: 2989, learning rate: 0.000018\n",
      "global step: 2990, learning rate: 0.000018\n",
      "global step: 2991, learning rate: 0.000018\n",
      "global step: 2992, learning rate: 0.000018\n",
      "global step: 2993, learning rate: 0.000018\n",
      "global step: 2994, learning rate: 0.000018\n",
      "global step: 2995, learning rate: 0.000018\n",
      "global step: 2996, learning rate: 0.000018\n",
      "global step: 2997, learning rate: 0.000018\n",
      "global step: 2998, learning rate: 0.000018\n",
      "global step: 2999, learning rate: 0.000018\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "global_steps = 3000\n",
    "learning_rate = 0.01\n",
    "decay_rate = 0.9#0.65\n",
    "decay_steps = 50#3500\n",
    "# 指数学习率衰减过程\n",
    "for global_step in range(global_steps):\n",
    "    decayed_learning_rate = learning_rate * decay_rate**(global_step / decay_steps)\n",
    "    X.append(global_step / decay_steps)\n",
    "    Y.append(decayed_learning_rate)\n",
    "    print(\"global step: %d, learning rate: %f\" % (global_step,decayed_learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAF5CAYAAACBThBWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQRAEQRDUHH8DE0Zez3xrW10AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5fe5da26d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(1)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "curve = ax.plot(X,Y,'r',label=\"learning rate\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"global_step / decay_steps\")\n",
    "ax.set_ylabel(\"learning_rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
